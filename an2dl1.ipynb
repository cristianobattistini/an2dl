{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79a2dd0",
   "metadata": {},
   "source": [
    "# **Artificial Neural Networks and Deep Learning**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b2cc3",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è **Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6513d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import copy\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa9263",
   "metadata": {},
   "source": [
    "## ‚è≥ **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = Path(\"./dataset\")\n",
    "\n",
    "# --- 2Ô∏è‚É£ Kaggle ---\n",
    "# DATASET_ROOT = Path(\"/kaggle/input/pirate-pain\")\n",
    "\n",
    "# --- 3Ô∏è‚É£ Server o cluster privato (es. Westworld/Elysium) ---\n",
    "# DATASET_ROOT = Path(\"/multiverse/datasets/private_dataset/pirate_pain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24be028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Caricamento dati\n",
    "X_train = pd.read_csv(DATASET_ROOT / \"pirate_pain_train.csv\")\n",
    "X_TRAIN = pd.read_csv(DATASET_ROOT / \"pirate_pain_train.csv\")\n",
    "\n",
    "y_train = pd.read_csv(DATASET_ROOT / \"pirate_pain_train_labels.csv\")\n",
    "Y_TRAIN = pd.read_csv(DATASET_ROOT / \"pirate_pain_train_labels.csv\")\n",
    "\n",
    "X_test  = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\n",
    "\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e88eff",
   "metadata": {},
   "source": [
    "## üîé **Exploration and Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4cca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controllo struttura e tipi\n",
    "display(X_train.head())\n",
    "X_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ef9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE BETWEEN TRAIN DATA AND LABELS\n",
    "# the labels are in a separated file linked through 'sample_index\n",
    "# here we merge X_train and y_train in a unique Dataframe to explore\n",
    "\n",
    "train_merge = X_train.merge(y_train, on=\"sample_index\", how=\"left\")\n",
    "\n",
    "# check whether all the labels have been associated or not\n",
    "missing_labels = train_merge[\"label\"].isna().sum()\n",
    "if missing_labels > 0:\n",
    "    print(f\"{missing_labels} rows without a label\")\n",
    "\n",
    "# check\n",
    "print(train_merge[[\"sample_index\",\"time\",\"label\"]].head())\n",
    "print(\"Class Distribution\")\n",
    "print(train_merge[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c814d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006c340",
   "metadata": {},
   "source": [
    "## üîÑ **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_conversion_type(df, label_conversion=False):\n",
    "    df = df.copy()\n",
    "\n",
    "    # select all columns starting with joint_\n",
    "    joint_cols = [col for col in df.columns if col.startswith(\"joint_\")]\n",
    "\n",
    "    # convert all joint columns in float32\n",
    "    df[joint_cols] = df[joint_cols].astype(np.float32)\n",
    "\n",
    "    # select all columns starting with pain_survey_\n",
    "    pain_survey_cols = [col for col in df.columns if col.startswith(\"pain_survey_\")]\n",
    "    df[pain_survey_cols] = df[pain_survey_cols].astype(np.int8)\n",
    "\n",
    "    legs_map = {\"two\": 0, \"one+peg_leg\": 1}\n",
    "    hands_map = {\"two\": 0, \"one+hook_hand\": 1}\n",
    "    eyes_map  = {\"two\": 0, \"one+eye_patch\": 1}\n",
    "\n",
    "    df[\"n_legs\"]  = df[\"n_legs\"].map(legs_map).astype(np.int8)\n",
    "    df[\"n_hands\"] = df[\"n_hands\"].map(hands_map).astype(np.int8)\n",
    "    df[\"n_eyes\"]  = df[\"n_eyes\"].map(eyes_map).astype(np.int8)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the count of timestamps for each kind of pain\n",
    "plt.figure(figsize=(17, 5))\n",
    "sns.countplot(\n",
    "    x='label',\n",
    "    data=train_merge,\n",
    "    order=train_merge['label'].value_counts().index,\n",
    "    palette='tab10'\n",
    ")\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('Label Timestamps')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of timesteps per sample\n",
    "seq_lengths = train_merge.groupby(\"sample_index\").size()\n",
    "\n",
    "# Quick look\n",
    "print(seq_lengths.describe())\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(seq_lengths, bins=30, kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribution of Sequence Lengths\")\n",
    "plt.xlabel(\"Timesteps per Sequence\")\n",
    "plt.ylabel(\"Number of Sequences\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to inspect sensor data for a specific pain\n",
    "def inspect_pain(pain, df, n_rows=500):\n",
    "    # Filter the DataFrame for the specified pain and limit to 500 rows\n",
    "    joint_cols = [col for col in df.columns if col.startswith(\"joint_\")]\n",
    "    data = df[df['label'] == pain][joint_cols][:n_rows]\n",
    "\n",
    "    # Dynamically adjust figure height based on number of joints\n",
    "    fig_height = len(joint_cols) * 1  # keep proportions similar to your original\n",
    "    axis = data.plot(subplots=True, figsize=(17, fig_height), title=pain)\n",
    "\n",
    "    # Adjust legend position for each subplot\n",
    "    for ax in axis:\n",
    "        ax.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the sensor data for the activity \"Standing\"\n",
    "inspect_pain(\"no_pain\", train_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the sensor data for the activity \"Standing\"\n",
    "inspect_pain(\"low_pain\", train_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the sensor data for the activity \"Standing\"\n",
    "inspect_pain(\"high_pain\", train_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ce476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_joints(df, \n",
    "                      drop_redundant=False, \n",
    "                      drop_near_zero=False, \n",
    "                      drop_low_var=False,\n",
    "                      verbose=True):\n",
    "    \"\"\"\n",
    "    Simplify joint_* preprocessing based on EDA results.\n",
    "    Removes constant, redundant, or near-zero-variance joints.\n",
    "\n",
    "    Returns a (df_out, feature_cols) tuple.\n",
    "    \"\"\"\n",
    "    joint_cols = sorted([c for c in df.columns if c.startswith(\"joint_\")],\n",
    "                        key=lambda x: int(x.split(\"_\")[1]))\n",
    "    drop = set()\n",
    "\n",
    "    # 1 Drop constant joint_30\n",
    "    if \"joint_30\" in joint_cols:\n",
    "        drop.add(\"joint_30\")\n",
    "\n",
    "    #  Drop redundant joints (from correlation heatmap)\n",
    "    if drop_redundant:\n",
    "        for c in [\"joint_01\", \"joint_02\", \"joint_05\"]:\n",
    "            if c in joint_cols:\n",
    "                drop.add(c)\n",
    "\n",
    "    # Drop near-zero variance joints (joint_13‚Äì25)\n",
    "    if drop_near_zero:\n",
    "        for i in range(13, 26):\n",
    "            c = f\"joint_{i:02d}\"\n",
    "            if c in joint_cols:\n",
    "                drop.add(c)\n",
    "\n",
    "    # (Optional) Drop low-variance but not-zero joints (joint_26‚Äì29)\n",
    "    if drop_low_var:\n",
    "        for i in range(26, 30):\n",
    "            c = f\"joint_{i:02d}\"\n",
    "            if c in joint_cols:\n",
    "                drop.add(c)\n",
    "\n",
    "    # apply\n",
    "    kept = [c for c in joint_cols if c not in drop]\n",
    "    df_out = df.drop(columns=list(drop), errors=\"ignore\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[preprocess_joints] start={len(joint_cols)} | kept={len(kept)} | dropped={len(drop)}\")\n",
    "        if drop:\n",
    "            print(\"  ‚Ä¢ dropped:\", sorted(list(drop)))\n",
    "\n",
    "    return df_out, kept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce115870",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _ = preprocess_joints(X_train)\n",
    "X_test, _ = preprocess_joints(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65bedcf",
   "metadata": {},
   "source": [
    "## üîÑ **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914db185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 ‚Äî Apply same preprocessing (feature + label mapping)\n",
    "X_train = dataset_conversion_type(X_train)\n",
    "X_test = dataset_conversion_type(X_test)\n",
    "\n",
    "# Step 3 ‚Äî Sanity check\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "print(\"Train columns:\", X_train.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 ‚Äî Copy merged train and raw test\n",
    "train_dataset = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reload quickly\n",
    "X_train = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b52f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train columns:\", train_dataset.columns.tolist())\n",
    "print(\"Test columns:\", X_test.columns.tolist())\n",
    "\n",
    "train_only = [c for c in X_train.columns if c not in X_test.columns]\n",
    "test_only  = [c for c in X_test.columns if c not in X_train.columns]\n",
    "\n",
    "if train_only or test_only:\n",
    "    print(\"Column mismatch detected!\")\n",
    "    if train_only:\n",
    "        print(\"  Present only in TRAIN:\", train_only)\n",
    "    if test_only:\n",
    "        print(\"  Present only in TEST:\", test_only)\n",
    "else:\n",
    "    print(\"‚úÖ Train and Test have identical columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed14591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. temporary merge X_train + y_train to create splits ---\n",
    "train_merged = X_train.merge(y_train, on=\"sample_index\")\n",
    "print(train_merged.shape)\n",
    "\n",
    "# Step 2. retrieve unique indexes and labels to stratify ---\n",
    "unique_samples = train_merged['sample_index'].unique()\n",
    "y_seq = train_merged.groupby('sample_index')['label'].first().reindex(unique_samples).values\n",
    "\n",
    "# Step 3. Divide in train e val (stratified) ---\n",
    "\n",
    "train_idxs, val_idxs = train_test_split(unique_samples, test_size=0.05, random_state=SEED, stratify=y_seq)\n",
    "print(f\"Train Size: {len(train_idxs)}, Val Size: {len(val_idxs)}, total: {len(train_idxs)+len(val_idxs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Apply split on X e y (separately) ---\n",
    "df_train = train_merged[train_merged['sample_index'].isin(train_idxs)]\n",
    "df_val   = train_merged[train_merged['sample_index'].isin(val_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: only features\n",
    "X_train = df_train.drop(columns=['label'])\n",
    "X_val   = df_val.drop(columns=['label'])\n",
    "\n",
    "# y: one label for each sequence\n",
    "y_train = df_train.groupby(\"sample_index\")[\"label\"].first().values\n",
    "y_val   = df_val.groupby(\"sample_index\")[\"label\"].first().values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, X_val shape: {X_val.shape}\")\n",
    "print(f\"y_train: {len(y_train)}, y_val: {len(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a99213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping once\n",
    "label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "inv_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Convert y_train/y_val from string ‚Üí int\n",
    "y_train = np.array([label_mapping[l] for l in y_train])\n",
    "y_val   = np.array([label_mapping[l] for l in y_val])\n",
    "\n",
    "# Compute label distributions\n",
    "train_counts = {inv_label_mapping[k]: np.sum(y_train == k) for k in np.unique(y_train)}\n",
    "val_counts   = {inv_label_mapping[k]: np.sum(y_val == k) for k in np.unique(y_val)}\n",
    "\n",
    "print(\"Training labels:\", train_counts)\n",
    "print(\"Validation labels:\", val_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd1a34",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6697764",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_columns = [col for col in X_train.columns if col.startswith(\"joint_\")]\n",
    "\n",
    "# calculate the minimum and maximum values from the training data only\n",
    "mins = X_train[scale_columns].min()\n",
    "maxs = X_train[scale_columns].max()\n",
    "\n",
    "# apply normalisation to the specified columns in all datasets (training and validation)\n",
    "for column in scale_columns:\n",
    "\n",
    "    # normalise the training set\n",
    "    X_train[column] = (X_train[column] - mins[column]) / (maxs[column] - mins[column])\n",
    "\n",
    "    # normalise the validation set\n",
    "    X_val[column] = (X_val[column] - mins[column]) / (maxs[column] - mins[column])\n",
    "\n",
    "    # normalise the test set\n",
    "    X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\", X_train[scale_columns].min().min(), \"‚Üí\", X_train[scale_columns].max().max())\n",
    "print(\"Val:  \", X_val[scale_columns].min().min(),   \"‚Üí\", X_val[scale_columns].max().max())\n",
    "print(\"Test: \", X_test[scale_columns].min().min(),   \"‚Üí\", X_test[scale_columns].max().max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the training DataFrame\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ff8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(df: pd.DataFrame, y: pd.DataFrame | None = None):\n",
    "    \"\"\" \n",
    "    Build full-length sequences for each sample_index.\n",
    "    Each sequence corresponds to one complete time series (160 timesteps)\n",
    "    with one label (pain level) if provided.\n",
    "\n",
    "    # N: Number of sequences\n",
    "    # T: Number of timesteps\n",
    "    # F: number of features (joints + pain surveys + static features)\n",
    "\n",
    "    Returns:\n",
    "        dataset: np.ndarray of shape (N,T,F)   \n",
    "        labels: np.ndarray of shape (N,) if y is provided, else None\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    labels = []\n",
    "\n",
    "    # Take dynamic and static feature columns\n",
    "    joint_cols  = [col for col in df.columns if col.startswith('joint_')]\n",
    "    pain_cols   = [col for col in df.columns if col.startswith('pain_survey_')]\n",
    "    static_cols = ['n_legs', 'n_hands', 'n_eyes']\n",
    "\n",
    "    # Sort by sample_index and time to preserve chronological order\n",
    "    df = df.sort_values([\"sample_index\", \"time\"])\n",
    "\n",
    "    # If labels are provided, build a lookup dictionary: sample_index ‚Üí label\n",
    "    label_dict = None\n",
    "    if y is not None:\n",
    "        # y can be a numpy array (aligned with unique sample_index order)\n",
    "        # or a DataFrame with columns ['sample_index', 'label']\n",
    "        if isinstance(y, np.ndarray):\n",
    "            # Build mapping using the unique order of sample_index in df\n",
    "            unique_ids = df[\"sample_index\"].unique()\n",
    "            label_dict = {sid: lbl for sid, lbl in zip(unique_ids, y)}\n",
    "        elif isinstance(y, pd.DataFrame):\n",
    "            label_dict = dict(zip(y[\"sample_index\"], y[\"label\"]))\n",
    "\n",
    "    # --- Group each full sequence ---\n",
    "    for sid, group in df.groupby(\"sample_index\"):\n",
    "        # --- Extract joint values (T, J) ---\n",
    "        X_joints = group[joint_cols].values.astype(np.float32)\n",
    "\n",
    "        # --- One-hot encode pain_survey_* columns (each has 3 categories: 0,1,2) ---\n",
    "        # This turns 4 integer columns into 12 binary columns (4 √ó 3 one-hot vectors)\n",
    "        pain_values   = group[pain_cols].values.astype(np.int64)  # shape (T, 4)\n",
    "        one_hot_pains = [np.eye(3, dtype=np.float32)[pain_values[:, i]] for i in range(pain_values.shape[1])]\n",
    "        X_pain        = np.concatenate(one_hot_pains, axis=1)     # shape (T, 12)\n",
    "\n",
    "        # --- Add static features (same for all timesteps but repeated) ---\n",
    "        # n_legs, n_hands, n_eyes are binary (0 or 1)\n",
    "        X_static = group[static_cols].values.astype(np.float32)   # shape (T, 3)\n",
    "\n",
    "        # --- Concatenate all feature groups along the last dimension ---\n",
    "        seq = np.concatenate([X_joints, X_pain, X_static], axis=1)  # shape (T, F_total)\n",
    "        dataset.append(seq)\n",
    "\n",
    "        # --- Retrieve the label for this sequence (if provided) ---\n",
    "        if label_dict is not None and sid in label_dict:\n",
    "            labels.append(int(label_dict[sid]))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    dataset = np.array(dataset, dtype=np.float32)          # shape (N, T, F)\n",
    "    labels  = np.array(labels, dtype=np.int64) if len(labels) > 0 else None\n",
    "\n",
    "    print(f\"Built {len(dataset)} sequences, each of shape {dataset[0].shape}\")\n",
    "    return dataset, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aebbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_df must contain sample_index + label columns\n",
    "y_train_df = pd.DataFrame({\n",
    "    \"sample_index\": X_train[\"sample_index\"].unique(),\n",
    "    \"label\": y_train\n",
    "})\n",
    "\n",
    "X_train_seq, y_train_seq = build_sequences(X_train, y_train_df)\n",
    "X_train_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_df = pd.DataFrame({\n",
    "    \"sample_index\": X_val[\"sample_index\"].unique(),\n",
    "    \"label\": y_val\n",
    "})\n",
    "\n",
    "X_val_seq, y_val_seq = build_sequences(X_val, y_val_df)\n",
    "y_val_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0245dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq, _ = build_sequences(X_test)  # no labels ‚Üí returns (dataset, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_seq.shape[1:] # extract the shape of a single sequence\n",
    "num_classes = len(np.unique(y_train)) # how many unique pain level exists\n",
    "input_shape, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n",
    "# each dataset now pairs each (T,F) TENSOR WITH ITS LABEL\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val_seq), torch.from_numpy(y_val_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef48f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size, which is the number of samples in each batch\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c461fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    # Determine optimal number of worker processes for data loading\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4,  # Load 4 batches aheads\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with different settings for each phase\n",
    "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884fa47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch from the training data loader\n",
    "for xb, yb in train_loader:\n",
    "    print(\"Features batch shape:\", xb.shape)\n",
    "    print(\"Labels batch shape:\", yb.shape)\n",
    "    break # Stop after getting one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d16b2e",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab9e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers.\n",
    "\n",
    "    This function is designed for models whose direct children are\n",
    "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to analyze.\n",
    "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store output shapes captured by forward hooks\n",
    "    output_shapes = {}\n",
    "    # List to track hook handles for later removal\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # Handle RNN layer outputs (returns a tuple)\n",
    "            if isinstance(output, tuple):\n",
    "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "\n",
    "                # Replace batch dimension (middle position) with -1\n",
    "                shape2[1] = -1\n",
    "\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "\n",
    "            # Handle standard layer outputs (e.g., Linear)\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1  # Replace batch dimension with -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    # 1. Determine the device where model parameters reside\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
    "\n",
    "    # 2. Create a dummy input tensor with batch_size=1\n",
    "    dummy_input = torch.randn(1, *input_size).to(device)\n",
    "\n",
    "    # 3. Register forward hooks on target layers\n",
    "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            # Register the hook and store its handle for cleanup\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    # 4. Execute a dummy forward pass in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            # Clean up hooks even if an error occurs\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    # 5. Remove all registered hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # --- 6. Print the summary table ---\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    # Column headers\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    # Iterate through modules again to collect and display parameter information\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            # Count total and trainable parameters for this module\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            # Format strings for display\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee00313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU).\n",
    "    Uses the last hidden state for classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Map string name to PyTorch RNN class\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        # Create the recurrent layer\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        # Calculate input size for the final classifier\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_length, input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        # LSTM returns (h_n, c_n), we only need h_n\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Reshape to (num_layers, 2, batch_size, hidden_size)\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "\n",
    "            # Concat last fwd (hidden[-1, 0, ...]) and bwd (hidden[-1, 1, ...])\n",
    "            # Final shape: (batch_size, hidden_size * 2)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            # Take the last layer's hidden state\n",
    "            # Final shape: (batch_size, hidden_size)\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        # Get logits\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c7b2a",
   "metadata": {},
   "source": [
    "## üß† **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28945707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best model tracking variables\n",
    "best_model = None\n",
    "best_performance = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25240af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    \"\"\"\n",
    "    Perform one complete training epoch through the entire training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): Lambda for L1 regularization\n",
    "        l2_lambda (float): Lambda for L2 regularization\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move data to device (GPU/CPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Clear gradients from previous step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision (if CUDA available)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Add L1 and L2 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='macro'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ff28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Perform one complete validation epoch through the entire validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        criterion (nn.Module): Loss function used to calculate validation loss\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
    "\n",
    "    Note:\n",
    "        This function automatically sets the model to evaluation mode and disables\n",
    "        gradient computation for efficiency during validation.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            # Move data to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision (if CUDA available)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='macro'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    \"\"\"\n",
    "    Log training metrics and model parameters to TensorBoard for visualization.\n",
    "\n",
    "    Args:\n",
    "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
    "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
    "        train_loss (float): Training loss for this epoch\n",
    "        train_f1 (float): Training f1 score for this epoch\n",
    "        val_loss (float): Validation loss for this epoch\n",
    "        val_f1 (float): Validation f1 score for this epoch\n",
    "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
    "\n",
    "    Note:\n",
    "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
    "        parameters and gradients, which helps monitor training progress and detect\n",
    "        issues like vanishing/exploding gradients.\n",
    "    \"\"\"\n",
    "    # Log scalar metrics\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "    # Log model parameters and gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            # Check if the tensor is not empty before adding a histogram\n",
    "            if param.numel() > 0:\n",
    "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
    "            if param.grad is not None:\n",
    "                # Check if the gradient tensor is not empty before adding a histogram\n",
    "                if param.grad.numel() > 0:\n",
    "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
    "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4550b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, epochs, train_criterion, val_criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, scheduler=None, # Added scheduler parameter\n",
    "        evaluation_metric=\"val_f1\", mode='max', \n",
    "        restore_best_weights=True, writer=None, verbose=1, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Train the neural network model on the training data and validate on the validation data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        epochs (int): Number of training epochs\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
    "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
    "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
    "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
    "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
    "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
    "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
    "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
    "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, training_history) - Trained model and metrics history\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "\n",
    "    # Configure early stopping if patience is set\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    # Main training loop: iterate through epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Forward pass through training data, compute gradients, update weights\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, train_criterion, optimizer, scaler, device\n",
    "        )\n",
    "\n",
    "        # Evaluate model on validation data without updating weights\n",
    "        if val_loader is not None:\n",
    "            val_loss, val_f1 = validate_one_epoch(model, val_loader, val_criterion, device)\n",
    "        else:\n",
    "            val_loss, val_f1 = None, None\n",
    "\n",
    "\n",
    "        # Step the scheduler if provided (typically after validation)\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_f1)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        # Store metrics for plotting and analysis\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        # Write metrics to TensorBoard for visualization\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        # Print progress every N epochs or on first epoch\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                if val_loss is not None:\n",
    "                    print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                          f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
    "                          f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                          f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f}\")\n",
    "\n",
    "\n",
    "        # Early stopping logic: monitor metric and save best model\n",
    "        if patience > 0 and val_loader is not None:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "        if patience > 0 and val_loader is None:\n",
    "            current_metric = training_history['train_loss'][-1]\n",
    "            is_improvement = (current_metric < best_metric)  # minimize train loss\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), f\"models/{experiment_name}_model.pt\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping (train loss) triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    # Restore best model weights if early stopping was used\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
    "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "\n",
    "    # Save final model if no early stopping\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa8c0c",
   "metadata": {},
   "source": [
    "## üßÆ **Network and Training Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 500\n",
    "PATIENCE = 50\n",
    "\n",
    "# Architecture\n",
    "HIDDEN_LAYERS = 2        # Hidden layers\n",
    "HIDDEN_SIZE = 128        # Neurons per layer\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.2         # Dropout probability\n",
    "L1_LAMBDA = 0            # L1 penalty\n",
    "L2_LAMBDA = 0            # L2 penalty\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# model\n",
    "MODEL='GRU'\n",
    "BIDIRECTIONAL=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1365430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and display architecture with parameter count\n",
    "rnn_model = RecurrentClassifier(\n",
    "    input_size=input_shape[-1], # Pass the number of features\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=HIDDEN_LAYERS,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    rnn_type=MODEL\n",
    "    ).to(device)\n",
    "recurrent_summary(rnn_model, input_size=input_shape)\n",
    "\n",
    "# Set up TensorBoard logging and save model architecture\n",
    "prefix = \"bi_\" if BIDIRECTIONAL else \"\"\n",
    "experiment_name = prefix + MODEL.lower()\n",
    "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
    "x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n",
    "writer.add_graph(rnn_model, x)\n",
    "\n",
    "# Define optimizer with L2 regularization\n",
    "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "# Enable mixed precision training for GPU acceleration\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94204b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Train model and track training history\n",
    "# rnn_model, training_history = fit(\n",
    "#     model=rnn_model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     epochs=EPOCHS,\n",
    "#     train_criterion=criterion,\n",
    "#     val_criterion=criterion,\n",
    "#     optimizer=optimizer,\n",
    "#     scaler=scaler,\n",
    "#     device=device,\n",
    "#     writer=writer,\n",
    "#     verbose=1,\n",
    "#     experiment_name=MODEL.lower(),\n",
    "#     patience=PATIENCE\n",
    "#     )\n",
    "\n",
    "# # Update best model if current performance is superior\n",
    "# if training_history['val_f1'][-1] > best_performance:\n",
    "#     best_model = rnn_modelbuild_se\n",
    "#     best_performance = training_history['val_f1'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f583a3",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea34aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot Hitory\n",
    "# Create a figure with two side-by-side subplots (two columns)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot of training and validation loss on the first axis\n",
    "ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot of training and validation accuracy on the second axis\n",
    "ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
    "ax2.set_title('F1 Score')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdf134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot Confusion Matrix\n",
    "# Collect predictions and ground truth labels\n",
    "val_preds, val_targets = [], []\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(device)\n",
    "\n",
    "        # Forward pass: get model predictions\n",
    "        logits = rnn_model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        # Store batch results\n",
    "        val_preds.append(preds)\n",
    "        val_targets.append(yb.numpy())\n",
    "\n",
    "# Combine all batches into single arrays\n",
    "val_preds = np.concatenate(val_preds)\n",
    "val_targets = np.concatenate(val_targets)\n",
    "\n",
    "# Calculate overall validation metrics\n",
    "val_acc = accuracy_score(val_targets, val_preds)\n",
    "val_prec = precision_score(val_targets, val_preds, average='macro')\n",
    "val_rec = recall_score(val_targets, val_preds, average='macro')\n",
    "val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n",
    "print(f\"Precision over the validation set: {val_prec:.4f}\")\n",
    "print(f\"Recall over the validation set: {val_rec:.4f}\")\n",
    "print(f\"F1 score over the validation set: {val_f1:.4f}\")\n",
    "\n",
    "# Generate confusion matrix for detailed error analysis\n",
    "cm = confusion_matrix(val_targets, val_preds)\n",
    "\n",
    "# Create numeric labels for heatmap annotation\n",
    "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
    "\n",
    "# Visualise confusion matrix\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(cm, annot=labels, fmt='',\n",
    "            cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix ‚Äî Validation Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose output directory manually\n",
    "\n",
    "# --- Kaggle ---\n",
    "# OUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "# --- Cluster (Westworld / Elysium) ---\n",
    "# OUT_DIR = \"/home/cristiano.battistini/storage/an2dl_outputs\"\n",
    "\n",
    "# --- Docker / local environment ---\n",
    "OUT_DIR = os.path.join(os.getcwd(), \"outputs\")\n",
    "\n",
    "# --- Create directory if it doesn't exist ---\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb83ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment_output(\n",
    "    model_name: str,\n",
    "    hyperparams: dict,\n",
    "    X_test_seq: np.ndarray,\n",
    "    label_mapping: dict,\n",
    "    sample_indices: list,\n",
    "    output_dir: str,\n",
    "    model=None,\n",
    "    batch_size: int = 256\n",
    "):\n",
    "    \"\"\"\n",
    "    Run inference on the test set, save predictions and hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the experiment (e.g. 'lstm', 'bilstm', 'ffn').\n",
    "        hyperparams (dict): Dict containing all hyperparameters and training config.\n",
    "        X_test_seq (np.ndarray): Test sequences of shape (N, T, F).\n",
    "        label_mapping (dict): Mapping from label string to class index.\n",
    "        sample_indices (list): List of sample_index identifiers (as strings).\n",
    "        output_dir (str): Folder where submission and metadata are saved.\n",
    "        model (torch.nn.Module): Trained model for inference.\n",
    "        batch_size (int): Inference batch size.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Reverse mapping\n",
    "    idx2label = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "    # --- Inference ---\n",
    "    model.eval().to(device)\n",
    "    with torch.inference_mode():\n",
    "        logits = []\n",
    "        for i in range(0, len(X_test_seq), batch_size):\n",
    "            xb = torch.from_numpy(X_test_seq[i:i+batch_size]).to(device)\n",
    "            logits.append(model(xb).cpu().numpy())\n",
    "        logits = np.concatenate(logits, axis=0)\n",
    "\n",
    "    pred_idx = logits.argmax(axis=1)\n",
    "    pred_labels = [idx2label[int(i)] for i in pred_idx]\n",
    "\n",
    "    # --- Build submission DataFrame ---\n",
    "    submission = pd.DataFrame({\n",
    "        \"sample_index\": [str(sid).zfill(3) for sid in sample_indices],\n",
    "        \"label\": pred_labels\n",
    "    })\n",
    "\n",
    "    # --- Build file names ---\n",
    "    run_name = f\"{model_name}_exp\"\n",
    "    csv_path = os.path.join(output_dir, f\"{run_name}_submission.csv\")\n",
    "    json_path = os.path.join(output_dir, f\"{run_name}_config.json\")\n",
    "\n",
    "    # --- Save submission ---\n",
    "    submission.to_csv(csv_path, index=False)\n",
    "\n",
    "    # --- Save hyperparameters as JSON ---\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(hyperparams, f, indent=4)\n",
    "\n",
    "    print(f\"Saved submission at: {csv_path}\")\n",
    "    print(f\"Saved hyperparameters at: {json_path}\")\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your hyperparameters as a dict\n",
    "# hyperparams = {\n",
    "#     \"m\": MODEL,\n",
    "#     \"lr\": LEARNING_RATE,\n",
    "#     \"epochs\": EPOCHS,\n",
    "#     \"pat\": PATIENCE,\n",
    "#     \"hl\": HIDDEN_LAYERS,\n",
    "#     \"hs\": HIDDEN_SIZE,\n",
    "#     \"dr\": DROPOUT_RATE,\n",
    "#     \"l1\": L1_LAMBDA,\n",
    "#     \"l2\": L2_LAMBDA,\n",
    "# }\n",
    "\n",
    "# model = best_model if \"best_model\" in globals() else rnn_model\n",
    "\n",
    "# # Run and save output\n",
    "# submission = save_experiment_output(\n",
    "#     model_name=MODEL.lower(),\n",
    "#     hyperparams=hyperparams,\n",
    "#     X_test_seq=X_test_seq,\n",
    "#     label_mapping={'no_pain': 0, 'low_pain': 1, 'high_pain': 2},\n",
    "#     sample_indices=X_test[\"sample_index\"].unique(),\n",
    "#     output_dir=OUT_DIR,\n",
    "#     model=model,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e3820",
   "metadata": {},
   "source": [
    "## **K-Shuffle-Split Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF, _ = preprocess_joints(X_TRAIN.copy())\n",
    "X_train = dataset_conversion_type(DF)\n",
    "y = Y_TRAIN.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shuffle_split_cross_validation_round_rnn(df, y, epochs, criterion, device, k, batch_size, \n",
    "                                               hidden_layers, hidden_size, learning_rate, dropout_rate, rnn_type, bidirectional,\n",
    "                            l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "                            restore_best_weights=True, writer=None, verbose=10, seed=SEED, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Perform K-fold shuffle split cross-validation with user-based splitting for time series data.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns ['user_id', 'activity', 'x_axis', 'y_axis', 'z_axis', 'id']\n",
    "        epochs: Number of training epochs\n",
    "        criterion: Loss function\n",
    "        device: torch.device for computation\n",
    "        k: Number of cross-validation splits\n",
    "        n_val_idxs: Number of indexes for validation set\n",
    "        batch_size: Batch size for training\n",
    "        hidden_layers: Number of recurrent layers\n",
    "        hidden_size: Hidden state dimensionality\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        dropout_rate: Dropout rate\n",
    "        rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
    "        bidirectional: Whether to use bidirectional RNN\n",
    "        l1_lambda: L1 regularization coefficient (if used)\n",
    "        l2_lambda: L2 regularization coefficient (weight_decay)\n",
    "        patience: Early stopping patience\n",
    "        evaluation_metric: Metric to monitor for early stopping\n",
    "        mode: 'max' or 'min' for evaluation metric\n",
    "        restore_best_weights: Whether to restore best weights after training\n",
    "        writer: TensorBoard writer\n",
    "        verbose: Verbosity level\n",
    "        seed: Random seed\n",
    "        experiment_name: Name for experiment logging\n",
    "\n",
    "    Returns:\n",
    "        fold_losses: Dict with validation losses for each split\n",
    "        fold_metrics: Dict with validation F1 scores for each split\n",
    "        best_scores: Dict with best F1 score for each split plus mean and std\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise containers for results across all splits\n",
    "    fold_losses = {}\n",
    "    fold_metrics = {}\n",
    "    best_scores = {}\n",
    "\n",
    "    # Step 1. temporary merge X_train + y_train to create splits ---\n",
    "    train_merged = df.merge(y, on=\"sample_index\")\n",
    "\n",
    "    # Step 2. Retrieve unique indexes ---\n",
    "    unique_samples = train_merged['sample_index'].unique()\n",
    "\n",
    "    num_classes = len(train_merged['label'].unique())\n",
    "\n",
    "    # Prepare stratified K-fold based on label per sample_index\n",
    "    # ---------------------------------------------------------------\n",
    "    # Extract one label per sample_index\n",
    "    label_per_sample = train_merged.groupby(\"sample_index\")[\"label\"].first().map({\n",
    "        \"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2\n",
    "    }).values\n",
    "\n",
    "    # Create stratified splitter\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    all_splits = list(skf.split(unique_samples, label_per_sample))\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    # Store initial weights to reset model for each split\n",
    "    initial_state = None\n",
    "\n",
    "    # Iterate through K random splits\n",
    "    for split_idx, (train_idx, val_idx) in enumerate(all_splits):\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"Split {split_idx+1}/{k}\")\n",
    "\n",
    "\n",
    "        # stratified split indices\n",
    "        train_idxs = unique_samples[train_idx]\n",
    "        val_idxs   = unique_samples[val_idx]\n",
    "\n",
    "\n",
    "        # Split the dataset into training, validation, and test sets based on user IDs\n",
    "        df_train = train_merged[train_merged['sample_index'].isin(train_idxs)].copy()\n",
    "        df_val = train_merged[train_merged['sample_index'].isin(val_idxs)].copy()\n",
    "\n",
    "        # X: only features\n",
    "        X_train = df_train.drop(columns=['label'])\n",
    "        X_val   = df_val.drop(columns=['label'])\n",
    "\n",
    "        # y: un‚Äôetichetta per ogni sequenza\n",
    "        y_train = df_train.groupby(\"sample_index\")[\"label\"].first().values\n",
    "        y_val   = df_val.groupby(\"sample_index\")[\"label\"].first().values\n",
    "\n",
    "        # Define mapping once\n",
    "        label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "        inv_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "        # Convert y_train/y_val from string ‚Üí int\n",
    "        y_train = np.array([label_mapping[l] for l in y_train])\n",
    "        y_val   = np.array([label_mapping[l] for l in y_val])\n",
    "\n",
    "        # Normalise features using training set statistics\n",
    "        scale_columns = [col for col in X_train.columns if col.startswith(\"joint_\")]\n",
    "\n",
    "        train_max = X_train[scale_columns].max()\n",
    "        train_min = X_train[scale_columns].min()\n",
    "\n",
    "        X_train[scale_columns] = (X_train[scale_columns] - train_min) / (train_max - train_min + 1e-8)\n",
    "        X_val[scale_columns] = (X_val[scale_columns] - train_min) / (train_max - train_min + 1e-8)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"  Training set shape: {X_train.shape}\")\n",
    "            print(f\"  Validation set shape: {X_val.shape}\")\n",
    "\n",
    "\n",
    "        y_train_df = pd.DataFrame({\n",
    "            \"sample_index\": X_train[\"sample_index\"].unique(),\n",
    "            \"label\": y_train\n",
    "        })\n",
    "\n",
    "        X_train_seq, y_train_seq = build_sequences(X_train, y_train_df)\n",
    "\n",
    "        y_val_df = pd.DataFrame({\n",
    "            \"sample_index\": X_val[\"sample_index\"].unique(),\n",
    "            \"label\": y_val\n",
    "        })\n",
    "\n",
    "        X_val_seq, y_val_seq = build_sequences(X_val, y_val_df)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"  Training sequences shape: {X_train_seq.shape}\")\n",
    "            print(f\"  Validation sequences shape: {X_val_seq.shape}\")\n",
    "\n",
    "        input_shape = X_train_seq.shape[1:] # extract the shape of a single sequence\n",
    "        num_classes = len(np.unique(y_train)) # how many unique pain level exists\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"  Input shape: {input_shape}\")\n",
    "            print(f\"  Num classes: {num_classes}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Create PyTorch datasets\n",
    "        train_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\n",
    "        val_ds   = TensorDataset(torch.from_numpy(X_val_seq), torch.from_numpy(y_val_seq))\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "        val_loader   = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "        # Initialise model architecture\n",
    "        model = RecurrentClassifier(\n",
    "            input_size=input_shape[-1],\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=hidden_layers,\n",
    "            num_classes=num_classes,\n",
    "            dropout_rate=dropout_rate,\n",
    "            bidirectional=bidirectional,\n",
    "            rnn_type=rnn_type\n",
    "        ).to(device)\n",
    "\n",
    "        # 3. save initial state at 1st split, reset in the following splits\n",
    "        if initial_state is None:\n",
    "            # the first split (split_idx == 0)\n",
    "            # save initial random weights\n",
    "            initial_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            # Questo √® uno split successivo (1, 2, ...)\n",
    "            # Resetta il modello ai pesi iniziali salvati\n",
    "            model.load_state_dict(initial_state)\n",
    "        # Define optimizer with L2 regularization\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "        # Enable mixed precision training for GPU acceleration\n",
    "        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "        # Create directory for model checkpoints\n",
    "        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
    "\n",
    "        # Train model on current split\n",
    "        model, training_history = fit(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=epochs,\n",
    "            train_criterion=criterion,\n",
    "            val_criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scaler=split_scaler,\n",
    "            device=device,\n",
    "            writer=writer,\n",
    "            patience=patience,\n",
    "            verbose=verbose,\n",
    "            evaluation_metric=evaluation_metric,\n",
    "            mode=mode,\n",
    "            restore_best_weights=restore_best_weights,\n",
    "            experiment_name=experiment_name+\"/split_\"+str(split_idx)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Store results for this split\n",
    "        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n",
    "        fold_metrics[f\"split_{split_idx}\"] = training_history['val_f1']\n",
    "        best_scores[f\"split_{split_idx}\"] = max(training_history['val_f1'])\n",
    "\n",
    "    # Compute mean and standard deviation of best scores across splits\n",
    "    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
    "    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f\"Best score: {best_scores['mean']:.4f}¬±{best_scores['std']:.4f}\")\n",
    "\n",
    "    return fold_losses, fold_metrics, best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff903bd4",
   "metadata": {},
   "source": [
    "## **Hyperparameters Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c88b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv_rnn(df, y, param_grid, fixed_params, cv_params, verbose=True):\n",
    "    \"\"\"\n",
    "    Execute grid search with K-shuffle-split cross-validation for RNN models on time series data.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns \n",
    "        param_grid: Dict of parameters to test, e.g. {'batch_size': [16, 32], 'rnn_type': ['LSTM', 'GRU']}\n",
    "        fixed_params: Dict of fixed hyperparameters (hidden_size, learning_rate, window_size, stride, etc.)\n",
    "        cv_params: Dict of CV settings (epochs, k, patience, criterion, scaler, device, etc.)\n",
    "        verbose: Print progress for each configuration\n",
    "\n",
    "    Returns:\n",
    "        results: Dict with scores for each configuration\n",
    "        best_config: Dict with best hyperparameter combination\n",
    "        best_score: Best mean F1 score achieved\n",
    "    \"\"\"\n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    combinations = list(product(*param_values))\n",
    "\n",
    "    results = {}\n",
    "    best_score = -np.inf\n",
    "    best_config = None\n",
    "\n",
    "    total = len(combinations)\n",
    "\n",
    "    for idx, combo in enumerate(combinations, 1):\n",
    "        # Create current configuration dict\n",
    "        current_config = dict(zip(param_names, combo))\n",
    "        config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nConfiguration {idx}/{total}:\")\n",
    "            for param, value in current_config.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "\n",
    "        # Merge current config with fixed parameters\n",
    "        run_params = {**fixed_params, **current_config}\n",
    "\n",
    "        # Execute cross-validation \n",
    "        # TODO: fixare\n",
    "        _, _, fold_scores = k_shuffle_split_cross_validation_round_rnn(\n",
    "            df=df,\n",
    "            y=y,\n",
    "            experiment_name=config_str,\n",
    "            **run_params,\n",
    "            **cv_params\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        results[config_str] = fold_scores\n",
    "\n",
    "        # Track best configuration\n",
    "        if fold_scores[\"mean\"] > best_score:\n",
    "            best_score = fold_scores[\"mean\"]\n",
    "            best_config = current_config.copy()\n",
    "            if verbose:\n",
    "                print(\"  NEW BEST SCORE!\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  F1 Score: {fold_scores['mean']:.4f}¬±{fold_scores['std']:.4f}\")\n",
    "\n",
    "    return results, best_config, best_score\n",
    "\n",
    "\n",
    "def plot_top_configurations_rnn(results, k_splits, top_n=5, figsize=(14, 7)):\n",
    "    \"\"\"\n",
    "    Visualise top N RNN configurations with boxplots of F1 scores across CV splits.\n",
    "\n",
    "    Args:\n",
    "        results: Dict of results from grid_search_cv_rnn\n",
    "        k_splits: Number of CV splits used\n",
    "        top_n: Number of top configurations to display\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    # Sort by mean score\n",
    "    config_scores = {name: data['mean'] for name, data in results.items()}\n",
    "    sorted_configs = sorted(config_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select top N\n",
    "    top_configs = sorted_configs[:min(top_n, len(sorted_configs))]\n",
    "\n",
    "    # Prepare boxplot data\n",
    "    boxplot_data = []\n",
    "    labels = []\n",
    "\n",
    "    # Define a dictionary for replacements, ordered to handle prefixes correctly\n",
    "    replacements = {\n",
    "        'batch_size_': 'BS=',\n",
    "        'learning_rate_': '\\nLR=',\n",
    "        'hidden_layers_': '\\nHL=',\n",
    "        'hidden_size_': '\\nHS=',\n",
    "        'dropout_rate_': '\\nDR=',\n",
    "        'rnn_type_': '\\nRNN=',\n",
    "        'bidirectional_': '\\nBIDIR=',\n",
    "        'l1_lambda_': '\\nL1=',\n",
    "        'l2_lambda_': '\\nL2='\n",
    "    }\n",
    "\n",
    "    # Replacements for separators\n",
    "    separator_replacements = {\n",
    "        '_learning_rate_': '\\nLR=',\n",
    "        '_hidden_layers_': '\\nHL=',\n",
    "        '_hidden_size_': '\\nHS=',\n",
    "        '_dropout_rate_': '\\nDR=',\n",
    "        '_rnn_type_': '\\nRNN=',\n",
    "        '_bidirectional_': '\\nBIDIR=',\n",
    "        '_l1_lambda_': '\\nL1=',\n",
    "        '_l2_lambda_': '\\nL2=',\n",
    "        '_': ''\n",
    "    }\n",
    "\n",
    "    for config_name, mean_score in top_configs:\n",
    "        # Extract best score from each split (auto-detect number of splits)\n",
    "        split_scores = []\n",
    "        for i in range(k_splits):\n",
    "            if f'split_{i}' in results[config_name]:\n",
    "                split_scores.append(results[config_name][f'split_{i}'])\n",
    "        boxplot_data.append(split_scores)\n",
    "\n",
    "        # Verify we have the expected number of splits\n",
    "        if len(split_scores) != k_splits:\n",
    "            print(f\"Warning: Config {config_name} has {len(split_scores)} splits, expected {k_splits}\")\n",
    "\n",
    "        # Create readable label using the replacements dictionary\n",
    "        readable_label = config_name\n",
    "        for old, new in replacements.items():\n",
    "            readable_label = readable_label.replace(old, new)\n",
    "\n",
    "        # Apply separator replacements\n",
    "        for old, new in separator_replacements.items():\n",
    "             readable_label = readable_label.replace(old, new)\n",
    "\n",
    "        labels.append(f\"{readable_label}\\n(Œº={mean_score:.3f})\")\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    bp = ax.boxplot(boxplot_data, labels=labels, patch_artist=True,\n",
    "                    showmeans=True, meanline=True)\n",
    "\n",
    "    # Styling\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "    # Highlight best configuration\n",
    "    ax.get_xticklabels()[0].set_fontweight('bold')\n",
    "\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_xlabel('Configuration')\n",
    "    ax.set_title(f'Top {len(top_configs)} RNN Configurations - F1 Score Distribution Across {k_splits} Splits')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "    plt.xticks(rotation=0, ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92942ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define parameters to search\n",
    "param_grid = {\n",
    "    'hidden_size': [128, 256],\n",
    "    'hidden_layers': [1, 2, 3],\n",
    "    'dropout_rate': [0.1, 0.3],\n",
    "    'learning_rate': [5e-4],\n",
    "    'rnn_type': ['GRU', 'LSTM'],\n",
    "    'bidirectional': [False],\n",
    "}\n",
    "\n",
    "# Fixed hyperparameters (not being tuned)\n",
    "fixed_params = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'hidden_layers': HIDDEN_LAYERS,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'dropout_rate': DROPOUT_RATE,\n",
    "    'l1_lambda': L1_LAMBDA,\n",
    "    'l2_lambda': L2_LAMBDA,\n",
    "    'rnn_type': RNN_TYPE,\n",
    "    'bidirectional': BIDIRECTIONAL\n",
    "}\n",
    "\n",
    "# Cross-validation settings\n",
    "cv_params = {\n",
    "    'epochs': EPOCHS,\n",
    "    'criterion': criterion,\n",
    "    'device': device,\n",
    "    'k': K,\n",
    "    'patience': PATIENCE,\n",
    "    'verbose': 0,\n",
    "    'seed': SEED\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e71877",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF, _ = preprocess_joints(X_TRAIN.copy())\n",
    "X_train = dataset_conversion_type(DF)\n",
    "y = Y_TRAIN.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute search\n",
    "results, best_config, best_score = grid_search_cv_rnn(\n",
    "    df=X_train,\n",
    "    y=y,\n",
    "    param_grid=param_grid,\n",
    "    fixed_params=fixed_params,\n",
    "    cv_params=cv_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b198321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "plot_top_configurations_rnn(results, k_splits=K, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b9e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL TRAINING ON THE FULL DATASET + SUBMISSION GENERATION\n",
    "# ============================================================\n",
    "\n",
    "# 1. Preprocess full training data\n",
    "DF, _ = preprocess_joints(X_TRAIN.copy())\n",
    "X_train_full = dataset_conversion_type(DF)\n",
    "y_full = Y_TRAIN.copy()\n",
    "\n",
    "# 2. Combine the best hyperparameters (found in grid search)\n",
    "final_best_params = {**fixed_params, **best_config}\n",
    "print(\"Training final model with best configuration:\")\n",
    "for k, v in final_best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# 3. Merge features and labels\n",
    "train_merged = X_train_full.merge(y_full, on=\"sample_index\")\n",
    "\n",
    "# 4. Encode labels numerically BEFORE building sequences\n",
    "label_mapping = {\"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2}\n",
    "train_merged[\"label\"] = train_merged[\"label\"].map(label_mapping)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Normalise feature values\n",
    "\n",
    "scale_columns = [col for col in train_merged.columns if col.startswith(\"joint_\")]\n",
    "# calculate the minimum and maximum values from the training data only\n",
    "mins = X_train[scale_columns].min()\n",
    "maxs = X_train[scale_columns].max()\n",
    "\n",
    "# apply normalisation to the specified columns in all datasets (training and validation)\n",
    "for column in scale_columns:\n",
    "\n",
    "    # normalise the training set\n",
    "    train_merged[column] = (train_merged[column] - mins[column]) / (maxs[column] - mins[column])\n",
    "\n",
    "# 6. Build full sequences\n",
    "X_train_seq, y_train_seq = build_sequences(train_merged, train_merged[[\"sample_index\", \"label\"]])\n",
    "\n",
    "\n",
    "# 7. DataLoader\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\n",
    "train_loader = make_loader(train_ds, batch_size=final_best_params[\"batch_size\"], shuffle=True, drop_last=False)\n",
    "\n",
    "# 8. Initialize model with tuned hyperparameters\n",
    "model = RecurrentClassifier(\n",
    "    input_size=X_train_seq.shape[2],\n",
    "    hidden_size=final_best_params[\"hidden_size\"],\n",
    "    num_layers=final_best_params[\"hidden_layers\"],\n",
    "    num_classes=len(label_mapping),\n",
    "    dropout_rate=final_best_params[\"dropout_rate\"],\n",
    "    bidirectional=final_best_params[\"bidirectional\"],\n",
    "    rnn_type=final_best_params[\"rnn_type\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=final_best_params[\"learning_rate\"])\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "# 9. Train model on the entire dataset\n",
    "model, history = fit(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=None,\n",
    "    epochs=EPOCHS,\n",
    "    train_criterion=criterion,\n",
    "    val_criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    patience=PATIENCE,\n",
    "    verbose=True,\n",
    "    evaluation_metric=\"val_f1\",  # ignored since no validation\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=False,\n",
    "    experiment_name=\"final_full_train\"\n",
    ")\n",
    "\n",
    "# 10. Prepare test set for inference\n",
    "X_test = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\n",
    "for column in scale_columns:\n",
    "\n",
    "    # normalise the training set\n",
    "    X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])\n",
    "X_test_seq, _ = build_sequences(X_test)\n",
    "\n",
    "# 11. Predict on test set\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    logits = model(torch.from_numpy(X_test_seq).to(device))\n",
    "    preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "# 12. Save predictions and configuration\n",
    "OUT_DIR = \"results_best_model\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "hyperparams = final_best_params.copy()\n",
    "hyperparams.update({\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"patience\": PATIENCE,\n",
    "    \"best_cv_f1\": best_score\n",
    "})\n",
    "\n",
    "submission = save_experiment_output(\n",
    "    model_name=final_best_params[\"rnn_type\"].lower(),\n",
    "    hyperparams=hyperparams,\n",
    "    X_test_seq=X_test_seq,\n",
    "    label_mapping=label_mapping,\n",
    "    output_dir=OUT_DIR,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Final model trained and submission saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
