{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13644638,
          "sourceType": "datasetVersion",
          "datasetId": 8673684
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Artificial Neural Networks and Deep Learning**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7QqT4hHxvylV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ **Libraries Import**"
      ],
      "metadata": {
        "id": "9ytR-7lYvylW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:10.428186Z",
          "iopub.execute_input": "2025-11-14T13:53:10.428459Z",
          "iopub.status.idle": "2025-11-14T13:53:10.432914Z",
          "shell.execute_reply.started": "2025-11-14T13:53:10.428434Z",
          "shell.execute_reply": "2025-11-14T13:53:10.432155Z"
        },
        "id": "UnnKI8_OvylW"
      },
      "outputs": [],
      "execution_count": 66
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gciqx5QBv2wh",
        "outputId": "5e34ff95-8e58-41ed-c360-16d0bd18ecc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logs_dir = \"tensorboard\"\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "!mkdir -p models\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import copy\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:10.443477Z",
          "iopub.execute_input": "2025-11-14T13:53:10.443696Z",
          "iopub.status.idle": "2025-11-14T13:53:10.905316Z",
          "shell.execute_reply.started": "2025-11-14T13:53:10.443679Z",
          "shell.execute_reply": "2025-11-14T13:53:10.904331Z"
        },
        "id": "H03ZqAA_vylX",
        "outputId": "78cce59c-a4d4-442e-c78c-729c14570809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "execution_count": 68
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â³ **Data Loading**"
      ],
      "metadata": {
        "id": "Kzr-FtsCvylX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ROOT = Path(\"./dataset\")\n",
        "\n",
        "# --- 2ï¸âƒ£ Kaggle ---\n",
        "DATASET_ROOT = Path(\"/kaggle/input/pirate-pain\")\n",
        "DATASET_ROOT = Path(\"/content/drive/MyDrive/pirate-pain\")\n",
        "\n",
        "\n",
        "# --- 3ï¸âƒ£ Server o cluster privato (es. Westworld/Elysium) ---\n",
        "# DATASET_ROOT = Path(\"/multiverse/datasets/private_dataset/pirate_pain\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:10.907080Z",
          "iopub.execute_input": "2025-11-14T13:53:10.907338Z",
          "iopub.status.idle": "2025-11-14T13:53:10.911707Z",
          "shell.execute_reply.started": "2025-11-14T13:53:10.907314Z",
          "shell.execute_reply": "2025-11-14T13:53:10.910865Z"
        },
        "id": "4IcYbGcbvylY"
      },
      "outputs": [],
      "execution_count": 69
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Caricamento dati\n",
        "X_train = pd.read_csv(DATASET_ROOT / \"pirate_pain_train.csv\")\n",
        "X_TRAIN = pd.read_csv(DATASET_ROOT / \"pirate_pain_train.csv\")\n",
        "\n",
        "y_train = pd.read_csv(DATASET_ROOT / \"pirate_pain_train_labels.csv\")\n",
        "Y_TRAIN = pd.read_csv(DATASET_ROOT / \"pirate_pain_train_labels.csv\")\n",
        "\n",
        "X_test  = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\n",
        "\n",
        "print(f\"  X_train: {X_train.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"  X_test:  {X_test.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:10.912470Z",
          "iopub.execute_input": "2025-11-14T13:53:10.912731Z",
          "iopub.status.idle": "2025-11-14T13:53:16.015941Z",
          "shell.execute_reply.started": "2025-11-14T13:53:10.912704Z",
          "shell.execute_reply": "2025-11-14T13:53:16.015169Z"
        },
        "id": "09LXqmh6vylY",
        "outputId": "00d0248d-d153-4e63-c328-bbbdda1e7641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  X_train: (105760, 40)\n",
            "  y_train: (661, 2)\n",
            "  X_test:  (211840, 40)\n"
          ]
        }
      ],
      "execution_count": 70
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”Ž **Exploration and Data Analysis**"
      ],
      "metadata": {
        "id": "7iu82rpZvylY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MERGE BETWEEN TRAIN DATA AND LABELS\n",
        "# the labels are in a separated file linked through 'sample_index\n",
        "# here we merge X_train and y_train in a unique Dataframe to explore\n",
        "\n",
        "train_merge = X_train.merge(y_train, on=\"sample_index\", how=\"left\")\n",
        "\n",
        "# check whether all the labels have been associated or not\n",
        "missing_labels = train_merge[\"label\"].isna().sum()\n",
        "if missing_labels > 0:\n",
        "    print(f\"{missing_labels} rows without a label\")\n",
        "\n",
        "# check\n",
        "print(train_merge[[\"sample_index\",\"time\",\"label\"]].head())\n",
        "print(\"Class Distribution\")\n",
        "print(train_merge[\"label\"].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.017895Z",
          "iopub.execute_input": "2025-11-14T13:53:16.018204Z",
          "iopub.status.idle": "2025-11-14T13:53:16.078469Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.018185Z",
          "shell.execute_reply": "2025-11-14T13:53:16.077857Z"
        },
        "id": "NynYCjV4vylY",
        "outputId": "6837230d-c751-4e74-c351-22749de39722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sample_index  time    label\n",
            "0             0     0  no_pain\n",
            "1             0     1  no_pain\n",
            "2             0     2  no_pain\n",
            "3             0     3  no_pain\n",
            "4             0     4  no_pain\n",
            "Class Distribution\n",
            "label\n",
            "no_pain      81760\n",
            "low_pain     15040\n",
            "high_pain     8960\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "execution_count": 71
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[\"label\"].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.079161Z",
          "iopub.execute_input": "2025-11-14T13:53:16.079476Z",
          "iopub.status.idle": "2025-11-14T13:53:16.084474Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.079458Z",
          "shell.execute_reply": "2025-11-14T13:53:16.083795Z"
        },
        "id": "p7FvLzSQvylY",
        "outputId": "b38b7dad-77ef-4548-934d-a74aafc42953",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "no_pain      511\n",
            "low_pain      94\n",
            "high_pain     56\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "execution_count": 72
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”„ **Data Preprocessing**"
      ],
      "metadata": {
        "id": "LFNaiPCFvylY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_conversion_type_embed_ready(df):\n",
        "    \"\"\"\n",
        "    Minimal, embedding-friendly preprocessing:\n",
        "    - joints: float32 (continuous features)\n",
        "    - pain_survey_*: int64 indices (0..2) for embeddings\n",
        "    - n_legs/hands/eyes: mapped to {0,1} as int64 for embeddings\n",
        "    Returns: df, meta\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1) continuous features\n",
        "    joint_cols = [c for c in df.columns if c.startswith(\"joint_\")]\n",
        "    df[joint_cols] = df[joint_cols].astype(\"float32\")\n",
        "\n",
        "    # 2) surveys as categorical indices (already 0/1/2)\n",
        "    pain_survey_cols = [c for c in df.columns if c.startswith(\"pain_survey_\")]\n",
        "    df[pain_survey_cols] = df[pain_survey_cols].astype(\"int64\")\n",
        "\n",
        "    # 3) 2-way categoricals â†’ indices\n",
        "    legs_map  = {\"two\": 0, \"one+peg_leg\": 1}\n",
        "    hands_map = {\"two\": 0, \"one+hook_hand\": 1}\n",
        "    eyes_map  = {\"two\": 0, \"one+eye_patch\": 1}\n",
        "\n",
        "    if \"n_legs\" in df.columns:\n",
        "        df[\"n_legs\"]  = df[\"n_legs\"].map(legs_map).astype(\"int64\")\n",
        "    if \"n_hands\" in df.columns:\n",
        "        df[\"n_hands\"] = df[\"n_hands\"].map(hands_map).astype(\"int64\")\n",
        "    if \"n_eyes\" in df.columns:\n",
        "        df[\"n_eyes\"]  = df[\"n_eyes\"].map(eyes_map).astype(\"int64\")\n",
        "\n",
        "    # 4) define columns\n",
        "    cat_two_cols = [c for c in [\"n_legs\",\"n_hands\",\"n_eyes\"] if c in df.columns]\n",
        "    cat_cols = pain_survey_cols + cat_two_cols\n",
        "    cont_cols = joint_cols  # keep only joints as continuous\n",
        "\n",
        "    # 5) cardinals for embeddings (compute on TRAIN ONLY in CV, reuse for VAL/TEST)\n",
        "    cardinals = {c: int(df[c].nunique()) for c in cat_cols}\n",
        "    # suggested tiny dims: 1 for binaries, 2 for 3-class surveys\n",
        "    emb_dims = {c: (1 if cardinals[c] == 2 else 2) for c in cat_cols}\n",
        "\n",
        "    meta = {\n",
        "        \"cont_cols\": cont_cols,\n",
        "        \"cat_cols\":  cat_cols,\n",
        "        \"cardinals\": cardinals,\n",
        "        \"emb_dims\":  emb_dims,\n",
        "        \"maps\": {\"n_legs\": legs_map, \"n_hands\": hands_map, \"n_eyes\": eyes_map},\n",
        "    }\n",
        "    return df, meta\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.085358Z",
          "iopub.execute_input": "2025-11-14T13:53:16.085752Z",
          "iopub.status.idle": "2025-11-14T13:53:16.099791Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.085733Z",
          "shell.execute_reply": "2025-11-14T13:53:16.099270Z"
        },
        "id": "uRJrsXUWvylY"
      },
      "outputs": [],
      "execution_count": 73
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_joints(df,\n",
        "                      drop_redundant=False,\n",
        "                      drop_near_zero=False,\n",
        "                      drop_low_var=False,\n",
        "                      verbose=True):\n",
        "    \"\"\"\n",
        "    Simplify joint_* preprocessing based on EDA results.\n",
        "    Removes constant, redundant, or near-zero-variance joints.\n",
        "\n",
        "    Returns a (df_out, feature_cols) tuple.\n",
        "    \"\"\"\n",
        "    joint_cols = sorted([c for c in df.columns if c.startswith(\"joint_\")],\n",
        "                        key=lambda x: int(x.split(\"_\")[1]))\n",
        "    drop = set()\n",
        "\n",
        "    # 1 Drop constant joint_30\n",
        "    if \"joint_30\" in joint_cols:\n",
        "        drop.add(\"joint_30\")\n",
        "\n",
        "    #  Drop redundant joints (from correlation heatmap)\n",
        "    if drop_redundant:\n",
        "        for c in [\"joint_01\", \"joint_02\", \"joint_05\"]:\n",
        "            if c in joint_cols:\n",
        "                drop.add(c)\n",
        "\n",
        "    # Drop near-zero variance joints (joint_13â€“25)\n",
        "    if drop_near_zero:\n",
        "        for i in range(13, 26):\n",
        "            c = f\"joint_{i:02d}\"\n",
        "            if c in joint_cols:\n",
        "                drop.add(c)\n",
        "\n",
        "    # (Optional) Drop low-variance but not-zero joints (joint_26â€“29)\n",
        "    if drop_low_var:\n",
        "        for i in range(26, 30):\n",
        "            c = f\"joint_{i:02d}\"\n",
        "            if c in joint_cols:\n",
        "                drop.add(c)\n",
        "\n",
        "    # apply\n",
        "    kept = [c for c in joint_cols if c not in drop]\n",
        "    df_out = df.drop(columns=list(drop), errors=\"ignore\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[preprocess_joints] start={len(joint_cols)} | kept={len(kept)} | dropped={len(drop)}\")\n",
        "        if drop:\n",
        "            print(\"  â€¢ dropped:\", sorted(list(drop)))\n",
        "\n",
        "    return df_out, kept\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.100722Z",
          "iopub.execute_input": "2025-11-14T13:53:16.100988Z",
          "iopub.status.idle": "2025-11-14T13:53:16.118534Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.100964Z",
          "shell.execute_reply": "2025-11-14T13:53:16.118003Z"
        },
        "id": "IteRlov3vylZ"
      },
      "outputs": [],
      "execution_count": 74
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, _ = preprocess_joints(X_train)\n",
        "X_test, _ = preprocess_joints(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.119314Z",
          "iopub.execute_input": "2025-11-14T13:53:16.119525Z",
          "iopub.status.idle": "2025-11-14T13:53:16.176450Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.119511Z",
          "shell.execute_reply": "2025-11-14T13:53:16.175804Z"
        },
        "id": "UuTrxjCmvylZ",
        "outputId": "266a4065-384f-4c46-ab2a-99ea09da3173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[preprocess_joints] start=31 | kept=30 | dropped=1\n",
            "  â€¢ dropped: ['joint_30']\n",
            "[preprocess_joints] start=31 | kept=30 | dropped=1\n",
            "  â€¢ dropped: ['joint_30']\n"
          ]
        }
      ],
      "execution_count": 75
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”„ **Data Preprocessing**"
      ],
      "metadata": {
        "id": "wTYIyyCivylZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Fit preprocessing on TRAIN ONLY\n",
        "X_train, meta = dataset_conversion_type_embed_ready(X_train)\n",
        "# BEFORE calling build_sequences, check your DataFrame\n",
        "print(\"\\n=== PREPROCESSING CHECK ===\")\n",
        "pain_cols = [c for c in X_train.columns if c.startswith('pain_survey_')]\n",
        "print(f\"Pain columns in X_train: {pain_cols}\")\n",
        "print(f\"Pain column dtypes: {X_train[pain_cols].dtypes}\")\n",
        "print(f\"Pain unique values:\")\n",
        "for col in pain_cols:\n",
        "    print(f\"  {col}: {X_train[col].unique()}\")\n",
        "\n",
        "static_cols = ['n_legs', 'n_hands', 'n_eyes']\n",
        "print(f\"\\nStatic columns in X_train: {[c for c in static_cols if c in X_train.columns]}\")\n",
        "for col in static_cols:\n",
        "    if col in X_train.columns:\n",
        "        print(f\"  {col} dtype: {X_train[col].dtype}\")\n",
        "        print(f\"  {col} unique: {X_train[col].unique()}\")\n",
        "print(\"===========================\\n\")\n",
        "\n",
        "# 2) Apply the SAME mappings/cardinals to TEST\n",
        "#    (pain_survey_* are already 0/1/2; for n_legs/hands/eyes we reuse meta[\"maps\"])\n",
        "X_test = X_test.copy()\n",
        "for c, m in meta[\"maps\"].items():\n",
        "    if c in X_test.columns:\n",
        "        X_test[c] = X_test[c].map(m).astype(\"int64\")\n",
        "\n",
        "# Cast types consistently with train\n",
        "X_test[meta[\"cont_cols\"]] = X_test[meta[\"cont_cols\"]].astype(\"float32\")\n",
        "for c in meta[\"cat_cols\"]:\n",
        "    X_test[c] = X_test[c].astype(\"int64\")\n",
        "\n",
        "# 3) Sanity checks\n",
        "print(\"Train cont/cat:\", len(meta[\"cont_cols\"]), len(meta[\"cat_cols\"]))\n",
        "print(\"Train cont cols:\", meta[\"cont_cols\"][:5], \"â€¦\")\n",
        "print(\"Train cat  cols:\", meta[\"cat_cols\"])\n",
        "print(\"Test has all cont cols?\", set(meta[\"cont_cols\"]).issubset(X_test.columns))\n",
        "print(\"Test has all cat  cols?\", set(meta[\"cat_cols\"]).issubset(X_test.columns))\n",
        "\n",
        "# Optional: verify cardinalities didnâ€™t explode on test (should be â‰¤ train)\n",
        "for c in meta[\"cat_cols\"]:\n",
        "    tr_card = meta[\"cardinals\"][c]\n",
        "    te_card = int(X_test[c].nunique())\n",
        "    if te_card > tr_card:\n",
        "        print(f\"WARNING: column {c} has unseen categories in TEST (train={tr_card}, test={te_card})\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.177309Z",
          "iopub.execute_input": "2025-11-14T13:53:16.178001Z",
          "iopub.status.idle": "2025-11-14T13:53:16.372299Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.177973Z",
          "shell.execute_reply": "2025-11-14T13:53:16.371207Z"
        },
        "id": "SnSxZADyvylZ",
        "outputId": "57c5dcde-c1ff-421b-e485-aab2c35c5fa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PREPROCESSING CHECK ===\n",
            "Pain columns in X_train: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
            "Pain column dtypes: pain_survey_1    int64\n",
            "pain_survey_2    int64\n",
            "pain_survey_3    int64\n",
            "pain_survey_4    int64\n",
            "dtype: object\n",
            "Pain unique values:\n",
            "  pain_survey_1: [2 0 1]\n",
            "  pain_survey_2: [0 2 1]\n",
            "  pain_survey_3: [2 0 1]\n",
            "  pain_survey_4: [1 2 0]\n",
            "\n",
            "Static columns in X_train: ['n_legs', 'n_hands', 'n_eyes']\n",
            "  n_legs dtype: int64\n",
            "  n_legs unique: [0 1]\n",
            "  n_hands dtype: int64\n",
            "  n_hands unique: [0 1]\n",
            "  n_eyes dtype: int64\n",
            "  n_eyes unique: [0 1]\n",
            "===========================\n",
            "\n",
            "Train cont/cat: 30 7\n",
            "Train cont cols: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04'] â€¦\n",
            "Train cat  cols: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes']\n",
            "Test has all cont cols? True\n",
            "Test has all cat  cols? True\n"
          ]
        }
      ],
      "execution_count": 76
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 â€” Copy merged train and raw test\n",
        "train_dataset = X_train.copy()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.375690Z",
          "iopub.execute_input": "2025-11-14T13:53:16.376086Z",
          "iopub.status.idle": "2025-11-14T13:53:16.403712Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.376053Z",
          "shell.execute_reply": "2025-11-14T13:53:16.402656Z"
        },
        "id": "sluq_OlKvylZ"
      },
      "outputs": [],
      "execution_count": 77
    },
    {
      "cell_type": "code",
      "source": [
        "# to reload quickly\n",
        "X_train = train_dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.404780Z",
          "iopub.execute_input": "2025-11-14T13:53:16.405097Z",
          "iopub.status.idle": "2025-11-14T13:53:16.409281Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.405072Z",
          "shell.execute_reply": "2025-11-14T13:53:16.408472Z"
        },
        "id": "Wffuno3ivylZ"
      },
      "outputs": [],
      "execution_count": 78
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train columns:\", X_train.columns.tolist())\n",
        "print(\"Test columns:\", X_test.columns.tolist())\n",
        "\n",
        "train_only = [c for c in X_train.columns if c not in X_test.columns]\n",
        "test_only  = [c for c in X_test.columns if c not in X_train.columns]\n",
        "\n",
        "if train_only or test_only:\n",
        "    print(\"Column mismatch detected!\")\n",
        "    if train_only:\n",
        "        print(\"  Present only in TRAIN:\", train_only)\n",
        "    if test_only:\n",
        "        print(\"  Present only in TEST:\", test_only)\n",
        "else:\n",
        "    print(\"âœ… Train and Test have identical columns.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.410105Z",
          "iopub.execute_input": "2025-11-14T13:53:16.410369Z",
          "iopub.status.idle": "2025-11-14T13:53:16.437193Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.410351Z",
          "shell.execute_reply": "2025-11-14T13:53:16.436437Z"
        },
        "id": "DK1nC47lvylZ",
        "outputId": "e74a1f9e-d3ec-49f0-c9dc-89146b67935c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train columns: ['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
            "Test columns: ['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\n",
            "âœ… Train and Test have identical columns.\n"
          ]
        }
      ],
      "execution_count": 79
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1. temporary merge X_train + y_train to create splits ---\n",
        "train_merged = X_train.merge(y_train, on=\"sample_index\")\n",
        "print(train_merged.shape)\n",
        "\n",
        "# Step 2. retrieve unique indexes and labels to stratify ---\n",
        "unique_samples = train_merged['sample_index'].unique()\n",
        "y_seq = train_merged.groupby('sample_index')['label'].first().reindex(unique_samples).values\n",
        "\n",
        "# Step 3. Divide in train e val (stratified) ---\n",
        "\n",
        "train_idxs, val_idxs = train_test_split(unique_samples, test_size=0.20, random_state=SEED, stratify=y_seq)\n",
        "print(f\"Train Size: {len(train_idxs)}, Val Size: {len(val_idxs)}, total: {len(train_idxs)+len(val_idxs)}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.438220Z",
          "iopub.execute_input": "2025-11-14T13:53:16.438563Z",
          "iopub.status.idle": "2025-11-14T13:53:16.478600Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.438536Z",
          "shell.execute_reply": "2025-11-14T13:53:16.477956Z"
        },
        "id": "jZRet8f_vyla",
        "outputId": "c36449b5-6880-444b-b947-422e51d1a56a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(105760, 40)\n",
            "Train Size: 528, Val Size: 133, total: 661\n"
          ]
        }
      ],
      "execution_count": 80
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4. Apply split on X e y (separately) ---\n",
        "df_train = train_merged[train_merged['sample_index'].isin(train_idxs)]\n",
        "df_val   = train_merged[train_merged['sample_index'].isin(val_idxs)]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.479397Z",
          "iopub.execute_input": "2025-11-14T13:53:16.479670Z",
          "iopub.status.idle": "2025-11-14T13:53:16.495541Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.479651Z",
          "shell.execute_reply": "2025-11-14T13:53:16.495011Z"
        },
        "id": "i8BrUh_Dvyla"
      },
      "outputs": [],
      "execution_count": 81
    },
    {
      "cell_type": "code",
      "source": [
        "# X: only features\n",
        "X_train = df_train.drop(columns=['label'])\n",
        "X_val   = df_val.drop(columns=['label'])\n",
        "\n",
        "# y: one label for each sequence\n",
        "y_train = df_train.groupby(\"sample_index\")[\"label\"].first().values\n",
        "y_val   = df_val.groupby(\"sample_index\")[\"label\"].first().values\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, X_val shape: {X_val.shape}\")\n",
        "print(f\"y_train: {len(y_train)}, y_val: {len(y_val)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.496148Z",
          "iopub.execute_input": "2025-11-14T13:53:16.496341Z",
          "iopub.status.idle": "2025-11-14T13:53:16.517216Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.496325Z",
          "shell.execute_reply": "2025-11-14T13:53:16.516389Z"
        },
        "id": "46-AppBhvyla",
        "outputId": "4637793d-b2c5-44c4-a560-769e185ac6b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (84480, 39), X_val shape: (21280, 39)\n",
            "y_train: 528, y_val: 133\n"
          ]
        }
      ],
      "execution_count": 82
    },
    {
      "cell_type": "code",
      "source": [
        "# Define mapping once\n",
        "label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
        "inv_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "# Convert y_train/y_val from string â†’ int\n",
        "y_train = np.array([label_mapping[l] for l in y_train])\n",
        "y_val   = np.array([label_mapping[l] for l in y_val])\n",
        "\n",
        "# Compute label distributions\n",
        "train_counts = {inv_label_mapping[k]: np.sum(y_train == k) for k in np.unique(y_train)}\n",
        "val_counts   = {inv_label_mapping[k]: np.sum(y_val == k) for k in np.unique(y_val)}\n",
        "\n",
        "print(\"Training labels:\", train_counts)\n",
        "print(\"Validation labels:\", val_counts)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.518262Z",
          "iopub.execute_input": "2025-11-14T13:53:16.518520Z",
          "iopub.status.idle": "2025-11-14T13:53:16.526332Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.518489Z",
          "shell.execute_reply": "2025-11-14T13:53:16.525353Z"
        },
        "id": "tHsaNnczvyla",
        "outputId": "4a52f4d4-cd92-4b2f-8b0d-569c4745a2da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training labels: {'no_pain': np.int64(408), 'low_pain': np.int64(75), 'high_pain': np.int64(45)}\n",
            "Validation labels: {'no_pain': np.int64(103), 'low_pain': np.int64(19), 'high_pain': np.int64(11)}\n"
          ]
        }
      ],
      "execution_count": 83
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalisation"
      ],
      "metadata": {
        "id": "15v7nKMUvyla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scale_columns = [col for col in X_train.columns if col.startswith(\"joint_\")]\n",
        "\n",
        "# calculate the minimum and maximum values from the training data only\n",
        "mins = X_train[scale_columns].min()\n",
        "maxs = X_train[scale_columns].max()\n",
        "\n",
        "# apply normalisation to the specified columns in all datasets (training and validation)\n",
        "for column in scale_columns:\n",
        "\n",
        "    # normalise the training set\n",
        "    X_train[column] = (X_train[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "    # normalise the validation set\n",
        "    X_val[column] = (X_val[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "    # normalise the test set\n",
        "    X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.527228Z",
          "iopub.execute_input": "2025-11-14T13:53:16.527494Z",
          "iopub.status.idle": "2025-11-14T13:53:16.616738Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.527470Z",
          "shell.execute_reply": "2025-11-14T13:53:16.616000Z"
        },
        "id": "pa_b4U2Fvyla"
      },
      "outputs": [],
      "execution_count": 84
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train:\", X_train[scale_columns].min().min(), \"â†’\", X_train[scale_columns].max().max())\n",
        "print(\"Val:  \", X_val[scale_columns].min().min(),   \"â†’\", X_val[scale_columns].max().max())\n",
        "print(\"Test: \", X_test[scale_columns].min().min(),   \"â†’\", X_test[scale_columns].max().max())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.617277Z",
          "iopub.execute_input": "2025-11-14T13:53:16.617459Z",
          "iopub.status.idle": "2025-11-14T13:53:16.721639Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.617444Z",
          "shell.execute_reply": "2025-11-14T13:53:16.720797Z"
        },
        "id": "L9NPAaA3vyla",
        "outputId": "743af1c3-ad58-4776-fc2a-b8e4d26a69da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 0.0 â†’ 1.0\n",
            "Val:   -0.007747788447886705 â†’ 7.217308044433594\n",
            "Test:  -0.07219822704792023 â†’ 4.829583644866943\n"
          ]
        }
      ],
      "execution_count": 85
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first five rows of the training DataFrame\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "\n",
        "X_train, y_train"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.722444Z",
          "iopub.execute_input": "2025-11-14T13:53:16.722736Z",
          "iopub.status.idle": "2025-11-14T13:53:16.744577Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.722713Z",
          "shell.execute_reply": "2025-11-14T13:53:16.743987Z"
        },
        "id": "WIf-NhAkvyla",
        "outputId": "ae136592-7526-48ae-f642-edd6449fe5e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(84480, 39) (528,)\n",
            "(21280, 39) (133,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(        sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n",
              " 0                  0     0              2              0              2   \n",
              " 1                  0     1              2              2              2   \n",
              " 2                  0     2              2              0              2   \n",
              " 3                  0     3              2              2              2   \n",
              " 4                  0     4              2              2              2   \n",
              " ...              ...   ...            ...            ...            ...   \n",
              " 105755           660   155              2              2              0   \n",
              " 105756           660   156              2              2              0   \n",
              " 105757           660   157              0              2              2   \n",
              " 105758           660   158              2              2              2   \n",
              " 105759           660   159              2              2              2   \n",
              " \n",
              "         pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...  joint_20  \\\n",
              " 0                   1       0        0       0  0.777046  ...  0.000001   \n",
              " 1                   2       0        0       0  0.805855  ...  0.000004   \n",
              " 2                   2       0        0       0  0.767110  ...  0.000001   \n",
              " 3                   2       0        0       0  0.665528  ...  0.000071   \n",
              " 4                   2       0        0       0  0.773829  ...  0.000039   \n",
              " ...               ...     ...      ...     ...       ...  ...       ...   \n",
              " 105755              0       0        0       0  0.746465  ...  0.000000   \n",
              " 105756              2       0        0       0  0.729322  ...  0.000000   \n",
              " 105757              2       0        0       0  0.790338  ...  0.000000   \n",
              " 105758              2       0        0       0  0.750993  ...  0.000000   \n",
              " 105759              0       0        0       0  0.715698  ...  0.000007   \n",
              " \n",
              "             joint_21      joint_22      joint_23  joint_24  joint_25  \\\n",
              " 0       2.426544e-06  1.503263e-06  1.052579e-04  0.000405  0.000004   \n",
              " 1       2.757563e-07  4.403064e-07  1.584209e-04  0.000001  0.000000   \n",
              " 2       1.063529e-07  1.575589e-08  3.805627e-05  0.000085  0.000003   \n",
              " 3       6.981461e-06  3.352260e-07  4.862392e-05  0.000002  0.000000   \n",
              " 4       3.076737e-06  1.885071e-08  4.086358e-05  0.000002  0.000007   \n",
              " ...              ...           ...           ...       ...       ...   \n",
              " 105755  0.000000e+00  1.739834e-07  7.708907e-08  0.000013  0.000001   \n",
              " 105756  1.010686e-06  1.741445e-07  0.000000e+00  0.000007  0.000004   \n",
              " 105757  0.000000e+00  1.742954e-07  6.375097e-05  0.000007  0.000007   \n",
              " 105758  1.210643e-07  1.744361e-07  1.594986e-05  0.000007  0.000001   \n",
              " 105759  4.086062e-07  1.745665e-07  0.000000e+00  0.000085  0.000001   \n",
              " \n",
              "         joint_26  joint_27  joint_28  joint_29  \n",
              " 0       0.014214  0.011376  0.018978  0.020291  \n",
              " 1       0.010748  0.000000  0.009473  0.010006  \n",
              " 2       0.013097  0.006830  0.017065  0.016856  \n",
              " 3       0.009505  0.006274  0.020264  0.017981  \n",
              " 4       0.004216  0.002132  0.023389  0.018477  \n",
              " ...          ...       ...       ...       ...  \n",
              " 105755  0.006255  0.022634  0.122919  0.161896  \n",
              " 105756  0.021736  0.010761  0.053784  0.085181  \n",
              " 105757  0.030063  0.023592  0.053808  0.057150  \n",
              " 105758  0.037765  0.015093  0.068772  0.077918  \n",
              " 105759  0.027207  0.035294  0.060020  0.119300  \n",
              " \n",
              " [84480 rows x 39 columns],\n",
              " array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0,\n",
              "        0, 2, 0, 0, 1, 1, 2, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2,\n",
              "        0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0,\n",
              "        1, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "        0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 2, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
              "        0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "        0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 1,\n",
              "        0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0,\n",
              "        0, 2, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "        2, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "        2, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 0, 0, 0, 2,\n",
              "        1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
              "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
              "        0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "execution_count": 86
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sequences(\n",
        "    df: pd.DataFrame,\n",
        "    y: pd.DataFrame | np.ndarray | None = None,\n",
        "    window: int | None = None,\n",
        "    stride: int | None = None,\n",
        "    pad: bool = False,\n",
        "    add_time_features: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Build sequences from the dataset, either:\n",
        "      - full-length per sample_index (when window/stride are None), or\n",
        "      - sliding windows with given window and stride.\n",
        "\n",
        "    Data assumptions for THIS notebook:\n",
        "      â€¢ df already normalized/mapped (categoricals numeric; e.g., n_legs/hands/eyes âˆˆ {0,1})\n",
        "      â€¢ df has columns: ['sample_index','time', joint_*, pain_survey_*, n_legs, n_hands, n_eyes]\n",
        "      â€¢ each sample_index has T=160 rows (fixed-length), but we still allow windowing/stride\n",
        "\n",
        "    Returns:\n",
        "        dataset: np.ndarray of shape (N,T,F) or (N,window,F)\n",
        "        labels:  np.ndarray of shape (N,) if y is provided, else None\n",
        "    \"\"\"\n",
        "    # ------------------------------------------------------------------\n",
        "    # Feature groups (already numeric at this stage)\n",
        "    joint_cols  = [c for c in df.columns if c.startswith('joint_')]\n",
        "    pain_cols   = [c for c in df.columns if c.startswith('pain_survey_')]\n",
        "    static_cols = [c for c in ['n_legs', 'n_hands', 'n_eyes'] if c in df.columns]\n",
        "\n",
        "    # Keep only the necessary columns in a copy; preserve order\n",
        "    cols_needed = ['sample_index', 'time'] + joint_cols + pain_cols + static_cols\n",
        "    df = df[cols_needed].copy()\n",
        "\n",
        "    # Sort to preserve chronological order within each sequence\n",
        "    df = df.sort_values([\"sample_index\", \"time\"])\n",
        "\n",
        "    # If labels are provided, build a lookup dictionary: sample_index â†’ label\n",
        "    label_dict = None\n",
        "    if y is not None:\n",
        "        if isinstance(y, np.ndarray):\n",
        "            # Build mapping using the unique order of sample_index in df\n",
        "            unique_ids = df[\"sample_index\"].unique()\n",
        "            label_dict = {sid: int(lbl) for sid, lbl in zip(unique_ids, y)}\n",
        "        elif isinstance(y, pd.DataFrame):\n",
        "            # Expect columns ['sample_index','label'] with already-int-mapped labels\n",
        "            label_dict = dict(zip(y[\"sample_index\"], y[\"label\"]))\n",
        "\n",
        "    # Prepare outputs\n",
        "    dataset = []\n",
        "    labels  = []\n",
        "\n",
        "    # If no window/stride provided â†’ fall back to full-length per sequence\n",
        "    full_length_mode = (window is None or stride is None)\n",
        "\n",
        "    # Iterate over each sequence\n",
        "    for sid, group in df.groupby(\"sample_index\", sort=False):\n",
        "        # --- Extract groups (preserve types for embeddings) ---\n",
        "        X_joints = group[joint_cols].to_numpy(dtype=np.float32)        # (T, J) - continuous features\n",
        "\n",
        "        # IMPORTANT: Pain survey features are categorical indices {0,1,2}\n",
        "        # Keep as int64 first, then convert to float32 to preserve exact integer values\n",
        "        X_pain = group[pain_cols].to_numpy(dtype=np.int64)             # (T, 4) - categorical indices\n",
        "        X_pain = X_pain.astype(np.float32)                              # Convert to float32 but keep 0.0, 1.0, 2.0\n",
        "\n",
        "        # IMPORTANT: Static features are categorical indices {0,1}\n",
        "        # Keep as int64 first, then convert to float32 to preserve exact integer values\n",
        "        if static_cols:\n",
        "            X_static = group[static_cols].to_numpy(dtype=np.int64)     # (T, 3) - categorical indices\n",
        "            X_static = X_static.astype(np.float32)                      # Convert to float32 but keep 0.0, 1.0\n",
        "        else:\n",
        "            X_static = None\n",
        "\n",
        "\n",
        "        # Time features: extract normalized time + sinusoidal encoding\n",
        "        if add_time_features:\n",
        "            time_values = group['time'].to_numpy(dtype=np.float32)\n",
        "            max_time = time_values.max()\n",
        "            normalized_time = time_values / max_time if max_time > 0 else time_values\n",
        "            time_sin = np.sin(2 * np.pi * normalized_time)\n",
        "            time_cos = np.cos(2 * np.pi * normalized_time)\n",
        "            X_time = np.stack([normalized_time, time_sin, time_cos], axis=1)  # (T, 3)\n",
        "        else:\n",
        "            X_time = None\n",
        "\n",
        "        # Concatenate all feature groups along last dimension\n",
        "        if X_static is not None:\n",
        "            X_full = np.concatenate([X_joints, X_pain, X_static], axis=1)  # (T, F_total)\n",
        "        else:\n",
        "            X_full = np.concatenate([X_joints, X_pain], axis=1)            # (T, F_total)\n",
        "\n",
        "        # Add time features if enabled\n",
        "        if X_time is not None:\n",
        "            X_full = np.concatenate([X_full, X_time], axis=1)              # (T, F_total + 3)\n",
        "\n",
        "        T = X_full.shape[0]\n",
        "\n",
        "        if full_length_mode:\n",
        "            # ----- FULL-LENGTH MODE -----\n",
        "            dataset.append(X_full)\n",
        "            if label_dict is not None and sid in label_dict:\n",
        "                labels.append(int(label_dict[sid]))\n",
        "        else:\n",
        "            # ----- WINDOWED MODE (window, stride) -----\n",
        "            W = int(window)\n",
        "            S = int(stride)\n",
        "            assert W > 0 and S > 0, \"window and stride must be positive integers\"\n",
        "\n",
        "            if pad and T % W != 0:\n",
        "                # pad at the end with zeros to allow the last partial window\n",
        "                pad_len = (W - (T % W)) % W\n",
        "                if pad_len > 0:\n",
        "                    X_pad = np.zeros((pad_len, X_full.shape[1]), dtype=np.float32)\n",
        "                    X_seq = np.concatenate([X_full, X_pad], axis=0)\n",
        "                else:\n",
        "                    X_seq = X_full\n",
        "                Tmax = X_seq.shape[0]\n",
        "                idx = 0\n",
        "                while idx + W <= Tmax:\n",
        "                    dataset.append(X_seq[idx:idx+W])\n",
        "                    if label_dict is not None and sid in label_dict:\n",
        "                        labels.append(int(label_dict[sid]))\n",
        "                    idx += S\n",
        "            else:\n",
        "                # no padding â†’ only windows fully inside the sequence\n",
        "                idx = 0\n",
        "                while idx + W <= T:\n",
        "                    dataset.append(X_full[idx:idx+W])\n",
        "                    if label_dict is not None and sid in label_dict:\n",
        "                        labels.append(int(label_dict[sid]))\n",
        "                    idx += S\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    dataset = np.asarray(dataset, dtype=np.float32) if len(dataset) > 0 else np.empty((0, 0, 0), dtype=np.float32)\n",
        "    labels  = np.asarray(labels,  dtype=np.int64)   if len(labels)  > 0 else None\n",
        "\n",
        "    if dataset.size > 0:\n",
        "        print(f\"Built {len(dataset)} sequence{'s' if len(dataset)!=1 else ''}; each shape = {dataset[0].shape}\")\n",
        "    else:\n",
        "        print(\"Built 0 sequences (check window/stride vs sequence length).\")\n",
        "\n",
        "    return dataset, labels\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.745338Z",
          "iopub.execute_input": "2025-11-14T13:53:16.745578Z",
          "iopub.status.idle": "2025-11-14T13:53:16.760683Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.745562Z",
          "shell.execute_reply": "2025-11-14T13:53:16.759784Z"
        },
        "id": "X1oj12n1vyla"
      },
      "outputs": [],
      "execution_count": 87
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ACF-DRIVEN WINDOWING (drop-in) ---------------------------------\n",
        "# Goal:\n",
        "#   Use autocorrelation (ACF) to pick a WINDOW_SIZE (W) and STRIDE (S)\n",
        "#   that reflect how far the signal â€œremembersâ€ its past.\n",
        "#\n",
        "# Design choices:\n",
        "#   â€¢ We use ONLY joint_* signals (continuous), excluding pain_survey_* and\n",
        "#     static cats (n_legs/hands/eyes), because ACF is meaningful for\n",
        "#     continuous, time-varying signals. Surveys/cats would distort it.\n",
        "#   â€¢ We compute ACF per joint, per sequence, then average to get a robust\n",
        "#     \"mean ACF\" over the dataset (train only â†’ no leakage).\n",
        "#   â€¢ From the mean ACF curve we pick W via a simple heuristic:\n",
        "#       1) first local peak (a natural cycle length)\n",
        "#       2) else, first lag where ACF falls below a cutoff (memory fades)\n",
        "#     then clamp to a safe lower bound (min_window) and snap to multiple of 4.\n",
        "#\n",
        "# How to read the outputs:\n",
        "#   â€¢ W_SUGG = suggested window (steps of lookback the model should \"see\")\n",
        "#   â€¢ S_SUGG = suggested stride (step between window starts; default W//4)\n",
        "#   â€¢ n_windows = number of windows per sequence with length T=160\n",
        "#   â€¢ covered_steps = n_windows * W  (overlaps â†’ can exceed T)\n",
        "#\n",
        "# Keep in mind:\n",
        "#   â€¢ This is NOT a replacement for cross-validation. Use CV to compare\n",
        "#     W in a small neighborhood (e.g., W, WÂ±4 or WÂ±8) and keep what wins.\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def _acf_1d(x: np.ndarray, max_lag: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the autocorrelation function (ACF) for a 1D vector up to `max_lag`.\n",
        "\n",
        "    Steps:\n",
        "      1) Normalize x to zero-mean and unit-variance so the ACF is scale-free.\n",
        "      2) Use np.correlate in 'full' mode to get correlation at all lags.\n",
        "      3) Slice the non-negative lags [0..max_lag].\n",
        "      4) Normalize by acf[0] (the variance) so ACF[0] == 1 and 0<=ACF<=1 (approx).\n",
        "\n",
        "    Args:\n",
        "      x (np.ndarray): 1D time series (length T)\n",
        "      max_lag (int): maximum lag we want to evaluate (<= T-1)\n",
        "\n",
        "    Returns:\n",
        "      acf (np.ndarray): shape (max_lag+1,), acf[0]=1, acf[k]=similarity at lag k\n",
        "    \"\"\"\n",
        "    x = x.astype(np.float64)\n",
        "    x = (x - x.mean()) / (x.std() + 1e-8)      # protect from near-constant series\n",
        "    acf_full = np.correlate(x, x, mode='full') # length 2T-1\n",
        "    # Keep the right half: lags 0..max_lag (index starts at the center)\n",
        "    acf = acf_full[len(x)-1 : len(x)-1 + max_lag + 1]\n",
        "    # Normalize so that ACF[0] == 1 (divide by variance term)\n",
        "    return acf / (acf[0] + 1e-8)\n",
        "\n",
        "def mean_acf_over_joints(df, max_lag=80, sample_cap=256):\n",
        "    \"\"\"\n",
        "    Compute the MEAN ACF across joints and (optionally) across a subset\n",
        "    of sequences for speed. This gives a single, smooth ACF curve.\n",
        "\n",
        "    Expected df columns:\n",
        "      â€¢ 'sample_index', 'time', and many 'joint_*' columns\n",
        "\n",
        "    Process:\n",
        "      For each sequence (sample_index):\n",
        "        - sort by time\n",
        "        - build a (T, J) matrix of joint features\n",
        "        - for each joint j, compute ACF_j[0..L] where L = min(max_lag, T-1)\n",
        "        - average ACF over joints â†’ one curve per sequence\n",
        "      Average all sequence curves â†’ mean ACF\n",
        "\n",
        "    Why average?\n",
        "      Reduces noise and avoids picking a window based on a single joint\n",
        "      or a single quirky sequence.\n",
        "\n",
        "    Args:\n",
        "      df (DataFrame): TRAIN subset only (to avoid leakage)\n",
        "      max_lag (int): maximum lag we consider (e.g., 80)\n",
        "      sample_cap (int): if many sequences, randomly subsample this many\n",
        "                        to keep runtime small\n",
        "\n",
        "    Returns:\n",
        "      lags (np.ndarray): [0, 1, ..., L]\n",
        "      mean_acf (np.ndarray): averaged ACF curve, length L+1\n",
        "    \"\"\"\n",
        "    joint_cols = [c for c in df.columns if c.startswith(\"joint_\")]\n",
        "    assert len(joint_cols) > 0, \"No joint_* columns found.\"\n",
        "\n",
        "    # Subsample sequences if needed (for speed on large datasets)\n",
        "    sids = df[\"sample_index\"].unique()\n",
        "    if len(sids) > sample_cap:\n",
        "        rng = np.random.default_rng(42)\n",
        "        sids = rng.choice(sids, size=sample_cap, replace=False)\n",
        "\n",
        "    acfs = []\n",
        "    for sid in sids:\n",
        "        # (T, J) matrix for this sequence\n",
        "        seq = (df.loc[df[\"sample_index\"] == sid]\n",
        "                 .sort_values(\"time\")[joint_cols].to_numpy(dtype=np.float32))\n",
        "        T, J = seq.shape\n",
        "        L = min(max_lag, T - 1)  # ACF defined up to T-1\n",
        "        # ACF for each joint, then average across joints â†’ one curve per sequence\n",
        "        per_joint = []\n",
        "        for j in range(J):\n",
        "            per_joint.append(_acf_1d(seq[:, j], L))  # shape (L+1,)\n",
        "        acfs.append(np.mean(np.stack(per_joint, axis=0), axis=0))  # (L+1,)\n",
        "\n",
        "    # Average across sequences â†’ single smooth ACF curve\n",
        "    mean_acf = np.mean(np.stack(acfs, axis=0), axis=0)  # (L+1,)\n",
        "    lags = np.arange(len(mean_acf))\n",
        "    return lags, mean_acf\n",
        "\n",
        "def suggest_window_from_acf(mean_acf: np.ndarray, min_window=12, cutoff=0.10):\n",
        "    \"\"\"\n",
        "    Heuristic to convert the mean ACF curve into a WINDOW_SIZE (W).\n",
        "\n",
        "    Intuition:\n",
        "      â€¢ If there's a visible \"cycle\", the ACF will have a local peak at its period.\n",
        "        â†’ pick the first local maximum after lag=1 (we ignore lag=0..1).\n",
        "      â€¢ If no clear peak, use the lag where correlation \"dies out\" (drops < cutoff).\n",
        "        â†’ pick first lag where ACF < cutoff (e.g., 0.10).\n",
        "      â€¢ Clamp to a minimum window (min_window) so we don't get too tiny a W.\n",
        "      â€¢ Snap to a multiple of 4 to make stride W//4 an integer (nice, common choice).\n",
        "\n",
        "    Args:\n",
        "      mean_acf (np.ndarray): curve from mean_acf_over_joints\n",
        "      min_window (int): lower bound to keep W usable for training (default 12)\n",
        "      cutoff (float): ACF threshold for \"memory fades\" (default 0.10)\n",
        "\n",
        "    Returns:\n",
        "      W (int): suggested window size\n",
        "    \"\"\"\n",
        "    # 1) Look for the first local MAX after lag=1 (i.e., k >= 2)\n",
        "    peak_lag = None\n",
        "    for k in range(2, len(mean_acf) - 1):\n",
        "        # local max if it is >= next and > previous\n",
        "        if mean_acf[k] > mean_acf[k - 1] and mean_acf[k] >= mean_acf[k + 1]:\n",
        "            peak_lag = k\n",
        "            break\n",
        "\n",
        "    # 2) First lag where ACF falls below cutoff (correlation has faded)\n",
        "    below = np.where(mean_acf < cutoff)[0]\n",
        "    drop_lag = int(below[0]) if len(below) else None\n",
        "\n",
        "    # Combine signals (prefer a peak if present, else use drop)\n",
        "    if peak_lag is not None and drop_lag is not None:\n",
        "        # keep it within [min_window, drop_lag]\n",
        "        W = min(max(peak_lag, min_window), drop_lag)\n",
        "    elif peak_lag is not None:\n",
        "        W = max(peak_lag, min_window)\n",
        "    elif drop_lag is not None:\n",
        "        W = max(drop_lag, min_window)\n",
        "    else:\n",
        "        # No clear info â†’ conservative fallback\n",
        "        W = max(24, min_window)\n",
        "\n",
        "    # Safety clamp within [min_window, 160] (your T=160)\n",
        "    W = int(np.clip(W, min_window, 160))\n",
        "\n",
        "    # Snap to multiple of 4 so stride = W//4 is an integer\n",
        "    if W % 4 != 0:\n",
        "        W += (4 - (W % 4))\n",
        "    return W\n",
        "\n",
        "def suggest_stride(window: int) -> int:\n",
        "    \"\"\"\n",
        "    Default stride choice: quarter window.\n",
        "    Why? Good balance between:\n",
        "      - enough overlap to preserve information\n",
        "      - not exploding the number of windows too much\n",
        "    \"\"\"\n",
        "    return max(1, window // 4)\n",
        "\n",
        "# ---- RUN IT ON YOUR TRAIN DF (after normalization/mapping) ----------\n",
        "# NOTE: Use df_train (TRAIN SPLIT ONLY) to avoid leaking validation info.\n",
        "lags, mean_acf = mean_acf_over_joints(df_train, max_lag=80, sample_cap=256)\n",
        "\n",
        "# Heuristic suggestions from ACF\n",
        "W_SUGG = suggest_window_from_acf(mean_acf, min_window=12, cutoff=0.10)\n",
        "S_SUGG = suggest_stride(W_SUGG)\n",
        "print(f\"[ACF] Suggested WINDOW_SIZE={W_SUGG}, STRIDE={S_SUGG}\")\n",
        "\n",
        "# Optional: coverage diagnostic for your fixed T=160\n",
        "T = 160\n",
        "n_windows = (T - W_SUGG) // S_SUGG + 1 if T >= W_SUGG else 0\n",
        "covered = n_windows * W_SUGG  # can exceed T because windows overlap\n",
        "print(f\"[ACF] With T={T}: n_windows={n_windows}, covered_steps={covered}/{T}\")\n",
        "# --------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:16.761668Z",
          "iopub.execute_input": "2025-11-14T13:53:16.761962Z",
          "iopub.status.idle": "2025-11-14T13:53:17.427488Z",
          "shell.execute_reply.started": "2025-11-14T13:53:16.761944Z",
          "shell.execute_reply": "2025-11-14T13:53:17.426864Z"
        },
        "id": "wm_TTnE2vylb",
        "outputId": "04c61f5f-0961-4ab5-d008-2168160cc5d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ACF] Suggested WINDOW_SIZE=12, STRIDE=3\n",
            "[ACF] With T=160: n_windows=50, covered_steps=600/160\n"
          ]
        }
      ],
      "execution_count": 88
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train_df must contain sample_index + label columns\n",
        "y_train_df = pd.DataFrame({\n",
        "    \"sample_index\": X_train[\"sample_index\"].unique(),\n",
        "    \"label\": y_train\n",
        "})\n",
        "\n",
        "X_train_seq_complete_window, y_train_seq_complete_window = build_sequences(X_train, y_train_df, window=160)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:17.428120Z",
          "iopub.execute_input": "2025-11-14T13:53:17.428333Z",
          "iopub.status.idle": "2025-11-14T13:53:18.020800Z",
          "shell.execute_reply.started": "2025-11-14T13:53:17.428314Z",
          "shell.execute_reply": "2025-11-14T13:53:18.019806Z"
        },
        "id": "VMA5X_nevylb",
        "outputId": "c2901225-2b0d-4dd7-e168-6a3204207b32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built 528 sequences; each shape = (160, 40)\n"
          ]
        }
      ],
      "execution_count": 89
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_df = pd.DataFrame({\n",
        "    \"sample_index\": X_val[\"sample_index\"].unique(),\n",
        "    \"label\": y_val\n",
        "})\n",
        "\n",
        "X_val_seq_complete_window, y_val_seq_complete_window = build_sequences(X_val, y_val_df, 160)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:18.021713Z",
          "iopub.execute_input": "2025-11-14T13:53:18.022078Z",
          "iopub.status.idle": "2025-11-14T13:53:18.179236Z",
          "shell.execute_reply.started": "2025-11-14T13:53:18.022060Z",
          "shell.execute_reply": "2025-11-14T13:53:18.178616Z"
        },
        "id": "qVd0onBGvylb",
        "outputId": "2b9dbf06-5506-4e5f-98fe-aaa0da20115d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built 133 sequences; each shape = (160, 40)\n"
          ]
        }
      ],
      "execution_count": 90
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_seq_complete_window, _ = build_sequences(X_test, window=160)  # no labels â†’ returns (dataset, None)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:18.180001Z",
          "iopub.execute_input": "2025-11-14T13:53:18.180331Z",
          "iopub.status.idle": "2025-11-14T13:53:19.740252Z",
          "shell.execute_reply.started": "2025-11-14T13:53:18.180309Z",
          "shell.execute_reply": "2025-11-14T13:53:19.739543Z"
        },
        "id": "ZaXbknuvvylb",
        "outputId": "e105dd61-7393-4351-f5ee-647595f681d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built 1324 sequences; each shape = (160, 40)\n"
          ]
        }
      ],
      "execution_count": 91
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_win, y_tr_win = build_sequences(X_train, y_train, window=W_SUGG, stride=S_SUGG, pad=False)\n",
        "X_va_win, y_va_win = build_sequences(X_val,y_val, window=W_SUGG, stride=S_SUGG, pad=False)\n",
        "X_te_win, _        = build_sequences(X_test,None, window=W_SUGG, stride=S_SUGG, pad=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:19.741005Z",
          "iopub.execute_input": "2025-11-14T13:53:19.741303Z",
          "iopub.status.idle": "2025-11-14T13:53:22.182936Z",
          "shell.execute_reply.started": "2025-11-14T13:53:19.741264Z",
          "shell.execute_reply": "2025-11-14T13:53:22.182199Z"
        },
        "id": "4N7AG-BZvylb",
        "outputId": "31be4c5d-a884-4dd0-9ef9-87bc03c99453",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built 26400 sequences; each shape = (12, 40)\n",
            "Built 6650 sequences; each shape = (12, 40)\n",
            "Built 66200 sequences; each shape = (12, 40)\n"
          ]
        }
      ],
      "execution_count": 92
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "\n",
        "labels_np = y_tr_win  # numpy array of shape (N_train_windows,)\n",
        "\n",
        "# Count windows per class\n",
        "class_sample_counts = np.bincount(labels_np, minlength=3)\n",
        "# e.g. array([N_no, N_low, N_high])\n",
        "\n",
        "# Inverse frequency weights: rarer class â‡’ larger weight\n",
        "class_weights_for_sampling = 1.0 / (class_sample_counts + 1e-8)\n",
        "\n",
        "# Per-sample weights: pick the weight of its class\n",
        "sample_weights = class_weights_for_sampling[labels_np]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.183642Z",
          "iopub.execute_input": "2025-11-14T13:53:22.183890Z",
          "iopub.status.idle": "2025-11-14T13:53:22.194533Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.183864Z",
          "shell.execute_reply": "2025-11-14T13:53:22.193783Z"
        },
        "id": "7pt1wOvfvylb"
      },
      "outputs": [],
      "execution_count": 93
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = X_tr_win.shape[1:] # extract the shape of a single sequence\n",
        "num_classes = len(np.unique(y_tr_win)) # how many unique pain level exists\n",
        "input_shape, num_classes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.195259Z",
          "iopub.execute_input": "2025-11-14T13:53:22.195591Z",
          "iopub.status.idle": "2025-11-14T13:53:22.208573Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.195568Z",
          "shell.execute_reply": "2025-11-14T13:53:22.207963Z"
        },
        "id": "T5d6r4Lwvylb",
        "outputId": "ed197572-e1f5-4968-8f3a-d28557e4214e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((12, 40), 3)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "execution_count": 94
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n",
        "# each dataset now pairs each (T,F) TENSOR WITH ITS LABEL\n",
        "train_ds = TensorDataset(torch.from_numpy(X_tr_win), torch.from_numpy(y_tr_win))\n",
        "val_ds   = TensorDataset(torch.from_numpy(X_va_win), torch.from_numpy(y_va_win))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.213184Z",
          "iopub.execute_input": "2025-11-14T13:53:22.213401Z",
          "iopub.status.idle": "2025-11-14T13:53:22.221557Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.213386Z",
          "shell.execute_reply": "2025-11-14T13:53:22.220854Z"
        },
        "id": "2YowJ_Zdvylb"
      },
      "outputs": [],
      "execution_count": 95
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size, which is the number of samples in each batch\n",
        "BATCH_SIZE = 256"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.222242Z",
          "iopub.execute_input": "2025-11-14T13:53:22.222409Z",
          "iopub.status.idle": "2025-11-14T13:53:22.235018Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.222397Z",
          "shell.execute_reply": "2025-11-14T13:53:22.234452Z"
        },
        "id": "F7c91ewDvylb"
      },
      "outputs": [],
      "execution_count": 96
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_loader(ds, batch_size, shuffle, drop_last, sampler=None):\n",
        "    # Determine optimal number of worker processes for data loading\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "\n",
        "    final_shuffle = shuffle\n",
        "    if sampler is not None:\n",
        "        final_shuffle = False\n",
        "\n",
        "    # Create DataLoader with performance optimizations\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=final_shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        sampler=sampler,\n",
        "        pin_memory=True,  # Faster GPU transfer\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,  # Load 4 batches aheads\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.235696Z",
          "iopub.execute_input": "2025-11-14T13:53:22.235930Z",
          "iopub.status.idle": "2025-11-14T13:53:22.249446Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.235915Z",
          "shell.execute_reply": "2025-11-14T13:53:22.248695Z"
        },
        "id": "3y2Tfs6Pvylb"
      },
      "outputs": [],
      "execution_count": 97
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, sampler=sampler, drop_last=False)\n",
        "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.250081Z",
          "iopub.execute_input": "2025-11-14T13:53:22.250373Z",
          "iopub.status.idle": "2025-11-14T13:53:22.268110Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.250346Z",
          "shell.execute_reply": "2025-11-14T13:53:22.267552Z"
        },
        "id": "00XbcDxHvylb"
      },
      "outputs": [],
      "execution_count": 98
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch from the training data loader\n",
        "for xb, yb in train_loader:\n",
        "    print(\"Features batch shape:\", xb.shape)\n",
        "    print(\"Labels batch shape:\", yb.shape)\n",
        "    labels = yb.cpu().numpy()  # assuming (X, y) batch\n",
        "    print('Batch class counts:', np.unique(labels, return_counts=True))\n",
        "    break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.268874Z",
          "iopub.execute_input": "2025-11-14T13:53:22.269201Z",
          "iopub.status.idle": "2025-11-14T13:53:22.514305Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.269179Z",
          "shell.execute_reply": "2025-11-14T13:53:22.513522Z"
        },
        "id": "mmUCl2T2vylh",
        "outputId": "0dfa2836-8b81-4705-ba22-619310d4b57c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features batch shape: torch.Size([256, 12, 40])\n",
            "Labels batch shape: torch.Size([256])\n",
            "Batch class counts: (array([0, 1, 2]), array([92, 73, 91]))\n"
          ]
        }
      ],
      "execution_count": 99
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ› ï¸ **Model Building**"
      ],
      "metadata": {
        "id": "BFp8SSwFvylh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightLabelLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combines:\n",
        "    - Advice 08/11: Class weights for imbalance\n",
        "    - Advice 09/11: Label smoothing for generalization (per-class)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=3, class_weights=None, smoothing_per_class=None):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Per-class label smoothing\n",
        "        if smoothing_per_class is None:\n",
        "            # Default: more smoothing for majority, less for minority\n",
        "            smoothing_per_class = [0.10, 0.05, 0.01]\n",
        "\n",
        "        if isinstance(smoothing_per_class, (list, tuple)):\n",
        "            # Convert list to tensor\n",
        "            self.smoothing_per_class = torch.tensor(smoothing_per_class, dtype=torch.float32)\n",
        "        else:\n",
        "            # Single value: use for all classes\n",
        "            self.smoothing_per_class = torch.tensor([smoothing_per_class] * num_classes, dtype=torch.float32)\n",
        "\n",
        "        # Register as buffer (moves with model to GPU)\n",
        "        self.register_buffer('smoothing', self.smoothing_per_class)\n",
        "\n",
        "        # Class weights\n",
        "        if class_weights is not None:\n",
        "            self.register_buffer('class_weights', class_weights)\n",
        "        else:\n",
        "            self.register_buffer('class_weights', torch.ones(num_classes))\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Apply label smoothing\n",
        "        log_probs = F.log_softmax(pred, dim=-1)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        # Create smoothed targets with PER-CLASS smoothing\n",
        "        smooth_targets = torch.zeros_like(pred)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            true_class = target[i].item()\n",
        "            smoothing = self.smoothing[true_class].item()  # Get smoothing for this class\n",
        "            confidence = 1.0 - smoothing\n",
        "\n",
        "            # Distribute smoothing uniformly across other classes\n",
        "            smooth_targets[i].fill_(smoothing / (self.num_classes - 1))\n",
        "            smooth_targets[i, true_class] = confidence\n",
        "\n",
        "        # Compute loss with class weights\n",
        "        loss_per_sample = -(smooth_targets * log_probs).sum(dim=-1)\n",
        "\n",
        "        # Apply class weights based on true label\n",
        "        weights = self.class_weights[target]\n",
        "        weighted_loss = (loss_per_sample * weights).mean()\n",
        "\n",
        "        return weighted_loss\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.515236Z",
          "iopub.execute_input": "2025-11-14T13:53:22.515521Z",
          "iopub.status.idle": "2025-11-14T13:53:22.523862Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.515486Z",
          "shell.execute_reply": "2025-11-14T13:53:22.523178Z"
        },
        "id": "kPTSsDezvylh"
      },
      "outputs": [],
      "execution_count": 100
    },
    {
      "cell_type": "code",
      "source": [
        "def recurrent_summary(model, input_size):\n",
        "    \"\"\"\n",
        "    Custom summary function that emulates torchinfo's output while correctly\n",
        "    counting parameters for RNN/GRU/LSTM layers.\n",
        "\n",
        "    This function is designed for models whose direct children are\n",
        "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to analyze.\n",
        "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store output shapes captured by forward hooks\n",
        "    output_shapes = {}\n",
        "    # List to track hook handles for later removal\n",
        "    hooks = []\n",
        "\n",
        "    def get_hook(name):\n",
        "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # Handle RNN layer outputs (returns a tuple)\n",
        "            if isinstance(output, tuple):\n",
        "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
        "                shape1 = list(output[0].shape)\n",
        "                shape1[0] = -1  # Replace batch dimension with -1\n",
        "\n",
        "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
        "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
        "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
        "                else:  # RNN/GRU case: h_n only\n",
        "                    shape2 = list(output[1].shape)\n",
        "\n",
        "                # Replace batch dimension (middle position) with -1\n",
        "                shape2[1] = -1\n",
        "\n",
        "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
        "\n",
        "            # Handle standard layer outputs (e.g., Linear)\n",
        "            else:\n",
        "                shape = list(output.shape)\n",
        "                shape[0] = -1  # Replace batch dimension with -1\n",
        "                output_shapes[name] = f\"{shape}\"\n",
        "        return hook\n",
        "\n",
        "    # 1. Determine the device where model parameters reside\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
        "\n",
        "    # 2. Create a dummy input tensor with batch_size=1\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "\n",
        "    # 3. Register forward hooks on target layers\n",
        "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
        "            # Register the hook and store its handle for cleanup\n",
        "            hook_handle = module.register_forward_hook(get_hook(name))\n",
        "            hooks.append(hook_handle)\n",
        "\n",
        "    # 4. Execute a dummy forward pass in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            model(dummy_input)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during dummy forward pass: {e}\")\n",
        "            # Clean up hooks even if an error occurs\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "            return\n",
        "\n",
        "    # 5. Remove all registered hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # --- 6. Print the summary table ---\n",
        "\n",
        "    print(\"-\" * 79)\n",
        "    # Column headers\n",
        "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
        "    print(\"=\" * 79)\n",
        "\n",
        "    total_params = 0\n",
        "    total_trainable_params = 0\n",
        "\n",
        "    # Iterate through modules again to collect and display parameter information\n",
        "    for name, module in model.named_children():\n",
        "        if name in output_shapes:\n",
        "            # Count total and trainable parameters for this module\n",
        "            module_params = sum(p.numel() for p in module.parameters())\n",
        "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "            total_params += module_params\n",
        "            total_trainable_params += trainable_params\n",
        "\n",
        "            # Format strings for display\n",
        "            layer_name = f\"{name} ({type(module).__name__})\"\n",
        "            output_shape_str = str(output_shapes[name])\n",
        "            params_str = f\"{trainable_params:,}\"\n",
        "\n",
        "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
        "\n",
        "    print(\"=\" * 79)\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
        "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
        "    print(\"-\" * 79)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.524660Z",
          "iopub.execute_input": "2025-11-14T13:53:22.524902Z",
          "iopub.status.idle": "2025-11-14T13:53:22.543440Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.524881Z",
          "shell.execute_reply": "2025-11-14T13:53:22.542849Z"
        },
        "id": "mPtTi-1gvylh"
      },
      "outputs": [],
      "execution_count": 101
    },
    {
      "cell_type": "code",
      "source": [
        "# CRITICAL: Count your features BEFORE creating model\n",
        "joint_cols = [c for c in X_train.columns if c.startswith('joint_')]\n",
        "pain_cols = [c for c in X_train.columns if c.startswith('pain_survey_')]\n",
        "static_cols = ['n_legs', 'n_hands', 'n_eyes']\n",
        "\n",
        "NUM_JOINT_FEATURES = len(joint_cols)\n",
        "NUM_PAIN_FEATURES = len(pain_cols)\n",
        "NUM_STATIC_FEATURES = len([c for c in static_cols if c in X_train.columns])  #\n",
        "NUM_TIME_FEATURES = 3\n",
        "\n",
        "print(f\"Feature counts:\")\n",
        "print(f\"  Joint: {NUM_JOINT_FEATURES}\")\n",
        "print(f\"  Pain: {NUM_PAIN_FEATURES}\")\n",
        "print(f\"  Static: {NUM_STATIC_FEATURES}\")\n",
        "print(f\"  Time: {NUM_TIME_FEATURES}\")\n",
        "print(f\"  Total: {NUM_JOINT_FEATURES + NUM_PAIN_FEATURES + NUM_STATIC_FEATURES + NUM_TIME_FEATURES}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.544189Z",
          "iopub.execute_input": "2025-11-14T13:53:22.544459Z",
          "iopub.status.idle": "2025-11-14T13:53:22.563605Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.544439Z",
          "shell.execute_reply": "2025-11-14T13:53:22.562862Z"
        },
        "id": "GSkNR1L_vylh",
        "outputId": "d120c5ee-216f-4cc8-ba01-da5e772bc091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature counts:\n",
            "  Joint: 30\n",
            "  Pain: 4\n",
            "  Static: 3\n",
            "  Time: 3\n",
            "  Total: 40\n"
          ]
        }
      ],
      "execution_count": 102
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RecurrentClassifier(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            num_classes,\n",
        "            rnn_type='GRU',\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.2,\n",
        "            rec_dropout_rate=None,\n",
        "            cnn_channels=None,\n",
        "            cnn_kernel_size=3,\n",
        "            cnn_dropout=None,\n",
        "            use_pain_embeddings=True,\n",
        "            pain_embedding_dim=4,\n",
        "            num_joint_features=30,\n",
        "            num_pain_features=4,\n",
        "            num_static_features=3,\n",
        "            num_time_features=3,\n",
        "            use_attention=True\n",
        "            ):\n",
        "        super().__init__()\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.uses_cnn = cnn_channels is not None\n",
        "        self.use_pain_embeddings = use_pain_embeddings\n",
        "        self.use_attention = use_attention\n",
        "        # ---------------------------------------------\n",
        "        # Store feature split indices (same as before)\n",
        "        self.num_joint_features = num_joint_features\n",
        "        self.num_pain_features = num_pain_features\n",
        "        self.num_static_features = num_static_features\n",
        "        self.num_time_features = num_time_features\n",
        "        self.joint_end = num_joint_features\n",
        "        self.pain_end = self.joint_end + num_pain_features\n",
        "        self.static_end = self.pain_end + num_static_features\n",
        "        # ---------------------------------------------\n",
        "        # Check input size\n",
        "        expected_input = num_joint_features + num_pain_features + num_static_features + num_time_features\n",
        "        if input_size != expected_input:\n",
        "            print(f\"WARNING: input_size={input_size} but expected {expected_input}\")\n",
        "        # ---------------------------------------------\n",
        "        # Embeddings (if enabled)\n",
        "        if self.use_pain_embeddings:\n",
        "            self.pain_embeddings = nn.ModuleList([\n",
        "                nn.Embedding(num_embeddings=3, embedding_dim=pain_embedding_dim)\n",
        "                for _ in range(num_pain_features)\n",
        "            ])\n",
        "            effective_input_size = (num_joint_features + num_pain_features * pain_embedding_dim + num_static_features + num_time_features)\n",
        "        else:\n",
        "            effective_input_size = input_size\n",
        "        # ---------------------------------------------\n",
        "        # Optionally CNN block\n",
        "        if self.uses_cnn:\n",
        "            self.cnn1 = nn.Conv1d(in_channels=effective_input_size,\n",
        "                                  out_channels=cnn_channels,\n",
        "                                  kernel_size=cnn_kernel_size,\n",
        "                                  padding=cnn_kernel_size // 2)\n",
        "            self.cnn_bn = nn.BatchNorm1d(cnn_channels)\n",
        "            self.cnn_act = nn.ReLU()\n",
        "            self.cnn_dropout = nn.Dropout(cnn_dropout if cnn_dropout is not None else dropout_rate)\n",
        "            rnn_input_size = cnn_channels\n",
        "        else:\n",
        "            rnn_input_size = effective_input_size\n",
        "        # ---------------------------------------------\n",
        "        # RNN\n",
        "        rnn_map = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=rnn_input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "        # ---------------------------------------------\n",
        "        # Classifier layer (after attention output)\n",
        "        rnn_output_size = hidden_size * (2 if bidirectional else 1)\n",
        "        self.classifier = nn.Linear(rnn_output_size, num_classes)\n",
        "        self.rec_dropout = nn.Dropout(rec_dropout_rate) if rec_dropout_rate is not None else None\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: (batch, 1, d)\n",
        "            key, value: (batch, seq_len, d)\n",
        "        Computes attention weights and context.\n",
        "        \"\"\"\n",
        "        # Compute dot products\n",
        "        scores = torch.matmul(query, key.transpose(1,2))  # (batch, 1, seq_len)\n",
        "        scores = scores / torch.sqrt(torch.tensor(query.size(-1), dtype=torch.float32, device=query.device))\n",
        "        attn_weights = F.softmax(scores, dim=-1)          # (batch, 1, seq_len)\n",
        "        context = torch.matmul(attn_weights, value)        # (batch, 1, d)\n",
        "        context = context.squeeze(1)                      # (batch, d)\n",
        "        return context, attn_weights.squeeze(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # --- Embedding and feature split ---\n",
        "        if self.use_pain_embeddings:\n",
        "            X_joints = x[:, :, :self.joint_end]\n",
        "            X_pain = x[:, :, self.joint_end:self.pain_end]\n",
        "            X_static = x[:, :, self.pain_end:self.static_end]\n",
        "            X_time = x[:, :, self.static_end:]\n",
        "            pain_embedded_list = [self.pain_embeddings[i](X_pain[:,:,i].long()) for i in range(self.num_pain_features)]\n",
        "            X_pain_embedded = torch.cat(pain_embedded_list, dim=-1)\n",
        "            x = torch.cat([X_joints, X_pain_embedded, X_static, X_time], dim=-1)\n",
        "        # --- Optionally CNN preprocessing ---\n",
        "        if self.uses_cnn:\n",
        "            x = x.transpose(1, 2)\n",
        "            x = self.cnn1(x)\n",
        "            x = self.cnn_bn(x)\n",
        "            x = self.cnn_act(x)\n",
        "            x = self.cnn_dropout(x)\n",
        "            x = x.transpose(1, 2)\n",
        "        # --- RNN ---\n",
        "        rnn_out, hidden = self.rnn(x)  # rnn_out: (batch, seq_len, hidden_size * directions)\n",
        "\n",
        "        # Use last hidden state as query for attention\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]\n",
        "        if self.bidirectional:\n",
        "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
        "            final_hidden = torch.cat([hidden[-1,0,:,:], hidden[-1,1,:,:]], dim=1)  # (batch, rnn_output_size)\n",
        "        else:\n",
        "            final_hidden = hidden[-1]  # (batch, rnn_output_size)\n",
        "        # --- Attention: query=final_hidden, key/value=all rnn_out ---\n",
        "        if self.use_attention:\n",
        "            query = final_hidden.unsqueeze(1)  # (batch, 1, rnn_output_size)\n",
        "            context, attn_weights = self.scaled_dot_product_attention(query, rnn_out, rnn_out)\n",
        "        else:\n",
        "            context = final_hidden\n",
        "\n",
        "        # --- Dropout and classification ---\n",
        "        if self.rec_dropout is not None:\n",
        "            context = self.rec_dropout(context)\n",
        "        logits = self.classifier(context)  # (batch, num_classes)\n",
        "        return logits  # Optionally also return attn_weights if you want visualization\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.564448Z",
          "iopub.execute_input": "2025-11-14T13:53:22.564676Z",
          "iopub.status.idle": "2025-11-14T13:53:22.588508Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.564661Z",
          "shell.execute_reply": "2025-11-14T13:53:22.587834Z"
        },
        "id": "T0TEXRhBvylh"
      },
      "outputs": [],
      "execution_count": 103
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  **Model Training**"
      ],
      "metadata": {
        "id": "4FcJNPTGvyli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize best model tracking variables\n",
        "best_model = None\n",
        "best_performance = float('-inf')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.589151Z",
          "iopub.execute_input": "2025-11-14T13:53:22.589390Z",
          "iopub.status.idle": "2025-11-14T13:53:22.607755Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.589363Z",
          "shell.execute_reply": "2025-11-14T13:53:22.607049Z"
        },
        "id": "-Hdq3v00vyli"
      },
      "outputs": [],
      "execution_count": 104
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler,\n",
        "                    device, l1_lambda=0, l2_lambda=0,max_grad_norm=1.0):\n",
        "    \"\"\"\n",
        "    Perform one complete training epoch through the entire training dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): Lambda for L1 regularization\n",
        "        l2_lambda (float): Lambda for L2 regularization\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Clear gradients from previous step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass with mixed precision (if CUDA available)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Add L1 and L2 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        if scaler is not None and device.type == 'cuda':\n",
        "            scaler.scale(loss).backward()            # grads are scaled\n",
        "            scaler.unscale_(optimizer)               # unscale to true grad values\n",
        "            torch.nn.utils.clip_grad_norm_(          # CLIP true gradients (magnitude cap)\n",
        "                model.parameters(), max_norm=max_grad_norm\n",
        "            )\n",
        "            scaler.step(optimizer)                   # safe optimizer.step() (skips on inf/NaN)\n",
        "            scaler.update()                          # update scaling factor\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='macro'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_f1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.608567Z",
          "iopub.execute_input": "2025-11-14T13:53:22.608833Z",
          "iopub.status.idle": "2025-11-14T13:53:22.623792Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.608791Z",
          "shell.execute_reply": "2025-11-14T13:53:22.623098Z"
        },
        "id": "ZlRL0wYDvyli"
      },
      "outputs": [],
      "execution_count": 105
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
        "\n",
        "    Note:\n",
        "        This function automatically sets the model to evaluation mode and disables\n",
        "        gradient computation for efficiency during validation.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision (if CUDA available)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='macro'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.624672Z",
          "iopub.execute_input": "2025-11-14T13:53:22.624966Z",
          "iopub.status.idle": "2025-11-14T13:53:22.641121Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.624951Z",
          "shell.execute_reply": "2025-11-14T13:53:22.640280Z"
        },
        "id": "ht1PGUw9vyli"
      },
      "outputs": [],
      "execution_count": 106
    },
    {
      "cell_type": "code",
      "source": [
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_loss (float): Training loss for this epoch\n",
        "        train_f1 (float): Training f1 score for this epoch\n",
        "        val_loss (float): Validation loss for this epoch\n",
        "        val_f1 (float): Validation f1 score for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "\n",
        "    Note:\n",
        "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
        "        parameters and gradients, which helps monitor training progress and detect\n",
        "        issues like vanishing/exploding gradients.\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty before adding a histogram\n",
        "            if param.numel() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty before adding a histogram\n",
        "                if param.grad.numel() > 0:\n",
        "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
        "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.641770Z",
          "iopub.execute_input": "2025-11-14T13:53:22.641991Z",
          "iopub.status.idle": "2025-11-14T13:53:22.658736Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.641977Z",
          "shell.execute_reply": "2025-11-14T13:53:22.658083Z"
        },
        "id": "nsnt3MZ2vyli"
      },
      "outputs": [],
      "execution_count": 107
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, train_loader, val_loader, epochs, train_criterion, val_criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, scheduler=None, # Added scheduler parameter\n",
        "        evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=1, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Train the neural network model on the training data and validate on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        epochs (int): Number of training epochs\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
        "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
        "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
        "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
        "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
        "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
        "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
        "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
        "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, training_history) - Trained model and metrics history\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping if patience is set\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop: iterate through epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Forward pass through training data, compute gradients, update weights\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, train_criterion, optimizer, scaler, device\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation data without updating weights\n",
        "        if val_loader is not None:\n",
        "            val_loss, val_f1 = validate_one_epoch(model, val_loader, val_criterion, device)\n",
        "        else:\n",
        "            val_loss, val_f1 = None, None\n",
        "\n",
        "\n",
        "        # Step the scheduler if provided (typically after validation)\n",
        "        if scheduler is not None:\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) or isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
        "                scheduler.step(val_f1)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "        # Store metrics for plotting and analysis\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Write metrics to TensorBoard for visualization\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
        "            )\n",
        "\n",
        "        # Print progress every N epochs or on first epoch\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "                if val_loss is not None:\n",
        "                    print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                          f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
        "                          f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                          f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f}\")\n",
        "\n",
        "\n",
        "        # Early stopping logic: monitor metric and save best model\n",
        "        if patience > 0 and val_loader is not None:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "\n",
        "    # Restore best model weights if early stopping was used\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "\n",
        "    # Save final model if no early stopping\n",
        "    if patience == 0:\n",
        "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "\n",
        "    if patience > 0:\n",
        "        training_history['best_epoch'] = best_epoch\n",
        "        training_history['best_metric'] = best_metric\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.659511Z",
          "iopub.execute_input": "2025-11-14T13:53:22.659718Z",
          "iopub.status.idle": "2025-11-14T13:53:22.677677Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.659696Z",
          "shell.execute_reply": "2025-11-14T13:53:22.676860Z"
        },
        "id": "fXJVMsPOvyli"
      },
      "outputs": [],
      "execution_count": 108
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§® **Network and Training Hyperparameters**"
      ],
      "metadata": {
        "id": "uMRbSS2dvyli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_np = np.array(y_train)\n",
        "num_classes = np.max(y_train_np) + 1\n",
        "class_counts = np.bincount(y_train_np, minlength=num_classes)\n",
        "total = len(y_train_np)\n",
        "\n",
        "print(\"Training class distribution:\")\n",
        "for i, name in enumerate(['no_pain', 'low_pain', 'high_pain']):\n",
        "    print(f\"  {name} ({i}): {class_counts[i]} ({class_counts[i]/total*100:.1f}%)\")\n",
        "\n",
        "# SQRT dampening\n",
        "max_count = class_counts.max()\n",
        "class_weights_raw = max_count / class_counts\n",
        "class_weights_dampened = np.sqrt(class_weights_raw)\n",
        "\n",
        "# Convert to tensor (CRITICAL!)\n",
        "class_weights = torch.tensor(class_weights_dampened, dtype=torch.float32)\n",
        "\n",
        "print(f\"\\nClass weights (sqrt dampened): {class_weights}\")\n",
        "print(f\"Type: {type(class_weights)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.678550Z",
          "iopub.execute_input": "2025-11-14T13:53:22.679067Z",
          "iopub.status.idle": "2025-11-14T13:53:22.738983Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.679044Z",
          "shell.execute_reply": "2025-11-14T13:53:22.738342Z"
        },
        "id": "c4uYN8C_vyli",
        "outputId": "25af2373-5a9b-4470-e646-fd52b138becd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training class distribution:\n",
            "  no_pain (0): 408 (77.3%)\n",
            "  low_pain (1): 75 (14.2%)\n",
            "  high_pain (2): 45 (8.5%)\n",
            "\n",
            "Class weights (sqrt dampened): tensor([1.0000, 2.3324, 3.0111])\n",
            "Type: <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "execution_count": 109
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== TRAINING CONFIGURATION =====\n",
        "LEARNING_RATE = 3e-4\n",
        "EPOCHS = 150\n",
        "PATIENCE = 15\n",
        "\n",
        "# ===== ARCHITECTURE =====\n",
        "HIDDEN_LAYERS = 2\n",
        "HIDDEN_SIZE = 192\n",
        "\n",
        "# ===== REGULARIZATION =====\n",
        "DROPOUT_RATE = 0.3\n",
        "RECURRENT_DROPOUT = 0.2\n",
        "L2_LAMBDA = 3e-5\n",
        "L1_LAMBDA = 0\n",
        "\n",
        "# ===== CNN CONFIGURATION =====\n",
        "CNN_CHANNELS    = 32\n",
        "CNN_KERNEL_SIZE = 4\n",
        "CNN_DROPOUT     = 0.35\n",
        "\n",
        "\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "\n",
        "# Loss function with FocalLabelSmoothing\n",
        "train_criterion = WeightLabelLoss(\n",
        "    num_classes=3,\n",
        ").to(device)\n",
        "\n",
        "val_criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# model\n",
        "MODEL='GRU'\n",
        "RNN_TYPE=MODEL\n",
        "BIDIRECTIONAL=True"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.739615Z",
          "iopub.execute_input": "2025-11-14T13:53:22.739829Z",
          "iopub.status.idle": "2025-11-14T13:53:22.746529Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.739794Z",
          "shell.execute_reply": "2025-11-14T13:53:22.745854Z"
        },
        "id": "buee-HTUvyli"
      },
      "outputs": [],
      "execution_count": 110
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model and display architecture with parameter count\n",
        "rnn_model = RecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=HIDDEN_LAYERS,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    rec_dropout_rate = RECURRENT_DROPOUT,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    cnn_channels=CNN_CHANNELS,                  # enables CNN1D preprocessing\n",
        "    cnn_kernel_size=CNN_KERNEL_SIZE,                # usually 3, 5, or 7 works well\n",
        "    cnn_dropout=CNN_DROPOUT,                  # (optional; default uses RNN dropout)\n",
        "    rnn_type=MODEL,\n",
        "    use_attention=False\n",
        "    ).to(device)\n",
        "# recurrent_summary(rnn_model, input_size=input_shape)\n",
        "\n",
        "# Set up TensorBoard logging and save model architecture\n",
        "prefix = \"bi_\" if BIDIRECTIONAL else \"\"\n",
        "experiment_name = prefix + MODEL.lower()\n",
        "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
        "# x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n",
        "# writer.add_graph(rnn_model, x)\n",
        "\n",
        "# Define optimizer with L2 regularization\n",
        "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
        "scheduler = CosineAnnealingWarmRestarts(\n",
        "    optimizer,\n",
        "    T_0=15,        # Restart every 15 epochs\n",
        "    T_mult=2,      # Double restart period each time\n",
        "    eta_min=1e-6   # Minimum LR\n",
        ")\n",
        "\n",
        "# scheduler = ReduceLROnPlateau(\n",
        "#     optimizer,\n",
        "#     mode='max',           # Maximize F1\n",
        "#     factor=0.5,           # Reduce LR by 50% when plateau\n",
        "#     patience=10,          # Wait 10 epochs before reducing\n",
        "#     min_lr=1e-6\n",
        "# )\n",
        "\n",
        "# Enable mixed precision training for GPU acceleration\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:22.747280Z",
          "iopub.execute_input": "2025-11-14T13:53:22.747601Z",
          "iopub.status.idle": "2025-11-14T13:53:25.317374Z",
          "shell.execute_reply.started": "2025-11-14T13:53:22.747579Z",
          "shell.execute_reply": "2025-11-14T13:53:25.316757Z"
        },
        "id": "YzVzK0fZvyli"
      },
      "outputs": [],
      "execution_count": 111
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Train model and track training history\n",
        "rnn_model, training_history = fit(\n",
        "    model=rnn_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    train_criterion=train_criterion,\n",
        "    val_criterion=val_criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    writer=writer,\n",
        "    verbose=1,\n",
        "    experiment_name=MODEL.lower(),\n",
        "    patience=PATIENCE\n",
        "    )\n",
        "\n",
        "# Update best model if current performance is superior\n",
        "if training_history['val_f1'][-1] > best_performance:\n",
        "    best_model = rnn_model\n",
        "    best_performance = training_history['val_f1'][-1]\n",
        "\n",
        "best_epoch_number = training_history['best_epoch']\n",
        "print(f\"Best epoch was: {best_epoch_number}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:53:25.318131Z",
          "iopub.execute_input": "2025-11-14T13:53:25.318659Z",
          "iopub.status.idle": "2025-11-14T13:58:32.325355Z",
          "shell.execute_reply.started": "2025-11-14T13:53:25.318630Z",
          "shell.execute_reply": "2025-11-14T13:58:32.324548Z"
        },
        "id": "b9ZZpF4Bvyli",
        "outputId": "186c4a85-9180-4b8d-82d4-a36629dcbd8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 150 epochs...\n",
            "Epoch   1/150 | Train: Loss=0.9550, F1 Score=0.5235 | Val: Loss=0.8417, F1 Score=0.5392\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "Pmvb2G4Tvyli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Hitory\n",
        "# Create a figure with two side-by-side subplots (two columns)\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
        "\n",
        "# Plot of training and validation loss on the first axis\n",
        "ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Plot of training and validation accuracy on the second axis\n",
        "ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
        "ax2.set_title('F1 Score')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Adjust the layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(right=0.85)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:58:32.326510Z",
          "iopub.execute_input": "2025-11-14T13:58:32.327247Z",
          "iopub.status.idle": "2025-11-14T13:58:32.954361Z",
          "shell.execute_reply.started": "2025-11-14T13:58:32.327219Z",
          "shell.execute_reply": "2025-11-14T13:58:32.953627Z"
        },
        "id": "F41DXuSuvyli"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Confusion Matrix\n",
        "# Collect predictions and ground truth labels\n",
        "val_preds, val_targets = [], []\n",
        "window_to_sample = []  # Track which windows belong to which sample\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation for inference\n",
        "    for xb, yb in val_loader:\n",
        "        xb = xb.to(device)\n",
        "\n",
        "        # Forward pass: get model predictions\n",
        "        logits = rnn_model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        # Store batch results (these are still per-window)\n",
        "        val_preds.append(preds)\n",
        "        val_targets.append(yb.numpy())\n",
        "\n",
        "# Combine all batches into single arrays (still per-window)\n",
        "val_preds_windows = np.concatenate(val_preds)\n",
        "val_targets_windows = np.concatenate(val_targets)\n",
        "\n",
        "# ============= AGGREGATE WINDOWS TO SEQUENCES =============\n",
        "# Reconstruct mapping: each sequence produces (160 - W) // S + 1 windows\n",
        "n_windows_per_seq = (160 - W_SUGG) // S_SUGG + 1\n",
        "\n",
        "# Map window predictions back to sample_index\n",
        "unique_samples = X_val[\"sample_index\"].unique()\n",
        "sequence_preds = {}\n",
        "sequence_targets = {}\n",
        "\n",
        "for idx, sid in enumerate(unique_samples):\n",
        "    # Extract windows for this sequence\n",
        "    start_idx = idx * n_windows_per_seq\n",
        "    end_idx = start_idx + n_windows_per_seq\n",
        "\n",
        "    window_preds = val_preds_windows[start_idx:end_idx]\n",
        "    window_targets = val_targets_windows[start_idx:end_idx]\n",
        "\n",
        "    # Aggregate strategy: MAJORITY VOTE\n",
        "    from collections import Counter\n",
        "    vote_counts = Counter(window_preds)\n",
        "    final_pred = vote_counts.most_common(1)[0][0]\n",
        "\n",
        "    # Target should be same across all windows (sanity check)\n",
        "    assert len(np.unique(window_targets)) == 1, f\"Sample {sid} has inconsistent labels!\"\n",
        "    final_target = window_targets[0]\n",
        "\n",
        "    sequence_preds[sid] = final_pred\n",
        "    sequence_targets[sid] = final_target\n",
        "\n",
        "# Convert to arrays for metrics\n",
        "val_preds = np.array([sequence_preds[sid] for sid in unique_samples])\n",
        "val_targets = np.array([sequence_targets[sid] for sid in unique_samples])\n",
        "# ============= END AGGREGATION =============\n",
        "\n",
        "# Calculate overall validation metrics (now sequence-level)\n",
        "val_acc = accuracy_score(val_targets, val_preds)\n",
        "val_prec = precision_score(val_targets, val_preds, average='macro')\n",
        "val_rec = recall_score(val_targets, val_preds, average='macro')\n",
        "val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
        "print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n",
        "print(f\"Precision over the validation set: {val_prec:.4f}\")\n",
        "print(f\"Recall over the validation set: {val_rec:.4f}\")\n",
        "print(f\"F1 score over the validation set: {val_f1:.4f}\")\n",
        "\n",
        "# Generate confusion matrix for detailed error analysis\n",
        "cm = confusion_matrix(val_targets, val_preds)\n",
        "\n",
        "# Create numeric labels for heatmap annotation\n",
        "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Visualise confusion matrix\n",
        "plt.figure(figsize=(8, 7))\n",
        "sns.heatmap(cm, annot=labels, fmt='',\n",
        "            cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix â€” Validation Set (Sequence-Level)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:58:32.955253Z",
          "iopub.execute_input": "2025-11-14T13:58:32.955499Z",
          "iopub.status.idle": "2025-11-14T13:58:34.210321Z",
          "shell.execute_reply.started": "2025-11-14T13:58:32.955481Z",
          "shell.execute_reply": "2025-11-14T13:58:34.209608Z"
        },
        "id": "aXV2e2J-vylj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# choose output directory manually\n",
        "\n",
        "# --- Kaggle ---\n",
        "# OUT_DIR = \"/kaggle/working\"\n",
        "\n",
        "# --- Cluster (Westworld / Elysium) ---\n",
        "# OUT_DIR = \"/home/cristiano.battistini/storage/an2dl_outputs\"\n",
        "\n",
        "# --- Docker / local environment ---\n",
        "OUT_DIR = os.path.join(os.getcwd(), \"outputs\")\n",
        "\n",
        "# --- Create directory if it doesn't exist ---\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:58:34.211315Z",
          "iopub.execute_input": "2025-11-14T13:58:34.211558Z",
          "iopub.status.idle": "2025-11-14T13:58:34.216010Z",
          "shell.execute_reply.started": "2025-11-14T13:58:34.211536Z",
          "shell.execute_reply": "2025-11-14T13:58:34.215310Z"
        },
        "id": "cc4nsjLvvylj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def save_experiment_output(\n",
        "    model_name: str,\n",
        "    hyperparams: dict,\n",
        "    X_test_seq: np.ndarray,\n",
        "    label_mapping: dict,\n",
        "    sample_indices: list,\n",
        "    output_dir: str,\n",
        "    model=None,\n",
        "    batch_size: int = 256,\n",
        "    window_size: int = None,\n",
        "    stride: int = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Run inference on the test set, save predictions and hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the experiment (e.g. 'lstm', 'bilstm', 'ffn').\n",
        "        hyperparams (dict): Dict containing all hyperparameters and training config.\n",
        "        X_test_seq (np.ndarray): Test sequences of shape (N_windows, W, F) â€” windowed data.\n",
        "        label_mapping (dict): Mapping from label string to class index.\n",
        "        sample_indices (list): List of sample_index identifiers (as strings).\n",
        "        output_dir (str): Folder where submission and metadata are saved.\n",
        "        model (torch.nn.Module): Trained model for inference.\n",
        "        batch_size (int): Inference batch size.\n",
        "        window_size (int): Window size used to create X_test_seq.\n",
        "        stride (int): Stride used to create X_test_seq.\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Reverse mapping\n",
        "    idx2label = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "    # --- Inference on windows ---\n",
        "    model.eval().to(device)\n",
        "    with torch.inference_mode():\n",
        "        logits = []\n",
        "        for i in range(0, len(X_test_seq), batch_size):\n",
        "            xb = torch.from_numpy(X_test_seq[i:i+batch_size]).to(device)\n",
        "            logits.append(model(xb).cpu().numpy())\n",
        "        logits = np.concatenate(logits, axis=0)  # (N_windows, num_classes)\n",
        "\n",
        "    # Get per-window predictions\n",
        "    pred_idx_windows = logits.argmax(axis=1)  # (N_windows,)\n",
        "\n",
        "    # --- Aggregate windows to sequences ---\n",
        "    # Calculate how many windows per sequence (assuming T=160)\n",
        "    if window_size is None or stride is None:\n",
        "        # Try to infer from hyperparams if not provided\n",
        "        window_size = hyperparams.get('window', 12)\n",
        "        stride = hyperparams.get('stride', 3)\n",
        "\n",
        "    n_windows_per_seq = (160 - window_size) // stride + 1\n",
        "\n",
        "    # ============= FIX: SORT SAMPLE INDICES =============\n",
        "    # CRITICAL: X_test_seq windows are created in the order that sample_indices\n",
        "    # appear in X_test. We must match this order for aggregation.\n",
        "    # If build_sequences processes sample_indices in sorted order, sort here too.\n",
        "    sample_indices = sorted(sample_indices)  # Ensure alignment\n",
        "    # ============= END FIX =============\n",
        "\n",
        "    # Group predictions by sample_index using majority vote\n",
        "    from collections import Counter\n",
        "    sequence_predictions = []\n",
        "\n",
        "    for idx in range(len(sample_indices)):\n",
        "        # Extract windows for this sequence\n",
        "        start_idx = idx * n_windows_per_seq\n",
        "        end_idx = start_idx + n_windows_per_seq\n",
        "\n",
        "        window_preds = pred_idx_windows[start_idx:end_idx]\n",
        "\n",
        "        # Majority vote across windows\n",
        "        vote_counts = Counter(window_preds)\n",
        "        final_pred_idx = vote_counts.most_common(1)[0][0]\n",
        "\n",
        "        sequence_predictions.append(final_pred_idx)\n",
        "\n",
        "    # Convert indices to labels\n",
        "    pred_labels = [idx2label[int(i)] for i in sequence_predictions]\n",
        "\n",
        "    # --- Build submission DataFrame ---\n",
        "    submission = pd.DataFrame({\n",
        "        \"sample_index\": [str(sid).zfill(3) for sid in sample_indices],\n",
        "        \"label\": pred_labels\n",
        "    })\n",
        "\n",
        "    # --- Build file names ---\n",
        "    run_name = f\"{model_name}_exp\"\n",
        "    csv_path = os.path.join(output_dir, f\"{run_name}_submission.csv\")\n",
        "    json_path = os.path.join(output_dir, f\"{run_name}_config.json\")\n",
        "\n",
        "    # --- Save submission ---\n",
        "    submission.to_csv(csv_path, index=False)\n",
        "\n",
        "    # --- Save hyperparameters as JSON ---\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(hyperparams, f, indent=4)\n",
        "\n",
        "    print(f\"Saved submission at: {csv_path}\")\n",
        "    print(f\"Saved hyperparameters at: {json_path}\")\n",
        "    return submission\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:58:34.216741Z",
          "iopub.execute_input": "2025-11-14T13:58:34.217037Z",
          "iopub.status.idle": "2025-11-14T13:58:34.229577Z",
          "shell.execute_reply.started": "2025-11-14T13:58:34.217011Z",
          "shell.execute_reply": "2025-11-14T13:58:34.228849Z"
        },
        "id": "kwttCMIfvylj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "HwfxL6mpvylj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your hyperparameters as a dict\n",
        "hyperparams = {\n",
        "    \"m\": MODEL,\n",
        "    \"lr\": LEARNING_RATE,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"pat\": PATIENCE,\n",
        "    \"hl\": HIDDEN_LAYERS,\n",
        "    \"hs\": HIDDEN_SIZE,\n",
        "    \"dr\": DROPOUT_RATE,\n",
        "    \"l1\": L1_LAMBDA,\n",
        "    \"l2\": L2_LAMBDA,\n",
        "    'bi': BIDIRECTIONAL,\n",
        "    \"cnn_channels\":CNN_CHANNELS,                  # enables CNN1D preprocessing\n",
        "    \"cnn_kernel_size\":CNN_KERNEL_SIZE,                # usually 3, 5, or 7 works well\n",
        "    \"cnn_dropout\":CNN_DROPOUT                 # (optional; default uses RNN dropout)\n",
        "}\n",
        "\n",
        "model = best_model if \"best_model\" in globals() else rnn_model\n",
        "model_name = f\"{MODEL.lower()}_bi\" if BIDIRECTIONAL else f\"{MODEL.lower()}_f\"\n",
        "\n",
        "# Run and save output\n",
        "submission = save_experiment_output(\n",
        "    model_name=model_name,\n",
        "    hyperparams=hyperparams,\n",
        "    X_test_seq=X_te_win,\n",
        "    label_mapping={'no_pain': 0, 'low_pain': 1, 'high_pain': 2},\n",
        "    sample_indices=X_test[\"sample_index\"].unique(),\n",
        "    output_dir=OUT_DIR,\n",
        "    model=model,\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:58:34.230427Z",
          "iopub.execute_input": "2025-11-14T13:58:34.230654Z",
          "iopub.status.idle": "2025-11-14T13:58:39.730307Z",
          "shell.execute_reply.started": "2025-11-14T13:58:34.230629Z",
          "shell.execute_reply": "2025-11-14T13:58:39.729625Z"
        },
        "id": "v453Ed3Wvylj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "raw",
      "source": [
        "err.tobreak"
      ],
      "metadata": {
        "id": "I2O4JQaUvylj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL TRAINING ON THE FULL DATASET + SUBMISSION GENERATION\n",
        "# ============================================================\n",
        "RNN_TYPE='GRU'\n",
        "\n",
        "# 1. Preprocess full training data\n",
        "DF, _ = preprocess_joints(X_TRAIN.copy())\n",
        "X_train_full, _ = dataset_conversion_type_embed_ready(DF)\n",
        "y_full = Y_TRAIN.copy()\n",
        "\n",
        "\n",
        "labels_full = y_full[\"label\"].map(label_mapping)\n",
        "y_full_np = labels_full.to_numpy()\n",
        "\n",
        "num_classes = np.max(y_full_np) + 1  # For 0-indexed labels: [0, 1, 2] â†’ num_classes=3\n",
        "class_counts = np.bincount(y_full_np, minlength=num_classes)\n",
        "total = len(y_full_np)\n",
        "\n",
        "print(\"\\nTraining class distribution:\")\n",
        "for i, name in enumerate(['no_pain', 'low_pain', 'high_pain']):\n",
        "    print(f\"  {name} ({i}): {class_counts[i]} ({class_counts[i]/total*100:.1f}%)\")\n",
        "\n",
        "final_epochs = best_epoch_number\n",
        "print('final epochs:', final_epochs)\n",
        "\n",
        "\n",
        "# 3. Merge features and labels\n",
        "train_merged = X_train_full.merge(y_full, on=\"sample_index\")\n",
        "\n",
        "# 4. Encode labels numerically BEFORE building sequences\n",
        "label_mapping = {\"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2}\n",
        "train_merged[\"label\"] = train_merged[\"label\"].map(label_mapping)\n",
        "\n",
        "# 5. Normalise feature values\n",
        "\n",
        "scale_columns = [col for col in train_merged.columns if col.startswith(\"joint_\")]\n",
        "# calculate the minimum and maximum values from the training data only\n",
        "mins = train_merged[scale_columns].min()\n",
        "maxs = train_merged[scale_columns].max()\n",
        "\n",
        "# apply normalisation to the specified columns in all datasets (training and validation)\n",
        "for column in scale_columns:\n",
        "\n",
        "    # normalise the training set\n",
        "    train_merged[column] = (train_merged[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "# 6. Build full sequences\n",
        "X_train_seq, y_train_seq = build_sequences(train_merged, train_merged[[\"sample_index\", \"label\"]], window=W_SUGG, stride=S_SUGG)\n",
        "\n",
        "\n",
        "# 7. DataLoader\n",
        "train_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\n",
        "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "\n",
        "# 8. Initialize model with tuned hyperparameters\n",
        "model = RecurrentClassifier(\n",
        "    input_size=X_train_seq.shape[2],\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=HIDDEN_LAYERS,\n",
        "    num_classes=len(label_mapping),\n",
        "    rec_dropout_rate = RECURRENT_DROPOUT,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        "    cnn_channels=CNN_CHANNELS,                  # enables CNN1D preprocessing\n",
        "    cnn_kernel_size=CNN_KERNEL_SIZE,                # usually 3, 5, or 7 works well\n",
        "    cnn_dropout=CNN_DROPOUT,                  # (optional; default uses RNN dropout)\n",
        "    rnn_type=RNN_TYPE\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "# 9. Train model on the entire dataset\n",
        "model, history = fit(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=None,\n",
        "    epochs=final_epochs,\n",
        "    train_criterion=train_criterion,\n",
        "    val_criterion=None,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    patience=PATIENCE,\n",
        "    verbose=True,\n",
        "    evaluation_metric=\"val_f1\",  # ignored since no validation\n",
        "    mode=\"max\",\n",
        "    restore_best_weights=False,\n",
        "    experiment_name=\"final_full_train\"\n",
        ")\n",
        "\n",
        "# 10. Prepare test set for inference\n",
        "X_test = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\n",
        "DF_test, _ = preprocess_joints(X_test.copy())\n",
        "X_test, _ = dataset_conversion_type_embed_ready(DF_test)\n",
        "\n",
        "for column in scale_columns:\n",
        "    # normalise the test set\n",
        "    X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "# Build windowed sequences (this creates N_windows, not N_sequences)\n",
        "X_test_seq, _ = build_sequences(X_test, None, window=W_SUGG, stride=S_SUGG)\n",
        "\n",
        "# 11. Save predictions and configuration\n",
        "OUT_DIR = \"results_FULL_model\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "hyperparams = {\n",
        "    \"m\": MODEL,\n",
        "    \"lr\": LEARNING_RATE,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"pat\": PATIENCE,\n",
        "    \"hl\": HIDDEN_LAYERS,\n",
        "    \"hs\": HIDDEN_SIZE,\n",
        "    \"dr\": DROPOUT_RATE,\n",
        "    \"l1\": L1_LAMBDA,\n",
        "    \"l2\": L2_LAMBDA,\n",
        "    'bi': BIDIRECTIONAL,\n",
        "    'w': W_SUGG,\n",
        "    's': S_SUGG\n",
        "}\n",
        "\n",
        "# MODIFIED: Pass window_size and stride for aggregation\n",
        "submission = save_experiment_output(\n",
        "    model_name=RNN_TYPE.lower(),\n",
        "    hyperparams=hyperparams,\n",
        "    X_test_seq=X_test_seq,  # Windowed data (N_windows, W, F)\n",
        "    label_mapping=label_mapping,\n",
        "    output_dir=OUT_DIR,\n",
        "    sample_indices=X_test[\"sample_index\"].unique(),\n",
        "    model=model,\n",
        "    window_size=W_SUGG,  # NEW: pass for aggregation\n",
        "    stride=S_SUGG  # NEW: pass for aggregation\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Final model trained and submission saved!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T13:58:39.731058Z",
          "iopub.execute_input": "2025-11-14T13:58:39.731265Z",
          "iopub.status.idle": "2025-11-14T14:03:07.635597Z",
          "shell.execute_reply.started": "2025-11-14T13:58:39.731247Z",
          "shell.execute_reply": "2025-11-14T14:03:07.634834Z"
        },
        "id": "U8phcWKQvylj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "erro.toBreak()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.636657Z",
          "iopub.execute_input": "2025-11-14T14:03:07.636951Z",
          "iopub.status.idle": "2025-11-14T14:03:07.687539Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.636925Z",
          "shell.execute_reply": "2025-11-14T14:03:07.686637Z"
        },
        "id": "mk8pq4LQvylj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **K-Shuffle-Split Cross Validation**"
      ],
      "metadata": {
        "id": "CFr4uf5gvylj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def k_shuffle_split_cross_validation_round_rnn(\n",
        "        df, y, epochs, device, k, batch_size, window, stride,\n",
        "        hidden_layers, hidden_size, learning_rate,\n",
        "        rec_dropout_rate, dropout_rate,\n",
        "        rnn_type, bidirectional,\n",
        "        cnn_channels, cnn_kernel_size, cnn_dropout,\n",
        "        use_attention=True,      # <--- NEW\n",
        "        l1_lambda=0, l2_lambda=0, patience=0,\n",
        "        evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True,\n",
        "        writer=None, verbose=10, seed=SEED,\n",
        "        experiment_name=\"\"\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Perform K-fold shuffle split cross-validation with user-based splitting for time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns\n",
        "        epochs: Number of training epochs\n",
        "        device: torch.device for computation\n",
        "        k: Number of cross-validation splits\n",
        "        batch_size: Batch size for training\n",
        "        hidden_layers: Number of recurrent layers\n",
        "        hidden_size: Hidden state dimensionality\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        rec_dropout_rate: Recurrent dropout rate inside RNN\n",
        "        dropout_rate: Dropout rate after RNN\n",
        "        rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
        "        bidirectional: Whether to use bidirectional RNN\n",
        "        cnn_channels: Number of Conv1D channels (None to disable CNN)\n",
        "        cnn_kernel_size: Conv1D kernel size\n",
        "        cnn_dropout: Dropout after Conv1D\n",
        "        use_attention: Whether to use temporal attention in RecurrentClassifier\n",
        "        l1_lambda: L1 regularization coefficient (if used)\n",
        "        l2_lambda: L2 regularization coefficient (weight_decay)\n",
        "        patience: Early stopping patience\n",
        "        evaluation_metric: Metric to monitor for early stopping\n",
        "        mode: 'max' or 'min' for evaluation metric\n",
        "        restore_best_weights: Whether to restore best weights after training\n",
        "        writer: TensorBoard writer\n",
        "        verbose: Verbosity level\n",
        "        seed: Random seed\n",
        "        experiment_name: Name for experiment logging\n",
        "\n",
        "    Returns:\n",
        "        fold_losses: Dict with validation losses for each split\n",
        "        fold_metrics: Dict with validation F1 scores for each split\n",
        "        best_scores: Dict with best F1 score for each split plus mean and std\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise containers for results across all splits\n",
        "    fold_losses = {}\n",
        "    fold_metrics = {}\n",
        "    best_scores = {}\n",
        "    best_epochs_per_fold = {}\n",
        "\n",
        "    DF, _ = preprocess_joints(X_TRAIN.copy())\n",
        "    X_train, _ = dataset_conversion_type_embed_ready(DF)\n",
        "    y = Y_TRAIN.copy()\n",
        "\n",
        "    # Step 1. temporary merge X_train + y_train to create splits ---\n",
        "    train_merged = X_train.merge(y, on=\"sample_index\", how=\"left\")\n",
        "\n",
        "    # Step 2. Retrieve unique indexes ---\n",
        "    unique_samples = train_merged['sample_index'].unique()\n",
        "\n",
        "    num_classes = len(train_merged['label'].unique())\n",
        "\n",
        "    # Prepare stratified K-fold based on label per sample_index\n",
        "    # ---------------------------------------------------------------\n",
        "    label_per_sample = train_merged.groupby(\"sample_index\")[\"label\"].first().map({\n",
        "        \"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2\n",
        "    }).values\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
        "    all_splits = list(skf.split(unique_samples, label_per_sample))\n",
        "    # ---------------------------------------------------------------\n",
        "\n",
        "    # Store initial weights to reset model for each split\n",
        "    initial_state = None\n",
        "\n",
        "    # Iterate through K random splits\n",
        "    for split_idx, (train_idx, val_idx) in enumerate(all_splits):\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"Split {split_idx+1}/{k}\")\n",
        "\n",
        "        train_idxs = unique_samples[train_idx]\n",
        "        val_idxs   = unique_samples[val_idx]\n",
        "\n",
        "        # Split the dataset into training, validation sets based on sample_index\n",
        "        df_train = train_merged[train_merged['sample_index'].isin(train_idxs)].copy()\n",
        "        df_val   = train_merged[train_merged['sample_index'].isin(val_idxs)].copy()\n",
        "\n",
        "        # X: only features\n",
        "        X_train = df_train.drop(columns=['label'])\n",
        "        X_val   = df_val.drop(columns=['label'])\n",
        "\n",
        "        # y: un'etichetta per ogni sequenza\n",
        "        y_train = df_train.groupby(\"sample_index\")[\"label\"].first().values\n",
        "        y_val   = df_val.groupby(\"sample_index\")[\"label\"].first().values\n",
        "\n",
        "        # Define mapping once\n",
        "        label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
        "        inv_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "        # Convert y_train/y_val from string â†’ int\n",
        "        y_train = np.array([label_mapping[l] for l in y_train])\n",
        "        y_val   = np.array([label_mapping[l] for l in y_val])\n",
        "\n",
        "        # ============= COMPUTE CLASS WEIGHTS FROM TRAINING FOLD =============\n",
        "        num_classes_fold = np.max(y_train) + 1\n",
        "        class_counts_fold = np.bincount(y_train, minlength=num_classes_fold)\n",
        "\n",
        "        if verbose > 0:\n",
        "            total_train = len(y_train)\n",
        "            print(f\"  Training fold class distribution:\")\n",
        "            for i, name in enumerate(['no_pain', 'low_pain', 'high_pain']):\n",
        "                print(f\"    {name} ({i}): {class_counts_fold[i]} ({class_counts_fold[i]/total_train*100:.1f}%)\")\n",
        "\n",
        "        max_count_fold = class_counts_fold.max()\n",
        "        class_weights_fold = max_count_fold / class_counts_fold\n",
        "\n",
        "        # Use SQRT dampening for weights (more balanced)\n",
        "        class_weights_fold_dampened = np.sqrt(class_weights_fold)\n",
        "        class_weights = torch.tensor(class_weights_fold_dampened, dtype=torch.float32).to(device)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Class weights (raw): {class_weights_fold}\")\n",
        "            print(f\"  Class weights (sqrt dampened): {class_weights_fold_dampened}\")\n",
        "\n",
        "        # Define training criterion with FocalLabelSmoothing (NEW)\n",
        "        train_criterion = WeightLabelLoss(\n",
        "            num_classes=num_classes_fold,\n",
        "        ).to(device)\n",
        "        # ============= END CLASS WEIGHTS COMPUTATION =============\n",
        "\n",
        "        # Normalise features using training set statistics\n",
        "        scale_columns = [col for col in X_train.columns if col.startswith(\"joint_\")]\n",
        "\n",
        "        train_max = X_train[scale_columns].max()\n",
        "        train_min = X_train[scale_columns].min()\n",
        "\n",
        "        X_train[scale_columns] = (X_train[scale_columns] - train_min) / (train_max - train_min + 1e-8)\n",
        "        X_val[scale_columns]   = (X_val[scale_columns]   - train_min) / (train_max - train_min + 1e-8)\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training set shape: {X_train.shape}\")\n",
        "            print(f\"  Validation set shape: {X_val.shape}\")\n",
        "\n",
        "        y_train_df = pd.DataFrame({\n",
        "            \"sample_index\": X_train[\"sample_index\"].unique(),\n",
        "            \"label\": y_train\n",
        "        })\n",
        "\n",
        "        X_train_seq, y_train_seq = build_sequences(X_train, y_train_df, window, stride)\n",
        "\n",
        "        y_val_df = pd.DataFrame({\n",
        "            \"sample_index\": X_val[\"sample_index\"].unique(),\n",
        "            \"label\": y_val\n",
        "        })\n",
        "\n",
        "        X_val_seq, y_val_seq = build_sequences(X_val, y_val_df, window, stride)\n",
        "\n",
        "        labels_np = y_train_seq  # numpy array of shape (N_train_windows,)\n",
        "\n",
        "        # Count windows per class\n",
        "        class_sample_counts = np.bincount(labels_np, minlength=3)\n",
        "        # e.g. array([N_no, N_low, N_high])\n",
        "\n",
        "        # Inverse frequency weights: rarer class â‡’ larger weight\n",
        "        class_weights_for_sampling = 1.0 / (class_sample_counts + 1e-8)\n",
        "\n",
        "        # Per-sample weights: pick the weight of its class\n",
        "        sample_weights = class_weights_for_sampling[labels_np]\n",
        "\n",
        "        sampler = WeightedRandomSampler(\n",
        "            weights=sample_weights,\n",
        "            num_samples=len(sample_weights),\n",
        "            replacement=True\n",
        "        )\n",
        "\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training sequences shape: {X_train_seq.shape}\")\n",
        "            print(f\"  Validation sequences shape: {X_val_seq.shape}\")\n",
        "\n",
        "        input_shape = X_train_seq.shape[1:]   # shape of a single sequence\n",
        "        num_classes = len(np.unique(y_train))\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Input shape: {input_shape}\")\n",
        "            print(f\"  Num classes: {num_classes}\")\n",
        "\n",
        "        # Create PyTorch datasets\n",
        "        train_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\n",
        "        val_ds   = TensorDataset(torch.from_numpy(X_val_seq),   torch.from_numpy(y_val_seq))\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=True, sampler=sampler, drop_last=False)\n",
        "        val_loader   = make_loader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "        # Initialise model architecture (CNN + RNN + Attention)\n",
        "        model = RecurrentClassifier(\n",
        "            input_size=input_shape[-1],\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=hidden_layers,\n",
        "            num_classes=num_classes,\n",
        "            dropout_rate=dropout_rate,\n",
        "            rec_dropout_rate=rec_dropout_rate,\n",
        "            bidirectional=bidirectional,\n",
        "            cnn_channels=cnn_channels,\n",
        "            cnn_kernel_size=cnn_kernel_size,\n",
        "            cnn_dropout=cnn_dropout,\n",
        "            rnn_type=rnn_type,\n",
        "            use_attention=use_attention\n",
        "        ).to(device)\n",
        "\n",
        "        # Save initial state at 1st split, reset in the following splits\n",
        "        if initial_state is None:\n",
        "            initial_state = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            model.load_state_dict(initial_state)\n",
        "\n",
        "        # Define optimizer with L2 regularization\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "        # Mixed precision\n",
        "        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "        # LR scheduler\n",
        "        scheduler = CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=15,\n",
        "            T_mult=2,\n",
        "            eta_min=1e-6\n",
        "        )\n",
        "        # Create directory for model checkpoints\n",
        "        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
        "\n",
        "        # Validation criterion (standard CrossEntropy without smoothing)\n",
        "        val_criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "        # Train model on current split\n",
        "        model, training_history = fit(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=epochs,\n",
        "            train_criterion=train_criterion,\n",
        "            val_criterion=val_criterion,\n",
        "            optimizer=optimizer,\n",
        "            scaler=split_scaler,\n",
        "            scheduler=scheduler,\n",
        "            device=device,\n",
        "            writer=writer,\n",
        "            patience=patience,\n",
        "            verbose=verbose,\n",
        "            evaluation_metric=evaluation_metric,\n",
        "            mode=mode,\n",
        "            restore_best_weights=restore_best_weights,\n",
        "            experiment_name=experiment_name + \"/split_\" + str(split_idx)\n",
        "        )\n",
        "\n",
        "        # Store results for this split\n",
        "        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n",
        "        fold_metrics[f\"split_{split_idx}\"] = training_history['val_f1']\n",
        "        best_scores[f\"split_{split_idx}\"] = max(training_history['val_f1'])\n",
        "\n",
        "        best_epoch_idx = training_history['val_f1'].index(max(training_history['val_f1']))\n",
        "        best_epochs_per_fold[f\"split_{split_idx}\"] = best_epoch_idx\n",
        "\n",
        "    # Compute mean and standard deviation of best scores across splits\n",
        "    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "    best_scores[\"mean_best_epoch\"] = np.mean([\n",
        "        best_epochs_per_fold[k] for k in best_epochs_per_fold.keys() if k.startswith(\"split_\")\n",
        "    ])\n",
        "\n",
        "    if verbose > 0:\n",
        "        print(f\"Best score: {best_scores['mean']:.4f}Â±{best_scores['std']:.4f}\")\n",
        "\n",
        "    return fold_losses, fold_metrics, best_scores\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.688160Z",
          "iopub.status.idle": "2025-11-14T14:03:07.688527Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.688351Z",
          "shell.execute_reply": "2025-11-14T14:03:07.688389Z"
        },
        "id": "zgLumQcEvylj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hyperparameters Tuning**"
      ],
      "metadata": {
        "id": "YlkUUSOKvylk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def grid_search_cv_rnn(df, y, param_grid, fixed_params, cv_params, verbose=True, n_iter=60,\n",
        "                       checkpoint_every=10, early_prune_threshold=0.8):\n",
        "    \"\"\"\n",
        "    Execute grid search with K-shuffle-split cross-validation for RNN models on time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns\n",
        "        param_grid: Dict of parameters to test, e.g. {'batch_size': [16, 32], 'rnn_type': ['LSTM', 'GRU']}\n",
        "        fixed_params: Dict of fixed hyperparameters (hidden_size, learning_rate, window_size, stride, etc.)\n",
        "        cv_params: Dict of CV settings (epochs, k, patience, criterion, scaler, device, etc.)\n",
        "        verbose: Print progress for each configuration\n",
        "        checkpoint_every: Save results every N iterations (default 10)\n",
        "        early_prune_threshold: Stop config if score < best_score * threshold after 2 folds (default 0.7)\n",
        "\n",
        "    Returns:\n",
        "        results: Dict with scores for each configuration\n",
        "        best_config: Dict with best hyperparameter combination\n",
        "        best_score: Best mean F1 score achieved\n",
        "        best_config_epochs: Mean best epoch from best configuration\n",
        "    \"\"\"\n",
        "    # Generate all parameter combinations\n",
        "    param_names = list(param_grid.keys())\n",
        "    param_values = list(param_grid.values())\n",
        "    all_combinations = list(product(*param_values))\n",
        "\n",
        "    total_possible = len(all_combinations)\n",
        "\n",
        "\n",
        "    results = {}\n",
        "    best_score = -np.inf\n",
        "    best_config = None\n",
        "    best_config_epochs = None\n",
        "\n",
        "    # If n_iter is None, set it to total_possible to run a full grid search\n",
        "    if n_iter is None:\n",
        "        n_iter = total_possible\n",
        "\n",
        "    # Se n_iter Ã¨ minore del totale, scegli N combinazioni a caso\n",
        "    if n_iter < total_possible:\n",
        "        print(f\"--- Eseguendo RANDOM SEARCH ---\")\n",
        "        print(f\"Selezionate {n_iter} combinazioni casuali su {total_possible} possibili.\")\n",
        "        # Use seed for reproducibility\n",
        "        random.seed(cv_params.get('seed', 42))\n",
        "        combinations = random.sample(all_combinations, n_iter)\n",
        "    else:\n",
        "        print(f\"--- Eseguendo GRID SEARCH ---\")\n",
        "        print(f\"Testando tutte le {total_possible} combinazioni.\")\n",
        "        combinations = all_combinations\n",
        "\n",
        "    for idx, combo in enumerate(combinations, 1):\n",
        "        # Create current configuration dict\n",
        "        current_config = dict(zip(param_names, combo))\n",
        "        config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n",
        "\n",
        "        if verbose:\n",
        "            if n_iter < total_possible:\n",
        "                print(f\"\\nConfiguration {idx}/{n_iter}:\")\n",
        "            else:\n",
        "                 print(f\"\\nConfiguration {idx}/{total_possible}:\")\n",
        "            for param, value in current_config.items():\n",
        "                print(f\"  {param}: {value}\")\n",
        "\n",
        "        # Merge current config with fixed parameters\n",
        "        run_params = {**fixed_params, **current_config}\n",
        "\n",
        "        # Execute cross-validation\n",
        "        _, _, fold_scores = k_shuffle_split_cross_validation_round_rnn(\n",
        "            df=df,\n",
        "            y=y,\n",
        "            experiment_name=config_str,\n",
        "            **run_params,\n",
        "            **cv_params\n",
        "        )\n",
        "\n",
        "        # Early pruning: skip config if performance is too poor after initial folds\n",
        "        if best_score > -np.inf:  # Only prune after we have a baseline\n",
        "            partial_scores = fold_scores.get('scores', [])\n",
        "            if len(partial_scores) >= 2:\n",
        "                partial_mean = np.mean(partial_scores[:2])\n",
        "                if partial_mean < best_score * early_prune_threshold:\n",
        "                    if verbose:\n",
        "                        print(f\"  [PRUNED] Score {partial_mean:.4f} < {best_score * early_prune_threshold:.4f} (threshold), skipping.\")\n",
        "                    results[config_str] = fold_scores  # Still save for analysis\n",
        "                    continue\n",
        "\n",
        "        # Store results\n",
        "        results[config_str] = fold_scores\n",
        "\n",
        "        # Track best configuration\n",
        "        if fold_scores[\"mean\"] > best_score:\n",
        "            best_score = fold_scores[\"mean\"]\n",
        "            best_config = current_config.copy()\n",
        "            best_config_epochs = fold_scores[\"mean_best_epoch\"]\n",
        "            if verbose:\n",
        "                print(\"  NEW BEST SCORE!\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  F1 Score: {fold_scores['mean']:.4f}Â±{fold_scores['std']:.4f}\")\n",
        "\n",
        "        # Save checkpoint periodically\n",
        "        if idx % checkpoint_every == 0:\n",
        "            checkpoint_path = f'grid_search_checkpoint_{idx}.pkl'\n",
        "            with open(checkpoint_path, 'wb') as f:\n",
        "                pickle.dump({\n",
        "                    'results': results,\n",
        "                    'best_config': best_config,\n",
        "                    'best_score': best_score,\n",
        "                    'best_config_epochs': best_config_epochs,\n",
        "                    'completed_idx': idx,\n",
        "                    'total': n_iter if n_iter < total_possible else total_possible\n",
        "                }, f)\n",
        "            if verbose:\n",
        "                print(f\"  [CHECKPOINT] Salvato in {checkpoint_path}\")\n",
        "\n",
        "    # Final save\n",
        "    final_path = 'grid_search_final.pkl'\n",
        "    with open(final_path, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'results': results,\n",
        "            'best_config': best_config,\n",
        "            'best_score': best_score,\n",
        "            'best_config_epochs': best_config_epochs\n",
        "        }, f)\n",
        "    print(f\"\\n[COMPLETATO] Risultati salvati in {final_path}\")\n",
        "\n",
        "    return results, best_config, best_score, best_config_epochs\n",
        "\n",
        "\n",
        "def plot_top_configurations_rnn(results, k_splits, top_n=5, figsize=(14, 7)):\n",
        "    \"\"\"\n",
        "    Visualise top N RNN configurations with boxplots of F1 scores across CV splits.\n",
        "\n",
        "    Args:\n",
        "        results: Dict of results from grid_search_cv_rnn\n",
        "        k_splits: Number of CV splits used\n",
        "        top_n: Number of top configurations to display\n",
        "        figsize: Figure size tuple\n",
        "    \"\"\"\n",
        "    # Sort by mean score\n",
        "    config_scores = {name: data['mean'] for name, data in results.items()}\n",
        "    sorted_configs = sorted(config_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select top N\n",
        "    top_configs = sorted_configs[:min(top_n, len(sorted_configs))]\n",
        "\n",
        "    # Prepare boxplot data\n",
        "    boxplot_data = []\n",
        "    labels = []\n",
        "\n",
        "    # Define a dictionary for replacements, ordered to handle prefixes correctly\n",
        "    replacements = {\n",
        "        'window_':'W=',\n",
        "        'stride_':'S=',\n",
        "        'batch_size_': 'BS=',\n",
        "        'learning_rate_': '\\nLR=',\n",
        "        'hidden_layers_': '\\nHL=',\n",
        "        'hidden_size_': '\\nHS=',\n",
        "        'dropout_rate_': '\\nDR=',\n",
        "        'rnn_type_': '\\nRNN=',\n",
        "        'bidirectional_': '\\nBIDIR=',\n",
        "        'l1_lambda_': '\\nL1=',\n",
        "        'l2_lambda_': '\\nL2='\n",
        "    }\n",
        "\n",
        "    # Replacements for separators\n",
        "    separator_replacements = {\n",
        "        '_window_':'\\nW=',\n",
        "        '_stride_':'\\nS=',\n",
        "        '_learning_rate_': '\\nLR=',\n",
        "        '_hidden_layers_': '\\nHL=',\n",
        "        '_hidden_size_': '\\nHS=',\n",
        "        '_dropout_rate_': '\\nDR=',\n",
        "        '_rnn_type_': '\\nRNN=',\n",
        "        '_bidirectional_': '\\nBIDIR=',\n",
        "        '_l1_lambda_': '\\nL1=',\n",
        "        '_l2_lambda_': '\\nL2=',\n",
        "        '_': ''\n",
        "    }\n",
        "\n",
        "    for config_name, mean_score in top_configs:\n",
        "        # Extract best score from each split (auto-detect number of splits)\n",
        "        split_scores = []\n",
        "        for i in range(k_splits):\n",
        "            if f'split_{i}' in results[config_name]:\n",
        "                split_scores.append(results[config_name][f'split_{i}'])\n",
        "        boxplot_data.append(split_scores)\n",
        "\n",
        "        # Verify we have the expected number of splits\n",
        "        if len(split_scores) != k_splits:\n",
        "            print(f\"Warning: Config {config_name} has {len(split_scores)} splits, expected {k_splits}\")\n",
        "\n",
        "        # Create readable label using the replacements dictionary\n",
        "        readable_label = config_name\n",
        "        for old, new in replacements.items():\n",
        "            readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        # Apply separator replacements\n",
        "        for old, new in separator_replacements.items():\n",
        "             readable_label = readable_label.replace(old, new)\n",
        "\n",
        "        labels.append(f\"{readable_label}\\n(Î¼={mean_score:.3f})\")\n",
        "\n",
        "    # Create plot\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    bp = ax.boxplot(boxplot_data, labels=labels, patch_artist=True,\n",
        "                    showmeans=True, meanline=True)\n",
        "\n",
        "    # Styling\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    # Highlight best configuration\n",
        "    ax.get_xticklabels()[0].set_fontweight('bold')\n",
        "\n",
        "    ax.set_ylabel('F1 Score')\n",
        "    ax.set_xlabel('Configuration')\n",
        "    ax.set_title(f'Top {len(top_configs)} RNN Configurations - F1 Score Distribution Across {k_splits} Splits')\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "    plt.xticks(rotation=0, ha='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.690057Z",
          "iopub.status.idle": "2025-11-14T14:03:07.690382Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.690216Z",
          "shell.execute_reply": "2025-11-14T14:03:07.690233Z"
        },
        "id": "J9RL6Wkjvylk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "DF, _ = preprocess_joints(X_TRAIN.copy())\n",
        "X_train, _ = dataset_conversion_type_embed_ready(DF)\n",
        "y = Y_TRAIN.copy()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.691462Z",
          "iopub.status.idle": "2025-11-14T14:03:07.691798Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.691641Z",
          "shell.execute_reply": "2025-11-14T14:03:07.691660Z"
        },
        "id": "D5yAqFMfvylk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "K = 5\n",
        "VAL_FRAC = 0.20  # 20% sequences for validation in each split\n",
        "N = X_train['sample_index'].nunique()  # Returns integer\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.693062Z",
          "iopub.status.idle": "2025-11-14T14:03:07.693302Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.693183Z",
          "shell.execute_reply": "2025-11-14T14:03:07.693193Z"
        },
        "id": "KHZBY4wPvylk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "param_grid_phase1 = {\n",
        "    'batch_size':     [128, 256],\n",
        "    'learning_rate':  [3e-4, 5e-4],\n",
        "\n",
        "    # RNN regularisation\n",
        "    'dropout_rate':      [0.3],     # post-RNN dropout\n",
        "    'rec_dropout_rate':  [0.2],     # recurrent dropout inside GRU\n",
        "\n",
        "    # Fixed to your proven baseline for now\n",
        "    'hidden_size':   [192],\n",
        "    'hidden_layers': [2],\n",
        "    'l2_lambda':     [3e-5],\n",
        "    'window':        [12],\n",
        "    'stride':        [3],\n",
        "    'rnn_type':      ['GRU'],\n",
        "    'bidirectional': [True],\n",
        "    'epochs':        [150],\n",
        "    'patience':      [10],\n",
        "\n",
        "    # CNN + attention\n",
        "    'cnn_channels':    [48],\n",
        "    'cnn_kernel_size': [3],\n",
        "    'cnn_dropout':     [0.35],\n",
        "    'use_attention':   [True],\n",
        "}\n",
        "\n",
        "fixed_params_phase1 = {\n",
        "    'l1_lambda': L1_LAMBDA,\n",
        "}\n",
        "\n",
        "cv_params = {\n",
        "    'device': device,\n",
        "    'k': K,\n",
        "    'verbose': 0,\n",
        "    'seed': SEED,\n",
        "}\n",
        "\n",
        "results1, best_config1, best_score1, best_epochs1 = grid_search_cv_rnn(\n",
        "    df=df_train,\n",
        "    y=y_train,\n",
        "    param_grid=param_grid_phase1,\n",
        "    fixed_params=fixed_params_phase1,\n",
        "    cv_params=cv_params,\n",
        "    verbose=True,\n",
        "    n_iter=None,\n",
        "    checkpoint_every=10,\n",
        "    early_prune_threshold=0.70\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"BEST CONFIGURATION (PHASE 1):\")\n",
        "for k, v in best_config1.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(f\"Best F1 Score: {best_score1:.4f}\")\n",
        "print(f\"Best Epochs: {best_epochs1:.1f}\")\n",
        "print(f\"{'='*60}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.694888Z",
          "iopub.status.idle": "2025-11-14T14:03:07.695235Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.695059Z",
          "shell.execute_reply": "2025-11-14T14:03:07.695074Z"
        },
        "id": "3biWU94zvylk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise results\n",
        "plot_top_configurations_rnn(results1, k_splits=K, top_n=5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.697266Z",
          "iopub.status.idle": "2025-11-14T14:03:07.698151Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.697968Z",
          "shell.execute_reply": "2025-11-14T14:03:07.697985Z"
        },
        "id": "frZaWHhDvylk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL TRAINING ON THE FULL DATASET + SUBMISSION GENERATION\n",
        "# ============================================================\n",
        "\n",
        "best_config = best_config1\n",
        "# 1. Preprocess full training data\n",
        "DF, _ = preprocess_joints(X_TRAIN.copy())\n",
        "X_train_full, _ = dataset_conversion_type_embed_ready(DF)\n",
        "y_full = Y_TRAIN.copy()\n",
        "\n",
        "\n",
        "labels_full = y_full[\"label\"].map(label_mapping)\n",
        "y_full_np = labels_full.to_numpy()\n",
        "\n",
        "num_classes = np.max(y_full_np) + 1  # For 0-indexed labels: [0, 1, 2] â†’ num_classes=3\n",
        "class_counts = np.bincount(y_full_np, minlength=num_classes)\n",
        "total = len(y_full_np)\n",
        "\n",
        "print(\"\\nTraining class distribution:\")\n",
        "for i, name in enumerate(['no_pain', 'low_pain', 'high_pain']):\n",
        "    print(f\"  {name} ({i}): {class_counts[i]} ({class_counts[i]/total*100:.1f}%)\")\n",
        "\n",
        "# Compute class weights using the 'maximum count' rule (makes the most common class weight=1.0)\n",
        "max_count = class_counts.max()\n",
        "class_weights = max_count / class_counts\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# For use in PyTorch or your label smoothing class:\n",
        "class_weights_dampened = np.sqrt(class_weights)\n",
        "class_weights = torch.tensor(class_weights_dampened, dtype=torch.float32)\n",
        "\n",
        "# Define training criterion with class-aware label smoothing\n",
        "train_criterion = WeightLabelLoss(\n",
        "    num_classes=3,\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# 2. Combine the best hyperparameters (found in grid search)\n",
        "final_best_params = {**fixed_params, **best_config}\n",
        "\n",
        "# Use mean best epoch from CV + 20% buffer (more data â†’ slightly more epochs)\n",
        "final_epochs = int(best_epochs * 1)\n",
        "print(f\"\\nðŸ“Š CV found best performance at epoch {best_epochs:.1f} (average)\")\n",
        "print(f\"   Final training will use {final_epochs} epochs (20% buffer for full dataset)\")\n",
        "\n",
        "final_best_params['epochs'] = final_epochs\n",
        "\n",
        "\n",
        "print(\"Training final model with best configuration:\")\n",
        "for k, v in final_best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# 3. Merge features and labels\n",
        "train_merged = X_train_full.merge(y_full, on=\"sample_index\")\n",
        "\n",
        "# 4. Encode labels numerically BEFORE building sequences\n",
        "label_mapping = {\"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2}\n",
        "train_merged[\"label\"] = train_merged[\"label\"].map(label_mapping)\n",
        "\n",
        "\n",
        "\n",
        "# 5. Normalise feature values\n",
        "\n",
        "scale_columns = [col for col in train_merged.columns if col.startswith(\"joint_\")]\n",
        "# calculate the minimum and maximum values from the training data only\n",
        "mins = X_train[scale_columns].min()\n",
        "maxs = X_train[scale_columns].max()\n",
        "\n",
        "# apply normalisation to the specified columns in all datasets (training and validation)\n",
        "for column in scale_columns:\n",
        "\n",
        "    # normalise the training set\n",
        "    train_merged[column] = (train_merged[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "# 6. Build full sequences\n",
        "X_train_seq, y_train_seq = build_sequences(train_merged, train_merged[[\"sample_index\", \"label\"]], window=final_best_params[\"window\"], stride=final_best_params[\"stride\"])\n",
        "\n",
        "labels_np = y_train_seq  # numpy array of shape (N_train_windows,)\n",
        "\n",
        "# Count windows per class\n",
        "class_sample_counts = np.bincount(labels_np, minlength=3)\n",
        "# e.g. array([N_no, N_low, N_high])\n",
        "\n",
        "# Inverse frequency weights: rarer class â‡’ larger weight\n",
        "class_weights_for_sampling = 1.0 / (class_sample_counts + 1e-8)\n",
        "\n",
        "# Per-sample weights: pick the weight of its class\n",
        "sample_weights = class_weights_for_sampling[labels_np]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "# 7. DataLoader\n",
        "train_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\n",
        "train_loader = make_loader(train_ds, batch_size=final_best_params[\"batch_size\"], sampler=sampler, shuffle=True, drop_last=False)\n",
        "\n",
        "# 8. Initialize model with tuned hyperparameters\n",
        "model = RecurrentClassifier(\n",
        "    input_size=X_train_seq.shape[2],\n",
        "    hidden_size=final_best_params[\"hidden_size\"],\n",
        "    num_layers=final_best_params[\"hidden_layers\"],\n",
        "    num_classes=len(label_mapping),\n",
        "    rec_dropout_rate = RECURRENT_DROPOUT,\n",
        "    dropout_rate=final_best_params[\"dropout_rate\"],\n",
        "    bidirectional=final_best_params[\"bidirectional\"],\n",
        "    rnn_type=final_best_params[\"rnn_type\"],\n",
        "    cnn_channels=final_best_params[\"cnn_channels\"],                  # enables CNN1D preprocessing\n",
        "    cnn_kernel_size=final_best_params[\"cnn_kernel_size\"],                # usually 3, 5, or 7 works well\n",
        "    cnn_dropout=final_best_params[\"cnn_dropout\"],                  # (optional; default uses RNN dropout)\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=final_best_params[\"learning_rate\"], weight_decay=final_best_params['l2_lambda'])\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "# 9. Train model on the entire dataset\n",
        "model, history = fit(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=None,\n",
        "    epochs=final_epochs,\n",
        "    train_criterion=train_criterion,\n",
        "    val_criterion=None,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    patience=final_best_params[\"patience\"],\n",
        "    verbose=True,\n",
        "    evaluation_metric=\"val_f1\",  # ignored since no validation\n",
        "    mode=\"max\",\n",
        "    restore_best_weights=False,\n",
        "    experiment_name=\"final_full_train\"\n",
        ")\n",
        "\n",
        "# 10. Prepare test set for inference\n",
        "X_test = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\n",
        "DF_test, _ = preprocess_joints(X_test.copy())\n",
        "X_test, _ = dataset_conversion_type_embed_ready(DF_test)\n",
        "\n",
        "for column in scale_columns:\n",
        "    # normalise the test set\n",
        "    X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "# Build windowed sequences (this creates N_windows, not N_sequences)\n",
        "X_test_seq, _ = build_sequences(X_test, None, window=final_best_params[\"window\"], stride=final_best_params[\"stride\"])\n",
        "\n",
        "# 11. Save predictions and configuration\n",
        "OUT_DIR = \"results_best_model1\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "hyperparams = final_best_params.copy()\n",
        "hyperparams.update({\n",
        "    \"best_cv_f1\": best_score\n",
        "})\n",
        "\n",
        "# MODIFIED: Pass window_size and stride for aggregation\n",
        "submission = save_experiment_output(\n",
        "    model_name=final_best_params[\"rnn_type\"].lower(),\n",
        "    hyperparams=hyperparams,\n",
        "    X_test_seq=X_test_seq,  # Windowed data (N_windows, W, F)\n",
        "    label_mapping=label_mapping,\n",
        "    output_dir=OUT_DIR,\n",
        "    sample_indices=X_test[\"sample_index\"].unique(),\n",
        "    model=model,\n",
        "    window_size=final_best_params[\"window\"],  # NEW: pass for aggregation\n",
        "    stride=final_best_params[\"stride\"]  # NEW: pass for aggregation\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Final model trained and submission saved!\")"
      ],
      "metadata": {
        "id": "YKCbi9fF7MeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "param_grid_phase2 = {\n",
        "    'batch_size':     [best_config1['batch_size']],\n",
        "    'learning_rate':  [best_config1['learning_rate']],\n",
        "\n",
        "    # Explore architecture depth/width\n",
        "    'hidden_size':   [128, 192, 256],\n",
        "    'hidden_layers': [2, 3],\n",
        "\n",
        "    # Keep same regularisation as best from phase1\n",
        "    'dropout_rate':     [best_config1['dropout_rate']],\n",
        "    'rec_dropout_rate': [best_config1['rec_dropout_rate']],\n",
        "\n",
        "    # CNN + attention fixed for now (from phase1 best)\n",
        "    'cnn_channels':    [best_config1['cnn_channels']],\n",
        "    'cnn_kernel_size': [best_config1['cnn_kernel_size']],\n",
        "    'cnn_dropout':     [best_config1['cnn_dropout']],\n",
        "    'use_attention':   [best_config1['use_attention']],\n",
        "\n",
        "    # Fixed\n",
        "    'l2_lambda':     [best_config1['l2_lambda']],\n",
        "    'window':        [12],\n",
        "    'stride':        [3],\n",
        "    'rnn_type':      ['GRU'],\n",
        "    'bidirectional': [True],\n",
        "    'epochs':        [150],\n",
        "    'patience':      [10],\n",
        "}\n",
        "\n",
        "fixed_params_phase2 = {\n",
        "    'l1_lambda': L1_LAMBDA,\n",
        "}\n",
        "\n",
        "results2, best_config2, best_score2, best_epochs2 = grid_search_cv_rnn(\n",
        "    df=df_train,\n",
        "    y=y_train,\n",
        "    param_grid=param_grid_phase2,\n",
        "    fixed_params=fixed_params_phase2,\n",
        "    cv_params=cv_params,\n",
        "    verbose=True,\n",
        "    n_iter=None,\n",
        "    checkpoint_every=10,\n",
        "    early_prune_threshold=0.70\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"BEST CONFIGURATION (PHASE 2):\")\n",
        "for k, v in best_config2.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(f\"Best F1 Score: {best_score2:.4f}\")\n",
        "print(f\"Best Epochs: {best_epochs2:.1f}\")\n",
        "print(f\"{'='*60}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.699089Z",
          "iopub.status.idle": "2025-11-14T14:03:07.699415Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.699246Z",
          "shell.execute_reply": "2025-11-14T14:03:07.699263Z"
        },
        "id": "oUovcxi0vylk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise results\n",
        "plot_top_configurations_rnn(results2, k_splits=K, top_n=5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.700418Z",
          "iopub.status.idle": "2025-11-14T14:03:07.700680Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.700568Z",
          "shell.execute_reply": "2025-11-14T14:03:07.700578Z"
        },
        "id": "z4DiW4cvvylk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL TRAINING ON THE FULL DATASET + SUBMISSION GENERATION\n",
        "# ============================================================\n",
        "\n",
        "best_config = best_config2\n",
        "# 1. Preprocess full training data\n",
        "DF, _ = preprocess_joints(X_TRAIN.copy())\n",
        "X_train_full, _ = dataset_conversion_type_embed_ready(DF)\n",
        "y_full = Y_TRAIN.copy()\n",
        "\n",
        "\n",
        "labels_full = y_full[\"label\"].map(label_mapping)\n",
        "y_full_np = labels_full.to_numpy()\n",
        "\n",
        "num_classes = np.max(y_full_np) + 1  # For 0-indexed labels: [0, 1, 2] â†’ num_classes=3\n",
        "class_counts = np.bincount(y_full_np, minlength=num_classes)\n",
        "total = len(y_full_np)\n",
        "\n",
        "print(\"\\nTraining class distribution:\")\n",
        "for i, name in enumerate(['no_pain', 'low_pain', 'high_pain']):\n",
        "    print(f\"  {name} ({i}): {class_counts[i]} ({class_counts[i]/total*100:.1f}%)\")\n",
        "\n",
        "# Compute class weights using the 'maximum count' rule (makes the most common class weight=1.0)\n",
        "max_count = class_counts.max()\n",
        "class_weights = max_count / class_counts\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# For use in PyTorch or your label smoothing class:\n",
        "class_weights_dampened = np.sqrt(class_weights)\n",
        "class_weights = torch.tensor(class_weights_dampened, dtype=torch.float32)\n",
        "\n",
        "# Define training criterion with class-aware label smoothing\n",
        "train_criterion = WeightLabelLoss(\n",
        "    num_classes=3,\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# 2. Combine the best hyperparameters (found in grid search)\n",
        "final_best_params = {**fixed_params, **best_config}\n",
        "\n",
        "# Use mean best epoch from CV + 20% buffer (more data â†’ slightly more epochs)\n",
        "final_epochs = int(best_epochs * 1)\n",
        "print(f\"\\nðŸ“Š CV found best performance at epoch {best_epochs:.1f} (average)\")\n",
        "print(f\"   Final training will use {final_epochs} epochs (20% buffer for full dataset)\")\n",
        "\n",
        "final_best_params['epochs'] = final_epochs\n",
        "\n",
        "\n",
        "print(\"Training final model with best configuration:\")\n",
        "for k, v in final_best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# 3. Merge features and labels\n",
        "train_merged = X_train_full.merge(y_full, on=\"sample_index\")\n",
        "\n",
        "# 4. Encode labels numerically BEFORE building sequences\n",
        "label_mapping = {\"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2}\n",
        "train_merged[\"label\"] = train_merged[\"label\"].map(label_mapping)\n",
        "\n",
        "\n",
        "\n",
        "# 5. Normalise feature values\n",
        "\n",
        "scale_columns = [col for col in train_merged.columns if col.startswith(\"joint_\")]\n",
        "# calculate the minimum and maximum values from the training data only\n",
        "mins = X_train[scale_columns].min()\n",
        "maxs = X_train[scale_columns].max()\n",
        "\n",
        "# apply normalisation to the specified columns in all datasets (training and validation)\n",
        "for column in scale_columns:\n",
        "\n",
        "    # normalise the training set\n",
        "    train_merged[column] = (train_merged[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "# 6. Build full sequences\n",
        "X_train_seq, y_train_seq = build_sequences(train_merged, train_merged[[\"sample_index\", \"label\"]], window=final_best_params[\"window\"], stride=final_best_params[\"stride\"])\n",
        "\n",
        "labels_np = y_train_seq  # numpy array of shape (N_train_windows,)\n",
        "\n",
        "# Count windows per class\n",
        "class_sample_counts = np.bincount(labels_np, minlength=3)\n",
        "# e.g. array([N_no, N_low, N_high])\n",
        "\n",
        "# Inverse frequency weights: rarer class â‡’ larger weight\n",
        "class_weights_for_sampling = 1.0 / (class_sample_counts + 1e-8)\n",
        "\n",
        "# Per-sample weights: pick the weight of its class\n",
        "sample_weights = class_weights_for_sampling[labels_np]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "# 7. DataLoader\n",
        "train_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\n",
        "train_loader = make_loader(train_ds, batch_size=final_best_params[\"batch_size\"], sampler=sampler, shuffle=True, drop_last=False)\n",
        "\n",
        "# 8. Initialize model with tuned hyperparameters\n",
        "model = RecurrentClassifier(\n",
        "    input_size=X_train_seq.shape[2],\n",
        "    hidden_size=final_best_params[\"hidden_size\"],\n",
        "    num_layers=final_best_params[\"hidden_layers\"],\n",
        "    num_classes=len(label_mapping),\n",
        "    rec_dropout_rate = RECURRENT_DROPOUT,\n",
        "    dropout_rate=final_best_params[\"dropout_rate\"],\n",
        "    bidirectional=final_best_params[\"bidirectional\"],\n",
        "    rnn_type=final_best_params[\"rnn_type\"],\n",
        "    cnn_channels=final_best_params[\"cnn_channels\"],                  # enables CNN1D preprocessing\n",
        "    cnn_kernel_size=final_best_params[\"cnn_kernel_size\"],                # usually 3, 5, or 7 works well\n",
        "    cnn_dropout=final_best_params[\"cnn_dropout\"],                  # (optional; default uses RNN dropout)\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=final_best_params[\"learning_rate\"], weight_decay=final_best_params['l2_lambda'])\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "# 9. Train model on the entire dataset\n",
        "model, history = fit(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=None,\n",
        "    epochs=final_epochs,\n",
        "    train_criterion=train_criterion,\n",
        "    val_criterion=None,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    patience=final_best_params[\"patience\"],\n",
        "    verbose=True,\n",
        "    evaluation_metric=\"val_f1\",  # ignored since no validation\n",
        "    mode=\"max\",\n",
        "    restore_best_weights=False,\n",
        "    experiment_name=\"final_full_train\"\n",
        ")\n",
        "\n",
        "# 10. Prepare test set for inference\n",
        "X_test = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\n",
        "DF_test, _ = preprocess_joints(X_test.copy())\n",
        "X_test, _ = dataset_conversion_type_embed_ready(DF_test)\n",
        "\n",
        "for column in scale_columns:\n",
        "    # normalise the test set\n",
        "    X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "# Build windowed sequences (this creates N_windows, not N_sequences)\n",
        "X_test_seq, _ = build_sequences(X_test, None, window=final_best_params[\"window\"], stride=final_best_params[\"stride\"])\n",
        "\n",
        "# 11. Save predictions and configuration\n",
        "OUT_DIR = \"results_best_model2\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "hyperparams = final_best_params.copy()\n",
        "hyperparams.update({\n",
        "    \"best_cv_f1\": best_score\n",
        "})\n",
        "\n",
        "# MODIFIED: Pass window_size and stride for aggregation\n",
        "submission = save_experiment_output(\n",
        "    model_name=final_best_params[\"rnn_type\"].lower(),\n",
        "    hyperparams=hyperparams,\n",
        "    X_test_seq=X_test_seq,  # Windowed data (N_windows, W, F)\n",
        "    label_mapping=label_mapping,\n",
        "    output_dir=OUT_DIR,\n",
        "    sample_indices=X_test[\"sample_index\"].unique(),\n",
        "    model=model,\n",
        "    window_size=final_best_params[\"window\"],  # NEW: pass for aggregation\n",
        "    stride=final_best_params[\"stride\"]  # NEW: pass for aggregation\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Final model trained and submission saved!\")"
      ],
      "metadata": {
        "id": "fPHeZgfU68Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "param_grid_phase3 = {\n",
        "    'batch_size':     [best_config2['batch_size']],\n",
        "    'learning_rate':  [best_config2['learning_rate']],\n",
        "    'hidden_size':    [best_config2['hidden_size']],\n",
        "    'hidden_layers':  [best_config2['hidden_layers']],\n",
        "    'dropout_rate':   [best_config2['dropout_rate']],\n",
        "    'rec_dropout_rate':[best_config2['rec_dropout_rate']],\n",
        "\n",
        "    # CNN + attention\n",
        "    'cnn_channels':    [best_config2['cnn_channels']],\n",
        "    'cnn_kernel_size': [best_config2['cnn_kernel_size']],\n",
        "    'cnn_dropout':     [best_config2['cnn_dropout']],\n",
        "    'use_attention':   [best_config2['use_attention']],\n",
        "\n",
        "    # Now explore window/stride/L2 (window/stride fixed if you trust ACF)\n",
        "    'window':   [12],\n",
        "    'stride':   [3],\n",
        "    'l2_lambda':[3e-5, 1e-4, 3e-4],\n",
        "\n",
        "    'rnn_type':     ['GRU'],\n",
        "    'bidirectional':[True],\n",
        "    'epochs':       [150],\n",
        "    'patience':     [10],\n",
        "}\n",
        "\n",
        "fixed_params_phase3 = {\n",
        "    'l1_lambda': L1_LAMBDA,\n",
        "}\n",
        "\n",
        "results3, best_config3, best_score3, best_epochs3 = grid_search_cv_rnn(\n",
        "    df=df_train,\n",
        "    y=y_train,\n",
        "    param_grid=param_grid_phase3,\n",
        "    fixed_params=fixed_params_phase3,\n",
        "    cv_params=cv_params,\n",
        "    verbose=True,\n",
        "    n_iter=13,\n",
        "    checkpoint_every=10,\n",
        "    early_prune_threshold=0.70\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"BEST CONFIGURATION (PHASE 3):\")\n",
        "for k, v in best_config3.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(f\"Best F1 Score: {best_score3:.4f}\")\n",
        "print(f\"Best Epochs: {best_epochs3:.1f}\")\n",
        "print(f\"{'='*60}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.701612Z",
          "iopub.status.idle": "2025-11-14T14:03:07.701940Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.701767Z",
          "shell.execute_reply": "2025-11-14T14:03:07.701780Z"
        },
        "id": "xf-XAfp8vylk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise results\n",
        "plot_top_configurations_rnn(results3, k_splits=K, top_n=5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.703032Z",
          "iopub.status.idle": "2025-11-14T14:03:07.703239Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.703140Z",
          "shell.execute_reply": "2025-11-14T14:03:07.703149Z"
        },
        "id": "NK0qullqvylk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL TRAINING ON THE FULL DATASET + SUBMISSION GENERATION\n",
        "# ============================================================\n",
        "\n",
        "best_config = best_config3\n",
        "# 1. Preprocess full training data\n",
        "DF, _ = preprocess_joints(X_TRAIN.copy())\n",
        "X_train_full, _ = dataset_conversion_type_embed_ready(DF)\n",
        "y_full = Y_TRAIN.copy()\n",
        "\n",
        "\n",
        "labels_full = y_full[\"label\"].map(label_mapping)\n",
        "y_full_np = labels_full.to_numpy()\n",
        "\n",
        "num_classes = np.max(y_full_np) + 1  # For 0-indexed labels: [0, 1, 2] â†’ num_classes=3\n",
        "class_counts = np.bincount(y_full_np, minlength=num_classes)\n",
        "total = len(y_full_np)\n",
        "\n",
        "print(\"\\nTraining class distribution:\")\n",
        "for i, name in enumerate(['no_pain', 'low_pain', 'high_pain']):\n",
        "    print(f\"  {name} ({i}): {class_counts[i]} ({class_counts[i]/total*100:.1f}%)\")\n",
        "\n",
        "# Compute class weights using the 'maximum count' rule (makes the most common class weight=1.0)\n",
        "max_count = class_counts.max()\n",
        "class_weights = max_count / class_counts\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# For use in PyTorch or your label smoothing class:\n",
        "class_weights_dampened = np.sqrt(class_weights)\n",
        "class_weights = torch.tensor(class_weights_dampened, dtype=torch.float32)\n",
        "\n",
        "# Define training criterion with class-aware label smoothing\n",
        "train_criterion = WeightLabelLoss(\n",
        "    num_classes=3,\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# 2. Combine the best hyperparameters (found in grid search)\n",
        "final_best_params = {**fixed_params, **best_config}\n",
        "\n",
        "# Use mean best epoch from CV + 20% buffer (more data â†’ slightly more epochs)\n",
        "final_epochs = int(best_epochs * 1)\n",
        "print(f\"\\nðŸ“Š CV found best performance at epoch {best_epochs:.1f} (average)\")\n",
        "print(f\"   Final training will use {final_epochs} epochs (20% buffer for full dataset)\")\n",
        "\n",
        "final_best_params['epochs'] = final_epochs\n",
        "\n",
        "\n",
        "print(\"Training final model with best configuration:\")\n",
        "for k, v in final_best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# 3. Merge features and labels\n",
        "train_merged = X_train_full.merge(y_full, on=\"sample_index\")\n",
        "\n",
        "# 4. Encode labels numerically BEFORE building sequences\n",
        "label_mapping = {\"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2}\n",
        "train_merged[\"label\"] = train_merged[\"label\"].map(label_mapping)\n",
        "\n",
        "\n",
        "\n",
        "# 5. Normalise feature values\n",
        "\n",
        "scale_columns = [col for col in train_merged.columns if col.startswith(\"joint_\")]\n",
        "# calculate the minimum and maximum values from the training data only\n",
        "mins = X_train[scale_columns].min()\n",
        "maxs = X_train[scale_columns].max()\n",
        "\n",
        "# apply normalisation to the specified columns in all datasets (training and validation)\n",
        "for column in scale_columns:\n",
        "\n",
        "    # normalise the training set\n",
        "    train_merged[column] = (train_merged[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "# 6. Build full sequences\n",
        "X_train_seq, y_train_seq = build_sequences(train_merged, train_merged[[\"sample_index\", \"label\"]], window=final_best_params[\"window\"], stride=final_best_params[\"stride\"])\n",
        "\n",
        "labels_np = y_train_seq  # numpy array of shape (N_train_windows,)\n",
        "\n",
        "# Count windows per class\n",
        "class_sample_counts = np.bincount(labels_np, minlength=3)\n",
        "# e.g. array([N_no, N_low, N_high])\n",
        "\n",
        "# Inverse frequency weights: rarer class â‡’ larger weight\n",
        "class_weights_for_sampling = 1.0 / (class_sample_counts + 1e-8)\n",
        "\n",
        "# Per-sample weights: pick the weight of its class\n",
        "sample_weights = class_weights_for_sampling[labels_np]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "# 7. DataLoader\n",
        "train_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\n",
        "train_loader = make_loader(train_ds, batch_size=final_best_params[\"batch_size\"], sampler=sampler, shuffle=True, drop_last=False)\n",
        "\n",
        "# 8. Initialize model with tuned hyperparameters\n",
        "model = RecurrentClassifier(\n",
        "    input_size=X_train_seq.shape[2],\n",
        "    hidden_size=final_best_params[\"hidden_size\"],\n",
        "    num_layers=final_best_params[\"hidden_layers\"],\n",
        "    num_classes=len(label_mapping),\n",
        "    rec_dropout_rate = RECURRENT_DROPOUT,\n",
        "    dropout_rate=final_best_params[\"dropout_rate\"],\n",
        "    bidirectional=final_best_params[\"bidirectional\"],\n",
        "    rnn_type=final_best_params[\"rnn_type\"],\n",
        "    cnn_channels=final_best_params[\"cnn_channels\"],                  # enables CNN1D preprocessing\n",
        "    cnn_kernel_size=final_best_params[\"cnn_kernel_size\"],                # usually 3, 5, or 7 works well\n",
        "    cnn_dropout=final_best_params[\"cnn_dropout\"],                  # (optional; default uses RNN dropout)\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=final_best_params[\"learning_rate\"], weight_decay=final_best_params['l2_lambda'])\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "# 9. Train model on the entire dataset\n",
        "model, history = fit(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=None,\n",
        "    epochs=final_epochs,\n",
        "    train_criterion=train_criterion,\n",
        "    val_criterion=None,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    patience=final_best_params[\"patience\"],\n",
        "    verbose=True,\n",
        "    evaluation_metric=\"val_f1\",  # ignored since no validation\n",
        "    mode=\"max\",\n",
        "    restore_best_weights=False,\n",
        "    experiment_name=\"final_full_train\"\n",
        ")\n",
        "\n",
        "# 10. Prepare test set for inference\n",
        "X_test = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\n",
        "DF_test, _ = preprocess_joints(X_test.copy())\n",
        "X_test, _ = dataset_conversion_type_embed_ready(DF_test)\n",
        "\n",
        "for column in scale_columns:\n",
        "    # normalise the test set\n",
        "    X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "# Build windowed sequences (this creates N_windows, not N_sequences)\n",
        "X_test_seq, _ = build_sequences(X_test, None, window=final_best_params[\"window\"], stride=final_best_params[\"stride\"])\n",
        "\n",
        "# 11. Save predictions and configuration\n",
        "OUT_DIR = \"results_best_model3\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "hyperparams = final_best_params.copy()\n",
        "hyperparams.update({\n",
        "    \"best_cv_f1\": best_score\n",
        "})\n",
        "\n",
        "# MODIFIED: Pass window_size and stride for aggregation\n",
        "submission = save_experiment_output(\n",
        "    model_name=final_best_params[\"rnn_type\"].lower(),\n",
        "    hyperparams=hyperparams,\n",
        "    X_test_seq=X_test_seq,  # Windowed data (N_windows, W, F)\n",
        "    label_mapping=label_mapping,\n",
        "    output_dir=OUT_DIR,\n",
        "    sample_indices=X_test[\"sample_index\"].unique(),\n",
        "    model=model,\n",
        "    window_size=final_best_params[\"window\"],  # NEW: pass for aggregation\n",
        "    stride=final_best_params[\"stride\"]  # NEW: pass for aggregation\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Final model trained and submission saved!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T14:03:07.704465Z",
          "iopub.status.idle": "2025-11-14T14:03:07.704808Z",
          "shell.execute_reply.started": "2025-11-14T14:03:07.704642Z",
          "shell.execute_reply": "2025-11-14T14:03:07.704657Z"
        },
        "id": "JqPiorRLvylk"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}