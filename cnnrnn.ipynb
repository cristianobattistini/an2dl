{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13644638,"sourceType":"datasetVersion","datasetId":8673684}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Artificial Neural Networks and Deep Learning**\n\n---","metadata":{}},{"cell_type":"markdown","source":"## âš™ï¸ **Libraries Import**","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:20.002810Z","iopub.execute_input":"2025-11-13T22:11:20.003078Z","iopub.status.idle":"2025-11-13T22:11:20.006860Z","shell.execute_reply.started":"2025-11-13T22:11:20.003053Z","shell.execute_reply":"2025-11-13T22:11:20.006210Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Set seed for reproducibility\nSEED = 42\n\n# Import necessary libraries\nimport os\n\n# Set environment variables before importing modules\nos.environ['PYTHONHASHSEED'] = str(SEED)\nos.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n\n# Suppress warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# Import necessary modules\nimport logging\nimport random\nimport numpy as np\n\n# Set seeds for random number generators in NumPy and Python\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\n# Import PyTorch\nimport torch\ntorch.manual_seed(SEED)\nfrom torch import nn\nfrom torchsummary import summary\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\nimport torch.nn.functional as F\n\nlogs_dir = \"tensorboard\"\n!pkill -f tensorboard\n%load_ext tensorboard\n!mkdir -p models\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.benchmark = True\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Device: {device}\")\n\n# Import other libraries\nimport copy\nimport shutil\nfrom datetime import datetime\nfrom itertools import product\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nfrom pathlib import Path\nimport pickle\n\n# Configure plot display settings\nsns.set(font_scale=1.4)\nsns.set_style('white')\nplt.rc('font', size=14)\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:20.022551Z","iopub.execute_input":"2025-11-13T22:11:20.022803Z","iopub.status.idle":"2025-11-13T22:11:52.303616Z","shell.execute_reply.started":"2025-11-13T22:11:20.022783Z","shell.execute_reply":"2025-11-13T22:11:52.302930Z"}},"outputs":[{"name":"stderr","text":"2025-11-13 22:11:30.355253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763071890.787355      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763071890.892546      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nDevice: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## â³ **Data Loading**","metadata":{}},{"cell_type":"code","source":"DATASET_ROOT = Path(\"./dataset\")\n\n# --- 2ï¸âƒ£ Kaggle ---\nDATASET_ROOT = Path(\"/kaggle/input/pirate-pain\")\n\n# --- 3ï¸âƒ£ Server o cluster privato (es. Westworld/Elysium) ---\n# DATASET_ROOT = Path(\"/multiverse/datasets/private_dataset/pirate_pain\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:52.305148Z","iopub.execute_input":"2025-11-13T22:11:52.305597Z","iopub.status.idle":"2025-11-13T22:11:52.309434Z","shell.execute_reply.started":"2025-11-13T22:11:52.305570Z","shell.execute_reply":"2025-11-13T22:11:52.308825Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\n\n# Caricamento dati\nX_train = pd.read_csv(DATASET_ROOT / \"pirate_pain_train.csv\")\nX_TRAIN = pd.read_csv(DATASET_ROOT / \"pirate_pain_train.csv\")\n\ny_train = pd.read_csv(DATASET_ROOT / \"pirate_pain_train_labels.csv\")\nY_TRAIN = pd.read_csv(DATASET_ROOT / \"pirate_pain_train_labels.csv\")\n\nX_test  = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\n\nprint(f\"  X_train: {X_train.shape}\")\nprint(f\"  y_train: {y_train.shape}\")\nprint(f\"  X_test:  {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:52.310159Z","iopub.execute_input":"2025-11-13T22:11:52.310419Z","iopub.status.idle":"2025-11-13T22:11:57.172830Z","shell.execute_reply.started":"2025-11-13T22:11:52.310400Z","shell.execute_reply":"2025-11-13T22:11:57.172091Z"}},"outputs":[{"name":"stdout","text":"  X_train: (105760, 40)\n  y_train: (661, 2)\n  X_test:  (211840, 40)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## ðŸ”Ž **Exploration and Data Analysis**","metadata":{}},{"cell_type":"code","source":"# MERGE BETWEEN TRAIN DATA AND LABELS\n# the labels are in a separated file linked through 'sample_index\n# here we merge X_train and y_train in a unique Dataframe to explore\n\ntrain_merge = X_train.merge(y_train, on=\"sample_index\", how=\"left\")\n\n# check whether all the labels have been associated or not\nmissing_labels = train_merge[\"label\"].isna().sum()\nif missing_labels > 0:\n    print(f\"{missing_labels} rows without a label\")\n\n# check\nprint(train_merge[[\"sample_index\",\"time\",\"label\"]].head())\nprint(\"Class Distribution\")\nprint(train_merge[\"label\"].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.173726Z","iopub.execute_input":"2025-11-13T22:11:57.174014Z","iopub.status.idle":"2025-11-13T22:11:57.245456Z","shell.execute_reply.started":"2025-11-13T22:11:57.173986Z","shell.execute_reply":"2025-11-13T22:11:57.244587Z"}},"outputs":[{"name":"stdout","text":"   sample_index  time    label\n0             0     0  no_pain\n1             0     1  no_pain\n2             0     2  no_pain\n3             0     3  no_pain\n4             0     4  no_pain\nClass Distribution\nlabel\nno_pain      81760\nlow_pain     15040\nhigh_pain     8960\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(y_train[\"label\"].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.247353Z","iopub.execute_input":"2025-11-13T22:11:57.247536Z","iopub.status.idle":"2025-11-13T22:11:57.252578Z","shell.execute_reply.started":"2025-11-13T22:11:57.247521Z","shell.execute_reply":"2025-11-13T22:11:57.251927Z"}},"outputs":[{"name":"stdout","text":"label\nno_pain      511\nlow_pain      94\nhigh_pain     56\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## ðŸ”„ **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"def dataset_conversion_type_embed_ready(df):\n    \"\"\"\n    Minimal, embedding-friendly preprocessing:\n    - joints: float32 (continuous features)\n    - pain_survey_*: int64 indices (0..2) for embeddings\n    - n_legs/hands/eyes: mapped to {0,1} as int64 for embeddings\n    Returns: df, meta\n    \"\"\"\n    df = df.copy()\n\n    # 1) continuous features\n    joint_cols = [c for c in df.columns if c.startswith(\"joint_\")]\n    df[joint_cols] = df[joint_cols].astype(\"float32\")\n\n    # 2) surveys as categorical indices (already 0/1/2)\n    pain_survey_cols = [c for c in df.columns if c.startswith(\"pain_survey_\")]\n    df[pain_survey_cols] = df[pain_survey_cols].astype(\"int64\")\n\n    # 3) 2-way categoricals â†’ indices\n    legs_map  = {\"two\": 0, \"one+peg_leg\": 1}\n    hands_map = {\"two\": 0, \"one+hook_hand\": 1}\n    eyes_map  = {\"two\": 0, \"one+eye_patch\": 1}\n\n    if \"n_legs\" in df.columns:\n        df[\"n_legs\"]  = df[\"n_legs\"].map(legs_map).astype(\"int64\")\n    if \"n_hands\" in df.columns:\n        df[\"n_hands\"] = df[\"n_hands\"].map(hands_map).astype(\"int64\")\n    if \"n_eyes\" in df.columns:\n        df[\"n_eyes\"]  = df[\"n_eyes\"].map(eyes_map).astype(\"int64\")\n\n    # 4) define columns\n    cat_two_cols = [c for c in [\"n_legs\",\"n_hands\",\"n_eyes\"] if c in df.columns]\n    cat_cols = pain_survey_cols + cat_two_cols\n    cont_cols = joint_cols  # keep only joints as continuous\n\n    # 5) cardinals for embeddings (compute on TRAIN ONLY in CV, reuse for VAL/TEST)\n    cardinals = {c: int(df[c].nunique()) for c in cat_cols}\n    # suggested tiny dims: 1 for binaries, 2 for 3-class surveys\n    emb_dims = {c: (1 if cardinals[c] == 2 else 2) for c in cat_cols}\n\n    meta = {\n        \"cont_cols\": cont_cols,\n        \"cat_cols\":  cat_cols,\n        \"cardinals\": cardinals,\n        \"emb_dims\":  emb_dims,\n        \"maps\": {\"n_legs\": legs_map, \"n_hands\": hands_map, \"n_eyes\": eyes_map},\n    }\n    return df, meta\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.253146Z","iopub.execute_input":"2025-11-13T22:11:57.253405Z","iopub.status.idle":"2025-11-13T22:11:57.272807Z","shell.execute_reply.started":"2025-11-13T22:11:57.253386Z","shell.execute_reply":"2025-11-13T22:11:57.272206Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def preprocess_joints(df, \n                      drop_redundant=False, \n                      drop_near_zero=False, \n                      drop_low_var=False,\n                      verbose=True):\n    \"\"\"\n    Simplify joint_* preprocessing based on EDA results.\n    Removes constant, redundant, or near-zero-variance joints.\n\n    Returns a (df_out, feature_cols) tuple.\n    \"\"\"\n    joint_cols = sorted([c for c in df.columns if c.startswith(\"joint_\")],\n                        key=lambda x: int(x.split(\"_\")[1]))\n    drop = set()\n\n    # 1 Drop constant joint_30\n    if \"joint_30\" in joint_cols:\n        drop.add(\"joint_30\")\n\n    #  Drop redundant joints (from correlation heatmap)\n    if drop_redundant:\n        for c in [\"joint_01\", \"joint_02\", \"joint_05\"]:\n            if c in joint_cols:\n                drop.add(c)\n\n    # Drop near-zero variance joints (joint_13â€“25)\n    if drop_near_zero:\n        for i in range(13, 26):\n            c = f\"joint_{i:02d}\"\n            if c in joint_cols:\n                drop.add(c)\n\n    # (Optional) Drop low-variance but not-zero joints (joint_26â€“29)\n    if drop_low_var:\n        for i in range(26, 30):\n            c = f\"joint_{i:02d}\"\n            if c in joint_cols:\n                drop.add(c)\n\n    # apply\n    kept = [c for c in joint_cols if c not in drop]\n    df_out = df.drop(columns=list(drop), errors=\"ignore\")\n\n    if verbose:\n        print(f\"[preprocess_joints] start={len(joint_cols)} | kept={len(kept)} | dropped={len(drop)}\")\n        if drop:\n            print(\"  â€¢ dropped:\", sorted(list(drop)))\n\n    return df_out, kept\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.273462Z","iopub.execute_input":"2025-11-13T22:11:57.273670Z","iopub.status.idle":"2025-11-13T22:11:57.300898Z","shell.execute_reply.started":"2025-11-13T22:11:57.273655Z","shell.execute_reply":"2025-11-13T22:11:57.300153Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"X_train, _ = preprocess_joints(X_train)\nX_test, _ = preprocess_joints(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.301738Z","iopub.execute_input":"2025-11-13T22:11:57.302014Z","iopub.status.idle":"2025-11-13T22:11:57.373397Z","shell.execute_reply.started":"2025-11-13T22:11:57.301987Z","shell.execute_reply":"2025-11-13T22:11:57.372575Z"}},"outputs":[{"name":"stdout","text":"[preprocess_joints] start=31 | kept=30 | dropped=1\n  â€¢ dropped: ['joint_30']\n[preprocess_joints] start=31 | kept=30 | dropped=1\n  â€¢ dropped: ['joint_30']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## ðŸ”„ **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"# 1) Fit preprocessing on TRAIN ONLY\nX_train, meta = dataset_conversion_type_embed_ready(X_train)\n# BEFORE calling build_sequences, check your DataFrame\nprint(\"\\n=== PREPROCESSING CHECK ===\")\npain_cols = [c for c in X_train.columns if c.startswith('pain_survey_')]\nprint(f\"Pain columns in X_train: {pain_cols}\")\nprint(f\"Pain column dtypes: {X_train[pain_cols].dtypes}\")\nprint(f\"Pain unique values:\")\nfor col in pain_cols:\n    print(f\"  {col}: {X_train[col].unique()}\")\n\nstatic_cols = ['n_legs', 'n_hands', 'n_eyes']\nprint(f\"\\nStatic columns in X_train: {[c for c in static_cols if c in X_train.columns]}\")\nfor col in static_cols:\n    if col in X_train.columns:\n        print(f\"  {col} dtype: {X_train[col].dtype}\")\n        print(f\"  {col} unique: {X_train[col].unique()}\")\nprint(\"===========================\\n\")\n\n# 2) Apply the SAME mappings/cardinals to TEST\n#    (pain_survey_* are already 0/1/2; for n_legs/hands/eyes we reuse meta[\"maps\"])\nX_test = X_test.copy()\nfor c, m in meta[\"maps\"].items():\n    if c in X_test.columns:\n        X_test[c] = X_test[c].map(m).astype(\"int64\")\n\n# Cast types consistently with train\nX_test[meta[\"cont_cols\"]] = X_test[meta[\"cont_cols\"]].astype(\"float32\")\nfor c in meta[\"cat_cols\"]:\n    X_test[c] = X_test[c].astype(\"int64\")\n\n# 3) Sanity checks\nprint(\"Train cont/cat:\", len(meta[\"cont_cols\"]), len(meta[\"cat_cols\"]))\nprint(\"Train cont cols:\", meta[\"cont_cols\"][:5], \"â€¦\")\nprint(\"Train cat  cols:\", meta[\"cat_cols\"])\nprint(\"Test has all cont cols?\", set(meta[\"cont_cols\"]).issubset(X_test.columns))\nprint(\"Test has all cat  cols?\", set(meta[\"cat_cols\"]).issubset(X_test.columns))\n\n# Optional: verify cardinalities didnâ€™t explode on test (should be â‰¤ train)\nfor c in meta[\"cat_cols\"]:\n    tr_card = meta[\"cardinals\"][c]\n    te_card = int(X_test[c].nunique())\n    if te_card > tr_card:\n        print(f\"WARNING: column {c} has unseen categories in TEST (train={tr_card}, test={te_card})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.374229Z","iopub.execute_input":"2025-11-13T22:11:57.374484Z","iopub.status.idle":"2025-11-13T22:11:57.603214Z","shell.execute_reply.started":"2025-11-13T22:11:57.374452Z","shell.execute_reply":"2025-11-13T22:11:57.602450Z"}},"outputs":[{"name":"stdout","text":"\n=== PREPROCESSING CHECK ===\nPain columns in X_train: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\nPain column dtypes: pain_survey_1    int64\npain_survey_2    int64\npain_survey_3    int64\npain_survey_4    int64\ndtype: object\nPain unique values:\n  pain_survey_1: [2 0 1]\n  pain_survey_2: [0 2 1]\n  pain_survey_3: [2 0 1]\n  pain_survey_4: [1 2 0]\n\nStatic columns in X_train: ['n_legs', 'n_hands', 'n_eyes']\n  n_legs dtype: int64\n  n_legs unique: [0 1]\n  n_hands dtype: int64\n  n_hands unique: [0 1]\n  n_eyes dtype: int64\n  n_eyes unique: [0 1]\n===========================\n\nTrain cont/cat: 30 7\nTrain cont cols: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04'] â€¦\nTrain cat  cols: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes']\nTest has all cont cols? True\nTest has all cat  cols? True\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Step 1 â€” Copy merged train and raw test\ntrain_dataset = X_train.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.604088Z","iopub.execute_input":"2025-11-13T22:11:57.604446Z","iopub.status.idle":"2025-11-13T22:11:57.629622Z","shell.execute_reply.started":"2025-11-13T22:11:57.604417Z","shell.execute_reply":"2025-11-13T22:11:57.628443Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# to reload quickly\nX_train = train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.630888Z","iopub.execute_input":"2025-11-13T22:11:57.631222Z","iopub.status.idle":"2025-11-13T22:11:57.636542Z","shell.execute_reply.started":"2025-11-13T22:11:57.631192Z","shell.execute_reply":"2025-11-13T22:11:57.635563Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"print(\"Train columns:\", X_train.columns.tolist())\nprint(\"Test columns:\", X_test.columns.tolist())\n\ntrain_only = [c for c in X_train.columns if c not in X_test.columns]\ntest_only  = [c for c in X_test.columns if c not in X_train.columns]\n\nif train_only or test_only:\n    print(\"Column mismatch detected!\")\n    if train_only:\n        print(\"  Present only in TRAIN:\", train_only)\n    if test_only:\n        print(\"  Present only in TEST:\", test_only)\nelse:\n    print(\"âœ… Train and Test have identical columns.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.637576Z","iopub.execute_input":"2025-11-13T22:11:57.637972Z","iopub.status.idle":"2025-11-13T22:11:57.664437Z","shell.execute_reply.started":"2025-11-13T22:11:57.637941Z","shell.execute_reply":"2025-11-13T22:11:57.663348Z"}},"outputs":[{"name":"stdout","text":"Train columns: ['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\nTest columns: ['sample_index', 'time', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'n_legs', 'n_hands', 'n_eyes', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04', 'joint_05', 'joint_06', 'joint_07', 'joint_08', 'joint_09', 'joint_10', 'joint_11', 'joint_12', 'joint_13', 'joint_14', 'joint_15', 'joint_16', 'joint_17', 'joint_18', 'joint_19', 'joint_20', 'joint_21', 'joint_22', 'joint_23', 'joint_24', 'joint_25', 'joint_26', 'joint_27', 'joint_28', 'joint_29']\nâœ… Train and Test have identical columns.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Step 1. temporary merge X_train + y_train to create splits ---\ntrain_merged = X_train.merge(y_train, on=\"sample_index\")\nprint(train_merged.shape)\n\n# Step 2. retrieve unique indexes and labels to stratify ---\nunique_samples = train_merged['sample_index'].unique()\ny_seq = train_merged.groupby('sample_index')['label'].first().reindex(unique_samples).values\n\n# Step 3. Divide in train e val (stratified) ---\n\ntrain_idxs, val_idxs = train_test_split(unique_samples, test_size=0.20, random_state=SEED, stratify=y_seq)\nprint(f\"Train Size: {len(train_idxs)}, Val Size: {len(val_idxs)}, total: {len(train_idxs)+len(val_idxs)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.665509Z","iopub.execute_input":"2025-11-13T22:11:57.665793Z","iopub.status.idle":"2025-11-13T22:11:57.719356Z","shell.execute_reply.started":"2025-11-13T22:11:57.665768Z","shell.execute_reply":"2025-11-13T22:11:57.718583Z"}},"outputs":[{"name":"stdout","text":"(105760, 40)\nTrain Size: 528, Val Size: 133, total: 661\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Step 4. Apply split on X e y (separately) ---\ndf_train = train_merged[train_merged['sample_index'].isin(train_idxs)]\ndf_val   = train_merged[train_merged['sample_index'].isin(val_idxs)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.722596Z","iopub.execute_input":"2025-11-13T22:11:57.723328Z","iopub.status.idle":"2025-11-13T22:11:57.739400Z","shell.execute_reply.started":"2025-11-13T22:11:57.723300Z","shell.execute_reply":"2025-11-13T22:11:57.738644Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# X: only features\nX_train = df_train.drop(columns=['label'])\nX_val   = df_val.drop(columns=['label'])\n\n# y: one label for each sequence\ny_train = df_train.groupby(\"sample_index\")[\"label\"].first().values\ny_val   = df_val.groupby(\"sample_index\")[\"label\"].first().values\n\nprint(f\"X_train shape: {X_train.shape}, X_val shape: {X_val.shape}\")\nprint(f\"y_train: {len(y_train)}, y_val: {len(y_val)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.740260Z","iopub.execute_input":"2025-11-13T22:11:57.740477Z","iopub.status.idle":"2025-11-13T22:11:57.761776Z","shell.execute_reply.started":"2025-11-13T22:11:57.740460Z","shell.execute_reply":"2025-11-13T22:11:57.761185Z"}},"outputs":[{"name":"stdout","text":"X_train shape: (84480, 39), X_val shape: (21280, 39)\ny_train: 528, y_val: 133\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Define mapping once\nlabel_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\ninv_label_mapping = {v: k for k, v in label_mapping.items()}\n\n# Convert y_train/y_val from string â†’ int\ny_train = np.array([label_mapping[l] for l in y_train])\ny_val   = np.array([label_mapping[l] for l in y_val])\n\n# Compute label distributions\ntrain_counts = {inv_label_mapping[k]: np.sum(y_train == k) for k in np.unique(y_train)}\nval_counts   = {inv_label_mapping[k]: np.sum(y_val == k) for k in np.unique(y_val)}\n\nprint(\"Training labels:\", train_counts)\nprint(\"Validation labels:\", val_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.762526Z","iopub.execute_input":"2025-11-13T22:11:57.762811Z","iopub.status.idle":"2025-11-13T22:11:57.769588Z","shell.execute_reply.started":"2025-11-13T22:11:57.762791Z","shell.execute_reply":"2025-11-13T22:11:57.768923Z"}},"outputs":[{"name":"stdout","text":"Training labels: {'no_pain': 408, 'low_pain': 75, 'high_pain': 45}\nValidation labels: {'no_pain': 103, 'low_pain': 19, 'high_pain': 11}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Normalisation","metadata":{}},{"cell_type":"code","source":"scale_columns = [col for col in X_train.columns if col.startswith(\"joint_\")]\n\n# calculate the minimum and maximum values from the training data only\nmins = X_train[scale_columns].min()\nmaxs = X_train[scale_columns].max()\n\n# apply normalisation to the specified columns in all datasets (training and validation)\nfor column in scale_columns:\n\n    # normalise the training set\n    X_train[column] = (X_train[column] - mins[column]) / (maxs[column] - mins[column])\n\n    # normalise the validation set\n    X_val[column] = (X_val[column] - mins[column]) / (maxs[column] - mins[column])\n\n    # normalise the test set\n    X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.770249Z","iopub.execute_input":"2025-11-13T22:11:57.770443Z","iopub.status.idle":"2025-11-13T22:11:57.861472Z","shell.execute_reply.started":"2025-11-13T22:11:57.770428Z","shell.execute_reply":"2025-11-13T22:11:57.860618Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print(\"Train:\", X_train[scale_columns].min().min(), \"â†’\", X_train[scale_columns].max().max())\nprint(\"Val:  \", X_val[scale_columns].min().min(),   \"â†’\", X_val[scale_columns].max().max())\nprint(\"Test: \", X_test[scale_columns].min().min(),   \"â†’\", X_test[scale_columns].max().max())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.862337Z","iopub.execute_input":"2025-11-13T22:11:57.862608Z","iopub.status.idle":"2025-11-13T22:11:57.965089Z","shell.execute_reply.started":"2025-11-13T22:11:57.862589Z","shell.execute_reply":"2025-11-13T22:11:57.964386Z"}},"outputs":[{"name":"stdout","text":"Train: 0.0 â†’ 1.0\nVal:   -0.007747788447886705 â†’ 7.217308044433594\nTest:  -0.07219822704792023 â†’ 4.829583644866943\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Display the first five rows of the training DataFrame\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)\n\nX_train, y_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.965761Z","iopub.execute_input":"2025-11-13T22:11:57.965982Z","iopub.status.idle":"2025-11-13T22:11:57.991375Z","shell.execute_reply.started":"2025-11-13T22:11:57.965965Z","shell.execute_reply":"2025-11-13T22:11:57.990512Z"}},"outputs":[{"name":"stdout","text":"(84480, 39) (528,)\n(21280, 39) (133,)\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(        sample_index  time  pain_survey_1  pain_survey_2  pain_survey_3  \\\n 0                  0     0              2              0              2   \n 1                  0     1              2              2              2   \n 2                  0     2              2              0              2   \n 3                  0     3              2              2              2   \n 4                  0     4              2              2              2   \n ...              ...   ...            ...            ...            ...   \n 105755           660   155              2              2              0   \n 105756           660   156              2              2              0   \n 105757           660   157              0              2              2   \n 105758           660   158              2              2              2   \n 105759           660   159              2              2              2   \n \n         pain_survey_4  n_legs  n_hands  n_eyes  joint_00  ...  joint_20  \\\n 0                   1       0        0       0  0.777046  ...  0.000001   \n 1                   2       0        0       0  0.805855  ...  0.000004   \n 2                   2       0        0       0  0.767110  ...  0.000001   \n 3                   2       0        0       0  0.665528  ...  0.000071   \n 4                   2       0        0       0  0.773829  ...  0.000039   \n ...               ...     ...      ...     ...       ...  ...       ...   \n 105755              0       0        0       0  0.746465  ...  0.000000   \n 105756              2       0        0       0  0.729322  ...  0.000000   \n 105757              2       0        0       0  0.790338  ...  0.000000   \n 105758              2       0        0       0  0.750993  ...  0.000000   \n 105759              0       0        0       0  0.715698  ...  0.000007   \n \n             joint_21      joint_22      joint_23  joint_24  joint_25  \\\n 0       2.426544e-06  1.503263e-06  1.052579e-04  0.000405  0.000004   \n 1       2.757563e-07  4.403064e-07  1.584209e-04  0.000001  0.000000   \n 2       1.063529e-07  1.575589e-08  3.805627e-05  0.000085  0.000003   \n 3       6.981461e-06  3.352260e-07  4.862392e-05  0.000002  0.000000   \n 4       3.076737e-06  1.885071e-08  4.086358e-05  0.000002  0.000007   \n ...              ...           ...           ...       ...       ...   \n 105755  0.000000e+00  1.739834e-07  7.708907e-08  0.000013  0.000001   \n 105756  1.010686e-06  1.741445e-07  0.000000e+00  0.000007  0.000004   \n 105757  0.000000e+00  1.742954e-07  6.375097e-05  0.000007  0.000007   \n 105758  1.210643e-07  1.744361e-07  1.594986e-05  0.000007  0.000001   \n 105759  4.086062e-07  1.745665e-07  0.000000e+00  0.000085  0.000001   \n \n         joint_26  joint_27  joint_28  joint_29  \n 0       0.014214  0.011376  0.018978  0.020291  \n 1       0.010748  0.000000  0.009473  0.010006  \n 2       0.013097  0.006830  0.017065  0.016856  \n 3       0.009505  0.006274  0.020264  0.017981  \n 4       0.004216  0.002132  0.023389  0.018477  \n ...          ...       ...       ...       ...  \n 105755  0.006255  0.022634  0.122919  0.161896  \n 105756  0.021736  0.010761  0.053784  0.085181  \n 105757  0.030063  0.023592  0.053808  0.057150  \n 105758  0.037765  0.015093  0.068772  0.077918  \n 105759  0.027207  0.035294  0.060020  0.119300  \n \n [84480 rows x 39 columns],\n array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0,\n        0, 2, 0, 0, 1, 1, 2, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2,\n        0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0,\n        1, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n        0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0,\n        0, 0, 0, 0, 0, 1, 2, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n        0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n        0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 1,\n        0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0,\n        0, 2, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n        2, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n        2, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 0, 0, 0, 2,\n        1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n        0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"def build_sequences(\n    df: pd.DataFrame,\n    y: pd.DataFrame | np.ndarray | None = None,\n    window: int | None = None,\n    stride: int | None = None,\n    pad: bool = False,\n    add_time_features: bool = True\n):\n    \"\"\" \n    Build sequences from the dataset, either:\n      - full-length per sample_index (when window/stride are None), or\n      - sliding windows with given window and stride.\n\n    Data assumptions for THIS notebook:\n      â€¢ df already normalized/mapped (categoricals numeric; e.g., n_legs/hands/eyes âˆˆ {0,1})\n      â€¢ df has columns: ['sample_index','time', joint_*, pain_survey_*, n_legs, n_hands, n_eyes]\n      â€¢ each sample_index has T=160 rows (fixed-length), but we still allow windowing/stride\n\n    Returns:\n        dataset: np.ndarray of shape (N,T,F) or (N,window,F)\n        labels:  np.ndarray of shape (N,) if y is provided, else None\n    \"\"\"\n    # ------------------------------------------------------------------\n    # Feature groups (already numeric at this stage)\n    joint_cols  = [c for c in df.columns if c.startswith('joint_')]\n    pain_cols   = [c for c in df.columns if c.startswith('pain_survey_')]\n    static_cols = [c for c in ['n_legs', 'n_hands', 'n_eyes'] if c in df.columns]\n\n    # Keep only the necessary columns in a copy; preserve order\n    cols_needed = ['sample_index', 'time'] + joint_cols + pain_cols + static_cols\n    df = df[cols_needed].copy()\n\n    # Sort to preserve chronological order within each sequence\n    df = df.sort_values([\"sample_index\", \"time\"])\n\n    # If labels are provided, build a lookup dictionary: sample_index â†’ label\n    label_dict = None\n    if y is not None:\n        if isinstance(y, np.ndarray):\n            # Build mapping using the unique order of sample_index in df\n            unique_ids = df[\"sample_index\"].unique()\n            label_dict = {sid: int(lbl) for sid, lbl in zip(unique_ids, y)}\n        elif isinstance(y, pd.DataFrame):\n            # Expect columns ['sample_index','label'] with already-int-mapped labels\n            label_dict = dict(zip(y[\"sample_index\"], y[\"label\"]))\n\n    # Prepare outputs\n    dataset = []\n    labels  = []\n\n    # If no window/stride provided â†’ fall back to full-length per sequence\n    full_length_mode = (window is None or stride is None)\n\n    # Iterate over each sequence\n    for sid, group in df.groupby(\"sample_index\", sort=False):\n        # --- Extract groups (preserve types for embeddings) ---\n        X_joints = group[joint_cols].to_numpy(dtype=np.float32)        # (T, J) - continuous features\n\n        # IMPORTANT: Pain survey features are categorical indices {0,1,2}\n        # Keep as int64 first, then convert to float32 to preserve exact integer values\n        X_pain = group[pain_cols].to_numpy(dtype=np.int64)             # (T, 4) - categorical indices\n        X_pain = X_pain.astype(np.float32)                              # Convert to float32 but keep 0.0, 1.0, 2.0\n\n        # IMPORTANT: Static features are categorical indices {0,1}\n        # Keep as int64 first, then convert to float32 to preserve exact integer values\n        if static_cols:\n            X_static = group[static_cols].to_numpy(dtype=np.int64)     # (T, 3) - categorical indices\n            X_static = X_static.astype(np.float32)                      # Convert to float32 but keep 0.0, 1.0\n        else:\n            X_static = None\n\n        \n        # Time features: extract normalized time + sinusoidal encoding\n        if add_time_features:\n            time_values = group['time'].to_numpy(dtype=np.float32)\n            max_time = time_values.max()\n            normalized_time = time_values / max_time if max_time > 0 else time_values\n            time_sin = np.sin(2 * np.pi * normalized_time)\n            time_cos = np.cos(2 * np.pi * normalized_time)\n            X_time = np.stack([normalized_time, time_sin, time_cos], axis=1)  # (T, 3)\n        else:\n            X_time = None\n\n        # Concatenate all feature groups along last dimension\n        if X_static is not None:\n            X_full = np.concatenate([X_joints, X_pain, X_static], axis=1)  # (T, F_total)\n        else:\n            X_full = np.concatenate([X_joints, X_pain], axis=1)            # (T, F_total)\n        \n        # Add time features if enabled\n        if X_time is not None:\n            X_full = np.concatenate([X_full, X_time], axis=1)              # (T, F_total + 3)\n\n        T = X_full.shape[0]\n\n        if full_length_mode:\n            # ----- FULL-LENGTH MODE -----\n            dataset.append(X_full)\n            if label_dict is not None and sid in label_dict:\n                labels.append(int(label_dict[sid]))\n        else:\n            # ----- WINDOWED MODE (window, stride) -----\n            W = int(window)\n            S = int(stride)\n            assert W > 0 and S > 0, \"window and stride must be positive integers\"\n\n            if pad and T % W != 0:\n                # pad at the end with zeros to allow the last partial window\n                pad_len = (W - (T % W)) % W\n                if pad_len > 0:\n                    X_pad = np.zeros((pad_len, X_full.shape[1]), dtype=np.float32)\n                    X_seq = np.concatenate([X_full, X_pad], axis=0)\n                else:\n                    X_seq = X_full\n                Tmax = X_seq.shape[0]\n                idx = 0\n                while idx + W <= Tmax:\n                    dataset.append(X_seq[idx:idx+W])\n                    if label_dict is not None and sid in label_dict:\n                        labels.append(int(label_dict[sid]))\n                    idx += S\n            else:\n                # no padding â†’ only windows fully inside the sequence\n                idx = 0\n                while idx + W <= T:\n                    dataset.append(X_full[idx:idx+W])\n                    if label_dict is not None and sid in label_dict:\n                        labels.append(int(label_dict[sid]))\n                    idx += S\n\n    # Convert to numpy arrays\n    dataset = np.asarray(dataset, dtype=np.float32) if len(dataset) > 0 else np.empty((0, 0, 0), dtype=np.float32)\n    labels  = np.asarray(labels,  dtype=np.int64)   if len(labels)  > 0 else None\n\n    if dataset.size > 0:\n        print(f\"Built {len(dataset)} sequence{'s' if len(dataset)!=1 else ''}; each shape = {dataset[0].shape}\")\n    else:\n        print(\"Built 0 sequences (check window/stride vs sequence length).\")\n\n    return dataset, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:57.992325Z","iopub.execute_input":"2025-11-13T22:11:57.992664Z","iopub.status.idle":"2025-11-13T22:11:58.010491Z","shell.execute_reply.started":"2025-11-13T22:11:57.992633Z","shell.execute_reply":"2025-11-13T22:11:58.009650Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# --- ACF-DRIVEN WINDOWING (drop-in) ---------------------------------\n# Goal:\n#   Use autocorrelation (ACF) to pick a WINDOW_SIZE (W) and STRIDE (S)\n#   that reflect how far the signal â€œremembersâ€ its past.\n#\n# Design choices:\n#   â€¢ We use ONLY joint_* signals (continuous), excluding pain_survey_* and\n#     static cats (n_legs/hands/eyes), because ACF is meaningful for\n#     continuous, time-varying signals. Surveys/cats would distort it.\n#   â€¢ We compute ACF per joint, per sequence, then average to get a robust\n#     \"mean ACF\" over the dataset (train only â†’ no leakage).\n#   â€¢ From the mean ACF curve we pick W via a simple heuristic:\n#       1) first local peak (a natural cycle length)\n#       2) else, first lag where ACF falls below a cutoff (memory fades)\n#     then clamp to a safe lower bound (min_window) and snap to multiple of 4.\n#\n# How to read the outputs:\n#   â€¢ W_SUGG = suggested window (steps of lookback the model should \"see\")\n#   â€¢ S_SUGG = suggested stride (step between window starts; default W//4)\n#   â€¢ n_windows = number of windows per sequence with length T=160\n#   â€¢ covered_steps = n_windows * W  (overlaps â†’ can exceed T)\n#\n# Keep in mind:\n#   â€¢ This is NOT a replacement for cross-validation. Use CV to compare\n#     W in a small neighborhood (e.g., W, WÂ±4 or WÂ±8) and keep what wins.\n# ---------------------------------------------------------------------\n\ndef _acf_1d(x: np.ndarray, max_lag: int) -> np.ndarray:\n    \"\"\"\n    Compute the autocorrelation function (ACF) for a 1D vector up to `max_lag`.\n\n    Steps:\n      1) Normalize x to zero-mean and unit-variance so the ACF is scale-free.\n      2) Use np.correlate in 'full' mode to get correlation at all lags.\n      3) Slice the non-negative lags [0..max_lag].\n      4) Normalize by acf[0] (the variance) so ACF[0] == 1 and 0<=ACF<=1 (approx).\n\n    Args:\n      x (np.ndarray): 1D time series (length T)\n      max_lag (int): maximum lag we want to evaluate (<= T-1)\n\n    Returns:\n      acf (np.ndarray): shape (max_lag+1,), acf[0]=1, acf[k]=similarity at lag k\n    \"\"\"\n    x = x.astype(np.float64)\n    x = (x - x.mean()) / (x.std() + 1e-8)      # protect from near-constant series\n    acf_full = np.correlate(x, x, mode='full') # length 2T-1\n    # Keep the right half: lags 0..max_lag (index starts at the center)\n    acf = acf_full[len(x)-1 : len(x)-1 + max_lag + 1]\n    # Normalize so that ACF[0] == 1 (divide by variance term)\n    return acf / (acf[0] + 1e-8)\n\ndef mean_acf_over_joints(df, max_lag=80, sample_cap=256):\n    \"\"\"\n    Compute the MEAN ACF across joints and (optionally) across a subset\n    of sequences for speed. This gives a single, smooth ACF curve.\n\n    Expected df columns:\n      â€¢ 'sample_index', 'time', and many 'joint_*' columns\n\n    Process:\n      For each sequence (sample_index):\n        - sort by time\n        - build a (T, J) matrix of joint features\n        - for each joint j, compute ACF_j[0..L] where L = min(max_lag, T-1)\n        - average ACF over joints â†’ one curve per sequence\n      Average all sequence curves â†’ mean ACF\n\n    Why average?\n      Reduces noise and avoids picking a window based on a single joint\n      or a single quirky sequence.\n\n    Args:\n      df (DataFrame): TRAIN subset only (to avoid leakage)\n      max_lag (int): maximum lag we consider (e.g., 80)\n      sample_cap (int): if many sequences, randomly subsample this many\n                        to keep runtime small\n\n    Returns:\n      lags (np.ndarray): [0, 1, ..., L]\n      mean_acf (np.ndarray): averaged ACF curve, length L+1\n    \"\"\"\n    joint_cols = [c for c in df.columns if c.startswith(\"joint_\")]\n    assert len(joint_cols) > 0, \"No joint_* columns found.\"\n\n    # Subsample sequences if needed (for speed on large datasets)\n    sids = df[\"sample_index\"].unique()\n    if len(sids) > sample_cap:\n        rng = np.random.default_rng(42)\n        sids = rng.choice(sids, size=sample_cap, replace=False)\n\n    acfs = []\n    for sid in sids:\n        # (T, J) matrix for this sequence\n        seq = (df.loc[df[\"sample_index\"] == sid]\n                 .sort_values(\"time\")[joint_cols].to_numpy(dtype=np.float32))\n        T, J = seq.shape\n        L = min(max_lag, T - 1)  # ACF defined up to T-1\n        # ACF for each joint, then average across joints â†’ one curve per sequence\n        per_joint = []\n        for j in range(J):\n            per_joint.append(_acf_1d(seq[:, j], L))  # shape (L+1,)\n        acfs.append(np.mean(np.stack(per_joint, axis=0), axis=0))  # (L+1,)\n\n    # Average across sequences â†’ single smooth ACF curve\n    mean_acf = np.mean(np.stack(acfs, axis=0), axis=0)  # (L+1,)\n    lags = np.arange(len(mean_acf))\n    return lags, mean_acf\n\ndef suggest_window_from_acf(mean_acf: np.ndarray, min_window=12, cutoff=0.10):\n    \"\"\"\n    Heuristic to convert the mean ACF curve into a WINDOW_SIZE (W).\n\n    Intuition:\n      â€¢ If there's a visible \"cycle\", the ACF will have a local peak at its period.\n        â†’ pick the first local maximum after lag=1 (we ignore lag=0..1).\n      â€¢ If no clear peak, use the lag where correlation \"dies out\" (drops < cutoff).\n        â†’ pick first lag where ACF < cutoff (e.g., 0.10).\n      â€¢ Clamp to a minimum window (min_window) so we don't get too tiny a W.\n      â€¢ Snap to a multiple of 4 to make stride W//4 an integer (nice, common choice).\n\n    Args:\n      mean_acf (np.ndarray): curve from mean_acf_over_joints\n      min_window (int): lower bound to keep W usable for training (default 12)\n      cutoff (float): ACF threshold for \"memory fades\" (default 0.10)\n\n    Returns:\n      W (int): suggested window size\n    \"\"\"\n    # 1) Look for the first local MAX after lag=1 (i.e., k >= 2)\n    peak_lag = None\n    for k in range(2, len(mean_acf) - 1):\n        # local max if it is >= next and > previous\n        if mean_acf[k] > mean_acf[k - 1] and mean_acf[k] >= mean_acf[k + 1]:\n            peak_lag = k\n            break\n\n    # 2) First lag where ACF falls below cutoff (correlation has faded)\n    below = np.where(mean_acf < cutoff)[0]\n    drop_lag = int(below[0]) if len(below) else None\n\n    # Combine signals (prefer a peak if present, else use drop)\n    if peak_lag is not None and drop_lag is not None:\n        # keep it within [min_window, drop_lag]\n        W = min(max(peak_lag, min_window), drop_lag)\n    elif peak_lag is not None:\n        W = max(peak_lag, min_window)\n    elif drop_lag is not None:\n        W = max(drop_lag, min_window)\n    else:\n        # No clear info â†’ conservative fallback\n        W = max(24, min_window)\n\n    # Safety clamp within [min_window, 160] (your T=160)\n    W = int(np.clip(W, min_window, 160))\n\n    # Snap to multiple of 4 so stride = W//4 is an integer\n    if W % 4 != 0:\n        W += (4 - (W % 4))\n    return W\n\ndef suggest_stride(window: int) -> int:\n    \"\"\"\n    Default stride choice: quarter window.\n    Why? Good balance between:\n      - enough overlap to preserve information\n      - not exploding the number of windows too much\n    \"\"\"\n    return max(1, window // 4)\n\n# ---- RUN IT ON YOUR TRAIN DF (after normalization/mapping) ----------\n# NOTE: Use df_train (TRAIN SPLIT ONLY) to avoid leaking validation info.\nlags, mean_acf = mean_acf_over_joints(df_train, max_lag=80, sample_cap=256)\n\n# Heuristic suggestions from ACF\nW_SUGG = suggest_window_from_acf(mean_acf, min_window=12, cutoff=0.10)\nS_SUGG = suggest_stride(W_SUGG)\nprint(f\"[ACF] Suggested WINDOW_SIZE={W_SUGG}, STRIDE={S_SUGG}\")\n\n# Optional: coverage diagnostic for your fixed T=160\nT = 160\nn_windows = (T - W_SUGG) // S_SUGG + 1 if T >= W_SUGG else 0\ncovered = n_windows * W_SUGG  # can exceed T because windows overlap\nprint(f\"[ACF] With T={T}: n_windows={n_windows}, covered_steps={covered}/{T}\")\n# --------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:58.011464Z","iopub.execute_input":"2025-11-13T22:11:58.011788Z","iopub.status.idle":"2025-11-13T22:11:58.779218Z","shell.execute_reply.started":"2025-11-13T22:11:58.011758Z","shell.execute_reply":"2025-11-13T22:11:58.778669Z"}},"outputs":[{"name":"stdout","text":"[ACF] Suggested WINDOW_SIZE=12, STRIDE=3\n[ACF] With T=160: n_windows=50, covered_steps=600/160\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# y_train_df must contain sample_index + label columns\ny_train_df = pd.DataFrame({\n    \"sample_index\": X_train[\"sample_index\"].unique(),\n    \"label\": y_train\n})\n\nX_train_seq_complete_window, y_train_seq_complete_window = build_sequences(X_train, y_train_df, window=160)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:58.780281Z","iopub.execute_input":"2025-11-13T22:11:58.780575Z","iopub.status.idle":"2025-11-13T22:11:59.429975Z","shell.execute_reply.started":"2025-11-13T22:11:58.780538Z","shell.execute_reply":"2025-11-13T22:11:59.429252Z"}},"outputs":[{"name":"stdout","text":"Built 528 sequences; each shape = (160, 40)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"y_val_df = pd.DataFrame({\n    \"sample_index\": X_val[\"sample_index\"].unique(),\n    \"label\": y_val\n})\n\nX_val_seq_complete_window, y_val_seq_complete_window = build_sequences(X_val, y_val_df, 160)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:59.431037Z","iopub.execute_input":"2025-11-13T22:11:59.431385Z","iopub.status.idle":"2025-11-13T22:11:59.596688Z","shell.execute_reply.started":"2025-11-13T22:11:59.431366Z","shell.execute_reply":"2025-11-13T22:11:59.596013Z"}},"outputs":[{"name":"stdout","text":"Built 133 sequences; each shape = (160, 40)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"X_test_seq_complete_window, _ = build_sequences(X_test, window=160)  # no labels â†’ returns (dataset, None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:11:59.597557Z","iopub.execute_input":"2025-11-13T22:11:59.597827Z","iopub.status.idle":"2025-11-13T22:12:01.244182Z","shell.execute_reply.started":"2025-11-13T22:11:59.597808Z","shell.execute_reply":"2025-11-13T22:12:01.243391Z"}},"outputs":[{"name":"stdout","text":"Built 1324 sequences; each shape = (160, 40)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"X_tr_win, y_tr_win = build_sequences(X_train, y_train, window=W_SUGG, stride=S_SUGG, pad=False)\nX_va_win, y_va_win = build_sequences(X_val,y_val, window=W_SUGG, stride=S_SUGG, pad=False)\nX_te_win, _        = build_sequences(X_test,None, window=W_SUGG, stride=S_SUGG, pad=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:01.245228Z","iopub.execute_input":"2025-11-13T22:12:01.245473Z","iopub.status.idle":"2025-11-13T22:12:03.824975Z","shell.execute_reply.started":"2025-11-13T22:12:01.245453Z","shell.execute_reply":"2025-11-13T22:12:03.824190Z"}},"outputs":[{"name":"stdout","text":"Built 26400 sequences; each shape = (12, 40)\nBuilt 6650 sequences; each shape = (12, 40)\nBuilt 66200 sequences; each shape = (12, 40)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"input_shape = X_tr_win.shape[1:] # extract the shape of a single sequence\nnum_classes = len(np.unique(y_tr_win)) # how many unique pain level exists\ninput_shape, num_classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:03.825938Z","iopub.execute_input":"2025-11-13T22:12:03.826226Z","iopub.status.idle":"2025-11-13T22:12:03.832525Z","shell.execute_reply.started":"2025-11-13T22:12:03.826200Z","shell.execute_reply":"2025-11-13T22:12:03.831844Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"((12, 40), 3)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n# each dataset now pairs each (T,F) TENSOR WITH ITS LABEL\ntrain_ds = TensorDataset(torch.from_numpy(X_tr_win), torch.from_numpy(y_tr_win))\nval_ds   = TensorDataset(torch.from_numpy(X_va_win), torch.from_numpy(y_va_win))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:03.833588Z","iopub.execute_input":"2025-11-13T22:12:03.834375Z","iopub.status.idle":"2025-11-13T22:12:03.890632Z","shell.execute_reply.started":"2025-11-13T22:12:03.834353Z","shell.execute_reply":"2025-11-13T22:12:03.889695Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Define the batch size, which is the number of samples in each batch\nBATCH_SIZE = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:03.891667Z","iopub.execute_input":"2025-11-13T22:12:03.891995Z","iopub.status.idle":"2025-11-13T22:12:03.912321Z","shell.execute_reply.started":"2025-11-13T22:12:03.891969Z","shell.execute_reply":"2025-11-13T22:12:03.911625Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from torch.utils.data import WeightedRandomSampler\n\ndef make_loader(ds, batch_size, shuffle, drop_last, sampler=None):\n    # Determine optimal number of worker processes for data loading\n    cpu_cores = os.cpu_count() or 2\n    num_workers = max(2, min(4, cpu_cores))\n\n    final_shuffle = shuffle\n    if sampler is not None:\n        final_shuffle = False\n\n    # Create DataLoader with performance optimizations\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=final_shuffle,\n        drop_last=drop_last,\n        num_workers=num_workers,\n        sampler=sampler,\n        pin_memory=True,  # Faster GPU transfer\n        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n        prefetch_factor=4,  # Load 4 batches aheads\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:03.913052Z","iopub.execute_input":"2025-11-13T22:12:03.913323Z","iopub.status.idle":"2025-11-13T22:12:03.933904Z","shell.execute_reply.started":"2025-11-13T22:12:03.913292Z","shell.execute_reply":"2025-11-13T22:12:03.933106Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# After building sequences\nX_tr_win, y_tr_win = build_sequences(X_train, y_train, window=W_SUGG, stride=S_SUGG, pad=False)\nX_va_win, y_va_win = build_sequences(X_val, y_val, window=W_SUGG, stride=S_SUGG, pad=False)\n\n# CRITICAL: Verify pain features are valid INTEGERS\nprint(\"\\n=== DATA VERIFICATION ===\")\nprint(f\"Train data shape: {X_tr_win.shape}\")\nprint(f\"Train data dtype: {X_tr_win.dtype}\")\n\n# Check pain features (assuming columns 30-33, adjust based on your num_joint_features)\nnum_joint_features = len([c for c in X_train.columns if c.startswith('joint_')])\npain_start = num_joint_features\npain_end = pain_start + 4\n\nsample_pain = X_tr_win[0, :, pain_start:pain_end]\nprint(f\"\\nPain features (columns {pain_start}-{pain_end-1}):\")\nprint(f\"  Range: [{sample_pain.min():.3f}, {sample_pain.max():.3f}]\")\nprint(f\"  Unique values: {np.unique(sample_pain)}\")\nprint(f\"  Expected: [0. 1. 2.]\")\n\n# Check if values are EXACTLY 0.0, 1.0, 2.0\nif not np.array_equal(np.unique(sample_pain), np.array([0., 1., 2.])):\n    print(\"\\nâŒ ERROR: Pain features are NOT {0, 1, 2}!\")\n    print(\"   Your preprocessing is wrong. Check dataset_conversion_type_embed_ready.\")\nelse:\n    print(\"\\nâœ“ Pain features are valid\")\n\n# Check static features\nstatic_start = pain_end\nstatic_end = static_start + 3\nsample_static = X_tr_win[0, :, static_start:static_end]\nprint(f\"\\nStatic features (columns {static_start}-{static_end-1}):\")\nprint(f\"  Range: [{sample_static.min():.3f}, {sample_static.max():.3f}]\")\nprint(f\"  Unique values: {np.unique(sample_static)}\")\nprint(f\"  Expected: [0. 1.]\")\n\nif not np.array_equal(np.unique(sample_static), np.array([0., 1.])):\n    print(\"\\nâŒ ERROR: Static features are NOT {0, 1}!\")\nelse:\n    print(\"\\nâœ“ Static features are valid\")\n\nprint(\"=========================\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:03.934858Z","iopub.execute_input":"2025-11-13T22:12:03.935189Z","iopub.status.idle":"2025-11-13T22:12:04.798840Z","shell.execute_reply.started":"2025-11-13T22:12:03.935166Z","shell.execute_reply":"2025-11-13T22:12:04.797926Z"}},"outputs":[{"name":"stdout","text":"Built 26400 sequences; each shape = (12, 40)\nBuilt 6650 sequences; each shape = (12, 40)\n\n=== DATA VERIFICATION ===\nTrain data shape: (26400, 12, 40)\nTrain data dtype: float32\n\nPain features (columns 30-33):\n  Range: [0.000, 2.000]\n  Unique values: [0. 1. 2.]\n  Expected: [0. 1. 2.]\n\nâœ“ Pain features are valid\n\nStatic features (columns 34-36):\n  Range: [0.000, 0.000]\n  Unique values: [0.]\n  Expected: [0. 1.]\n\nâŒ ERROR: Static features are NOT {0, 1}!\n=========================\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\nval_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:04.799754Z","iopub.execute_input":"2025-11-13T22:12:04.800128Z","iopub.status.idle":"2025-11-13T22:12:04.804709Z","shell.execute_reply.started":"2025-11-13T22:12:04.800103Z","shell.execute_reply":"2025-11-13T22:12:04.804030Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Get one batch from the training data loader\nfor xb, yb in train_loader:\n    print(\"Features batch shape:\", xb.shape)\n    print(\"Labels batch shape:\", yb.shape)\n    labels = yb.cpu().numpy()  # assuming (X, y) batch\n    print('Batch class counts:', np.unique(labels, return_counts=True))\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:04.805549Z","iopub.execute_input":"2025-11-13T22:12:04.805930Z","iopub.status.idle":"2025-11-13T22:12:05.309683Z","shell.execute_reply.started":"2025-11-13T22:12:04.805905Z","shell.execute_reply":"2025-11-13T22:12:05.308858Z"}},"outputs":[{"name":"stdout","text":"Features batch shape: torch.Size([32, 12, 40])\nLabels batch shape: torch.Size([32])\nBatch class counts: (array([0, 1, 2]), array([25,  6,  1]))\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## ðŸ› ï¸ **Model Building**","metadata":{}},{"cell_type":"code","source":"class WeightLabelLoss(nn.Module):\n    \"\"\"\n    Combines:\n    - Advice 08/11: Class weights for imbalance\n    - Advice 09/11: Label smoothing for generalization (per-class)\n    \"\"\"\n    def __init__(self, num_classes=3, class_weights=None, smoothing_per_class=None):\n        super().__init__()\n        self.num_classes = num_classes\n        \n        # Per-class label smoothing\n        if smoothing_per_class is None:\n            # Default: more smoothing for majority, less for minority\n            smoothing_per_class = [0.15, 0.01, 0.005]\n        \n        if isinstance(smoothing_per_class, (list, tuple)):\n            # Convert list to tensor\n            self.smoothing_per_class = torch.tensor(smoothing_per_class, dtype=torch.float32)\n        else:\n            # Single value: use for all classes\n            self.smoothing_per_class = torch.tensor([smoothing_per_class] * num_classes, dtype=torch.float32)\n        \n        # Register as buffer (moves with model to GPU)\n        self.register_buffer('smoothing', self.smoothing_per_class)\n        \n        # Class weights\n        if class_weights is not None:\n            self.register_buffer('class_weights', class_weights)\n        else:\n            self.register_buffer('class_weights', torch.ones(num_classes))\n    \n    def forward(self, pred, target):\n        # Apply label smoothing\n        log_probs = F.log_softmax(pred, dim=-1)\n        batch_size = target.size(0)\n        \n        # Create smoothed targets with PER-CLASS smoothing\n        smooth_targets = torch.zeros_like(pred)\n        \n        for i in range(batch_size):\n            true_class = target[i].item()\n            smoothing = self.smoothing[true_class].item()  # Get smoothing for this class\n            confidence = 1.0 - smoothing\n            \n            # Distribute smoothing uniformly across other classes\n            smooth_targets[i].fill_(smoothing / (self.num_classes - 1))\n            smooth_targets[i, true_class] = confidence\n        \n        # Compute loss with class weights\n        loss_per_sample = -(smooth_targets * log_probs).sum(dim=-1)\n        \n        # Apply class weights based on true label\n        weights = self.class_weights[target]\n        weighted_loss = (loss_per_sample * weights).mean()\n        \n        return weighted_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.310650Z","iopub.execute_input":"2025-11-13T22:12:05.310995Z","iopub.status.idle":"2025-11-13T22:12:05.319285Z","shell.execute_reply.started":"2025-11-13T22:12:05.310969Z","shell.execute_reply":"2025-11-13T22:12:05.318741Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def recurrent_summary(model, input_size):\n    \"\"\"\n    Custom summary function that emulates torchinfo's output while correctly\n    counting parameters for RNN/GRU/LSTM layers.\n\n    This function is designed for models whose direct children are\n    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n\n    Args:\n        model (nn.Module): The model to analyze.\n        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n    \"\"\"\n\n    # Dictionary to store output shapes captured by forward hooks\n    output_shapes = {}\n    # List to track hook handles for later removal\n    hooks = []\n\n    def get_hook(name):\n        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n        def hook(module, input, output):\n            # Handle RNN layer outputs (returns a tuple)\n            if isinstance(output, tuple):\n                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n                shape1 = list(output[0].shape)\n                shape1[0] = -1  # Replace batch dimension with -1\n\n                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n                    shape2 = list(output[1][0].shape)  # Extract h_n only\n                else:  # RNN/GRU case: h_n only\n                    shape2 = list(output[1].shape)\n\n                # Replace batch dimension (middle position) with -1\n                shape2[1] = -1\n\n                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n\n            # Handle standard layer outputs (e.g., Linear)\n            else:\n                shape = list(output.shape)\n                shape[0] = -1  # Replace batch dimension with -1\n                output_shapes[name] = f\"{shape}\"\n        return hook\n\n    # 1. Determine the device where model parameters reside\n    try:\n        device = next(model.parameters()).device\n    except StopIteration:\n        device = torch.device(\"cpu\")  # Fallback for models without parameters\n\n    # 2. Create a dummy input tensor with batch_size=1\n    dummy_input = torch.randn(1, *input_size).to(device)\n\n    # 3. Register forward hooks on target layers\n    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n    for name, module in model.named_children():\n        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n            # Register the hook and store its handle for cleanup\n            hook_handle = module.register_forward_hook(get_hook(name))\n            hooks.append(hook_handle)\n\n    # 4. Execute a dummy forward pass in evaluation mode\n    model.eval()\n    with torch.no_grad():\n        try:\n            model(dummy_input)\n        except Exception as e:\n            print(f\"Error during dummy forward pass: {e}\")\n            # Clean up hooks even if an error occurs\n            for h in hooks:\n                h.remove()\n            return\n\n    # 5. Remove all registered hooks\n    for h in hooks:\n        h.remove()\n\n    # --- 6. Print the summary table ---\n\n    print(\"-\" * 79)\n    # Column headers\n    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n    print(\"=\" * 79)\n\n    total_params = 0\n    total_trainable_params = 0\n\n    # Iterate through modules again to collect and display parameter information\n    for name, module in model.named_children():\n        if name in output_shapes:\n            # Count total and trainable parameters for this module\n            module_params = sum(p.numel() for p in module.parameters())\n            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n\n            total_params += module_params\n            total_trainable_params += trainable_params\n\n            # Format strings for display\n            layer_name = f\"{name} ({type(module).__name__})\"\n            output_shape_str = str(output_shapes[name])\n            params_str = f\"{trainable_params:,}\"\n\n            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n\n    print(\"=\" * 79)\n    print(f\"Total params: {total_params:,}\")\n    print(f\"Trainable params: {total_trainable_params:,}\")\n    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n    print(\"-\" * 79)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.320103Z","iopub.execute_input":"2025-11-13T22:12:05.320820Z","iopub.status.idle":"2025-11-13T22:12:05.341047Z","shell.execute_reply.started":"2025-11-13T22:12:05.320802Z","shell.execute_reply":"2025-11-13T22:12:05.340143Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# CRITICAL: Count your features BEFORE creating model\njoint_cols = [c for c in X_train.columns if c.startswith('joint_')]\npain_cols = [c for c in X_train.columns if c.startswith('pain_survey_')]\nstatic_cols = ['n_legs', 'n_hands', 'n_eyes']\n\nNUM_JOINT_FEATURES = len(joint_cols) \nNUM_PAIN_FEATURES = len(pain_cols) \nNUM_STATIC_FEATURES = len([c for c in static_cols if c in X_train.columns])  # \nNUM_TIME_FEATURES = 3\n\nprint(f\"Feature counts:\")\nprint(f\"  Joint: {NUM_JOINT_FEATURES}\")\nprint(f\"  Pain: {NUM_PAIN_FEATURES}\")\nprint(f\"  Static: {NUM_STATIC_FEATURES}\")\nprint(f\"  Time: {NUM_TIME_FEATURES}\")\nprint(f\"  Total: {NUM_JOINT_FEATURES + NUM_PAIN_FEATURES + NUM_STATIC_FEATURES + NUM_TIME_FEATURES}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.342003Z","iopub.execute_input":"2025-11-13T22:12:05.342241Z","iopub.status.idle":"2025-11-13T22:12:05.370353Z","shell.execute_reply.started":"2025-11-13T22:12:05.342223Z","shell.execute_reply":"2025-11-13T22:12:05.369651Z"}},"outputs":[{"name":"stdout","text":"Feature counts:\n  Joint: 30\n  Pain: 4\n  Static: 3\n  Time: 3\n  Total: 40\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"class RecurrentClassifier(nn.Module):\n    def __init__(\n            self,\n            input_size,\n            hidden_size,\n            num_layers,\n            num_classes,\n            rnn_type='GRU',\n            bidirectional=False,\n            dropout_rate=0.2,\n            rec_dropout_rate=None,\n            cnn_channels=None,\n            cnn_kernel_size=3,\n            cnn_dropout=None,\n            # NEW PARAMETERS\n            use_pain_embeddings=True,\n            pain_embedding_dim=4,\n            num_joint_features=NUM_JOINT_FEATURES,  # You must provide this\n            num_pain_features=NUM_PAIN_FEATURES,    # Number of pain_survey_* features (always 4)\n            num_static_features=NUM_STATIC_FEATURES,  # n_legs, n_hands, n_eyes (always 3)\n            num_time_features=NUM_TIME_FEATURES     # time features (always 3)\n            ):\n        super().__init__()\n\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n        self.uses_cnn = cnn_channels is not None\n        self.use_pain_embeddings = use_pain_embeddings\n        \n        # Store feature split indices\n        self.num_joint_features = num_joint_features\n        self.num_pain_features = num_pain_features\n        self.num_static_features = num_static_features\n        self.num_time_features = num_time_features\n        \n        # Compute split indices for forward()\n        self.joint_end = num_joint_features\n        self.pain_end = self.joint_end + num_pain_features\n        self.static_end = self.pain_end + num_static_features\n        # time_end = input_size (implicitly)\n        \n        # Verify expected input size matches\n        expected_input = num_joint_features + num_pain_features + num_static_features + num_time_features\n        if input_size != expected_input:\n            print(f\"WARNING: input_size={input_size} but expected {expected_input} \"\n                  f\"(joint={num_joint_features} + pain={num_pain_features} + \"\n                  f\"static={num_static_features} + time={num_time_features})\")\n        \n        # Embedding layers for pain features\n        if self.use_pain_embeddings:\n            self.pain_embeddings = nn.ModuleList([\n                nn.Embedding(num_embeddings=3, embedding_dim=pain_embedding_dim) \n                for _ in range(num_pain_features)\n            ])\n            # Effective input after embedding\n            effective_input_size = (num_joint_features + \n                                   num_pain_features * pain_embedding_dim + \n                                   num_static_features + \n                                   num_time_features)\n        else:\n            effective_input_size = input_size\n\n        # CNN block\n        if self.uses_cnn:\n            self.cnn1 = nn.Conv1d(in_channels=effective_input_size,\n                                  out_channels=cnn_channels,\n                                  kernel_size=cnn_kernel_size,\n                                  padding=cnn_kernel_size // 2)\n            self.cnn_bn = nn.BatchNorm1d(cnn_channels)\n            self.cnn_act = nn.ReLU()\n            self.cnn_dropout = nn.Dropout(cnn_dropout if cnn_dropout is not None else dropout_rate)\n            rnn_input_size = cnn_channels\n        else:\n            rnn_input_size = effective_input_size\n\n        # RNN layers\n        rnn_map = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}\n        if rnn_type not in rnn_map:\n            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n        rnn_module = rnn_map[rnn_type]\n        dropout_val = dropout_rate if num_layers > 1 else 0\n        self.rnn = rnn_module(\n            input_size=rnn_input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=bidirectional,\n            dropout=dropout_val\n        )\n        \n        if self.bidirectional:\n            classifier_input_size = hidden_size * 2\n        else:\n            classifier_input_size = hidden_size\n            \n        self.classifier = nn.Linear(classifier_input_size, num_classes)\n        self.rec_dropout = nn.Dropout(rec_dropout_rate) if rec_dropout_rate is not None else None\n\n    def forward(self, x):\n        # x shape: (batch_size, seq_len, input_size)\n        \n        if self.use_pain_embeddings:\n            # Split using computed indices\n            X_joints = x[:, :, :self.joint_end]                      # joint_*\n            X_pain = x[:, :, self.joint_end:self.pain_end]           # pain_survey_*\n            X_static = x[:, :, self.pain_end:self.static_end]        # n_legs/hands/eyes\n            X_time = x[:, :, self.static_end:]                       # time features\n            \n            # Embed pain features\n            pain_embedded_list = []\n            for i in range(self.num_pain_features):\n                pain_feature = X_pain[:, :, i].long()\n                embedded = self.pain_embeddings[i](pain_feature)\n                pain_embedded_list.append(embedded)\n            \n            X_pain_embedded = torch.cat(pain_embedded_list, dim=-1)\n            \n            # Reconstruct with embedded features\n            x = torch.cat([X_joints, X_pain_embedded, X_static, X_time], dim=-1)\n        \n        # CNN and RNN processing (unchanged)\n        if self.uses_cnn:\n            x = x.transpose(1, 2)\n            x = self.cnn1(x)\n            x = self.cnn_bn(x)\n            x = self.cnn_act(x)\n            x = self.cnn_dropout(x)\n            x = x.transpose(1, 2)\n            \n        rnn_out, hidden = self.rnn(x)\n        \n        if self.rnn_type == 'LSTM':\n            hidden = hidden[0]\n            \n        if self.bidirectional:\n            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n            hidden_to_classify = torch.cat([hidden[-1,0,:,:], hidden[-1,1,:,:]], dim=1)\n        else:\n            hidden_to_classify = hidden[-1]\n            \n        if self.rec_dropout is not None:\n            hidden_to_classify = self.rec_dropout(hidden_to_classify)\n            \n        logits = self.classifier(hidden_to_classify)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.371279Z","iopub.execute_input":"2025-11-13T22:12:05.371551Z","iopub.status.idle":"2025-11-13T22:12:05.393460Z","shell.execute_reply.started":"2025-11-13T22:12:05.371527Z","shell.execute_reply":"2025-11-13T22:12:05.392838Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"## ðŸ§  **Model Training**","metadata":{}},{"cell_type":"code","source":"# Initialize best model tracking variables\nbest_model = None\nbest_performance = float('-inf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.394318Z","iopub.execute_input":"2025-11-13T22:12:05.394769Z","iopub.status.idle":"2025-11-13T22:12:05.418833Z","shell.execute_reply.started":"2025-11-13T22:12:05.394742Z","shell.execute_reply":"2025-11-13T22:12:05.417914Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, criterion, optimizer, scaler, \n                    device, l1_lambda=0, l2_lambda=0,max_grad_norm=1.0):\n    \"\"\"\n    Perform one complete training epoch through the entire training dataset.\n\n    Args:\n        model (nn.Module): The neural network model to train\n        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n        l1_lambda (float): Lambda for L1 regularization\n        l2_lambda (float): Lambda for L2 regularization\n\n    Returns:\n        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n    \"\"\"\n    model.train()  # Set model to training mode\n\n    running_loss = 0.0\n    all_predictions = []\n    all_targets = []\n\n    # Iterate through training batches\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        # Move data to device (GPU/CPU)\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        # Clear gradients from previous step\n        optimizer.zero_grad(set_to_none=True)\n\n        # Forward pass with mixed precision (if CUDA available)\n        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n\n            # Add L1 and L2 regularization\n            l1_norm = sum(p.abs().sum() for p in model.parameters())\n            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n\n\n        # Backward pass with gradient scaling\n        if scaler is not None and device.type == 'cuda':\n            scaler.scale(loss).backward()            # grads are scaled\n            scaler.unscale_(optimizer)               # unscale to true grad values\n            torch.nn.utils.clip_grad_norm_(          # CLIP true gradients (magnitude cap)\n                model.parameters(), max_norm=max_grad_norm\n            )\n            scaler.step(optimizer)                   # safe optimizer.step() (skips on inf/NaN)\n            scaler.update()                          # update scaling factor\n        else:\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n            optimizer.step()\n\n        # Accumulate metrics\n        running_loss += loss.item() * inputs.size(0)\n        predictions = logits.argmax(dim=1)\n        all_predictions.append(predictions.cpu().numpy())\n        all_targets.append(targets.cpu().numpy())\n\n    # Calculate epoch metrics\n    epoch_loss = running_loss / len(train_loader.dataset)\n    epoch_f1 = f1_score(\n        np.concatenate(all_targets),\n        np.concatenate(all_predictions),\n        average='macro'\n    )\n\n    return epoch_loss, epoch_f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.419674Z","iopub.execute_input":"2025-11-13T22:12:05.419924Z","iopub.status.idle":"2025-11-13T22:12:05.440557Z","shell.execute_reply.started":"2025-11-13T22:12:05.419900Z","shell.execute_reply":"2025-11-13T22:12:05.439791Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def validate_one_epoch(model, val_loader, criterion, device):\n    \"\"\"\n    Perform one complete validation epoch through the entire validation dataset.\n\n    Args:\n        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n        criterion (nn.Module): Loss function used to calculate validation loss\n        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n\n    Returns:\n        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n\n    Note:\n        This function automatically sets the model to evaluation mode and disables\n        gradient computation for efficiency during validation.\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n\n    running_loss = 0.0\n    all_predictions = []\n    all_targets = []\n\n    # Disable gradient computation for validation\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            # Move data to device\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Forward pass with mixed precision (if CUDA available)\n            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n                logits = model(inputs)\n                loss = criterion(logits, targets)\n\n            # Accumulate metrics\n            running_loss += loss.item() * inputs.size(0)\n            predictions = logits.argmax(dim=1)\n            all_predictions.append(predictions.cpu().numpy())\n            all_targets.append(targets.cpu().numpy())\n\n    # Calculate epoch metrics\n    epoch_loss = running_loss / len(val_loader.dataset)\n    epoch_accuracy = f1_score(\n        np.concatenate(all_targets),\n        np.concatenate(all_predictions),\n        average='macro'\n    )\n\n    return epoch_loss, epoch_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.441538Z","iopub.execute_input":"2025-11-13T22:12:05.441770Z","iopub.status.idle":"2025-11-13T22:12:05.466592Z","shell.execute_reply.started":"2025-11-13T22:12:05.441753Z","shell.execute_reply":"2025-11-13T22:12:05.465929Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n    \"\"\"\n    Log training metrics and model parameters to TensorBoard for visualization.\n\n    Args:\n        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n        train_loss (float): Training loss for this epoch\n        train_f1 (float): Training f1 score for this epoch\n        val_loss (float): Validation loss for this epoch\n        val_f1 (float): Validation f1 score for this epoch\n        model (nn.Module): The neural network model (for logging weights/gradients)\n\n    Note:\n        This function logs scalar metrics (loss/f1 score) and histograms of model\n        parameters and gradients, which helps monitor training progress and detect\n        issues like vanishing/exploding gradients.\n    \"\"\"\n    # Log scalar metrics\n    writer.add_scalar('Loss/Training', train_loss, epoch)\n    writer.add_scalar('Loss/Validation', val_loss, epoch)\n    writer.add_scalar('F1/Training', train_f1, epoch)\n    writer.add_scalar('F1/Validation', val_f1, epoch)\n\n    # Log model parameters and gradients\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            # Check if the tensor is not empty before adding a histogram\n            if param.numel() > 0:\n                writer.add_histogram(f'{name}/weights', param.data, epoch)\n            if param.grad is not None:\n                # Check if the gradient tensor is not empty before adding a histogram\n                if param.grad.numel() > 0:\n                    if param.grad is not None and torch.isfinite(param.grad).all():\n                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.467476Z","iopub.execute_input":"2025-11-13T22:12:05.467743Z","iopub.status.idle":"2025-11-13T22:12:05.494359Z","shell.execute_reply.started":"2025-11-13T22:12:05.467716Z","shell.execute_reply":"2025-11-13T22:12:05.493406Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def fit(model, train_loader, val_loader, epochs, train_criterion, val_criterion, optimizer, scaler, device,\n        l1_lambda=0, l2_lambda=0, patience=0, scheduler=None, # Added scheduler parameter\n        evaluation_metric=\"val_f1\", mode='max', \n        restore_best_weights=True, writer=None, verbose=1, experiment_name=\"\"):\n    \"\"\"\n    Train the neural network model on the training data and validate on the validation data.\n\n    Args:\n        model (nn.Module): The neural network model to train\n        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n        epochs (int): Number of training epochs\n        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n        l1_lambda (float): L1 regularization coefficient (default: 0)\n        l2_lambda (float): L2 regularization coefficient (default: 0)\n        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n        verbose (int, optional): Frequency of printing training progress (default: 10)\n        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n\n    Returns:\n        tuple: (model, training_history) - Trained model and metrics history\n    \"\"\"\n\n    # Initialize metrics tracking\n    training_history = {\n        'train_loss': [], 'val_loss': [],\n        'train_f1': [], 'val_f1': []\n    }\n\n    # Configure early stopping if patience is set\n    if patience > 0:\n        patience_counter = 0\n        best_metric = float('-inf') if mode == 'max' else float('inf')\n        best_epoch = 0\n\n    print(f\"Training {epochs} epochs...\")\n\n    # Main training loop: iterate through epochs\n    for epoch in range(1, epochs + 1):\n\n        # Forward pass through training data, compute gradients, update weights\n        train_loss, train_f1 = train_one_epoch(\n            model, train_loader, train_criterion, optimizer, scaler, device\n        )\n\n        # Evaluate model on validation data without updating weights\n        if val_loader is not None:\n            val_loss, val_f1 = validate_one_epoch(model, val_loader, val_criterion, device)\n        else:\n            val_loss, val_f1 = None, None\n\n\n        # Step the scheduler if provided (typically after validation)\n        if scheduler is not None:\n            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) or isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingWarmRestarts):\n                scheduler.step(val_f1)\n            else:\n                scheduler.step()\n\n        # Store metrics for plotting and analysis\n        training_history['train_loss'].append(train_loss)\n        training_history['val_loss'].append(val_loss)\n        training_history['train_f1'].append(train_f1)\n        training_history['val_f1'].append(val_f1)\n\n        # Write metrics to TensorBoard for visualization\n        if writer is not None:\n            log_metrics_to_tensorboard(\n                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n            )\n\n        # Print progress every N epochs or on first epoch\n        if verbose > 0:\n            if epoch % verbose == 0 or epoch == 1:\n                if val_loss is not None:\n                    print(f\"Epoch {epoch:3d}/{epochs} | \"\n                          f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n                          f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n                else:\n                    print(f\"Epoch {epoch:3d}/{epochs} | \"\n                          f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f}\")\n\n\n        # Early stopping logic: monitor metric and save best model\n        if patience > 0 and val_loader is not None:\n            current_metric = training_history[evaluation_metric][-1]\n            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n\n            if is_improvement:\n                best_metric = current_metric\n                best_epoch = epoch\n                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping triggered after {epoch} epochs.\")\n                    break\n\n\n    # Restore best model weights if early stopping was used\n    if restore_best_weights and patience > 0:\n        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n\n    # Save final model if no early stopping\n    if patience == 0:\n        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n        \n    if patience > 0:\n        training_history['best_epoch'] = best_epoch\n        training_history['best_metric'] = best_metric\n        \n    # Close TensorBoard writer\n    if writer is not None:\n        writer.close()\n\n    return model, training_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.500622Z","iopub.execute_input":"2025-11-13T22:12:05.500833Z","iopub.status.idle":"2025-11-13T22:12:05.521835Z","shell.execute_reply.started":"2025-11-13T22:12:05.500816Z","shell.execute_reply":"2025-11-13T22:12:05.521220Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"## ðŸ§® **Network and Training Hyperparameters**","metadata":{}},{"cell_type":"code","source":"y_train_np = np.array(y_train)\nnum_classes = np.max(y_train_np) + 1\nclass_counts = np.bincount(y_train_np, minlength=num_classes)\ntotal = len(y_train_np)\n\nprint(\"Training class distribution:\")\nfor i, name in enumerate(['no_pain', 'low_pain', 'high_pain']):\n    print(f\"  {name} ({i}): {class_counts[i]} ({class_counts[i]/total*100:.1f}%)\")\n\n# SQRT dampening\nmax_count = class_counts.max()\nclass_weights_raw = max_count / class_counts\nclass_weights_dampened = np.sqrt(class_weights_raw)\n\n# Convert to tensor (CRITICAL!)\nclass_weights = torch.tensor(class_weights_dampened, dtype=torch.float32)\n\nprint(f\"\\nClass weights (sqrt dampened): {class_weights}\")\nprint(f\"Type: {type(class_weights)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.522621Z","iopub.execute_input":"2025-11-13T22:12:05.522933Z","iopub.status.idle":"2025-11-13T22:12:05.617747Z","shell.execute_reply.started":"2025-11-13T22:12:05.522909Z","shell.execute_reply":"2025-11-13T22:12:05.617118Z"}},"outputs":[{"name":"stdout","text":"Training class distribution:\n  no_pain (0): 408 (77.3%)\n  low_pain (1): 75 (14.2%)\n  high_pain (2): 45 (8.5%)\n\nClass weights (sqrt dampened): tensor([1.0000, 2.3324, 3.0111])\nType: <class 'torch.Tensor'>\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Training configuration\nLEARNING_RATE = 5e-4\nEPOCHS = 150\nPATIENCE = 8\n\n# Architecture\nHIDDEN_LAYERS = 2        # Hidden layers\nHIDDEN_SIZE = 256        # Neurons per layer\n\n# Regularisation\nDROPOUT_RATE = 0.4         # Dropout probability\nRECURRENT_DROPOUT = 0.2      # Adds mild regularization inside GRU layers\nL1_LAMBDA = 0            # L1 penalty\nL2_LAMBDA = 3e-4            # L2 penalty\n\nCNN_CHANNELS=64                  # enables CNN1D preprocessing\nCNN_KERNEL_SIZE=3                # usually 3, 5, or 7 works well\nCNN_DROPOUT=0.25                  # (optional; default uses RNN dropout)\n\n# ===== TRAINING CONFIGURATION =====\nLEARNING_RATE = 2e-4      # Reduced from 5e-4 (more stable with embeddings)\nEPOCHS = 150\nPATIENCE = 12             # Increased from 8 (more complex model needs patience)\n# Smaller batches help with imbalanced data\n\n# ===== ARCHITECTURE =====\nHIDDEN_LAYERS = 2         # Keep\nHIDDEN_SIZE = 256         # Keep\n\n# ===== REGULARIZATION =====\nDROPOUT_RATE = 0.25       # Reduced from 0.4 (was way too high)\nRECURRENT_DROPOUT = 0.1   # Reduced from 0.2\nL2_LAMBDA = 5e-5          # Reduced from 3e-4 (was too aggressive)\nL1_LAMBDA = 0 \n\n# ===== CNN CONFIGURATION =====\nCNN_CHANNELS = 96         # Increased from 64 (for multi-scale if you add it)\nCNN_KERNEL_SIZE = 3       # Keep\nCNN_DROPOUT = 0.2         # Reduced from 0.25\n\n\n# Set up loss function and optimizer\n\n# Loss function with FocalLabelSmoothing\ntrain_criterion = WeightLabelLoss(\n    num_classes=3,\n    class_weights=class_weights              # Use sqrt dampened weights\n).to(device)\n\nval_criterion = nn.CrossEntropyLoss().to(device)\n\n# model\nMODEL='GRU'\nRNN_TYPE=MODEL\nBIDIRECTIONAL=True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.618675Z","iopub.execute_input":"2025-11-13T22:12:05.618995Z","iopub.status.idle":"2025-11-13T22:12:05.631368Z","shell.execute_reply.started":"2025-11-13T22:12:05.618969Z","shell.execute_reply":"2025-11-13T22:12:05.630549Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Create model and display architecture with parameter count\nrnn_model = RecurrentClassifier(\n    input_size=input_shape[-1], # Pass the number of features\n    hidden_size=HIDDEN_SIZE,\n    num_layers=HIDDEN_LAYERS,\n    num_classes=num_classes,\n    dropout_rate=DROPOUT_RATE,\n    rec_dropout_rate = RECURRENT_DROPOUT,\n    bidirectional=BIDIRECTIONAL,\n    cnn_channels=CNN_CHANNELS,                  # enables CNN1D preprocessing\n    cnn_kernel_size=CNN_KERNEL_SIZE,                # usually 3, 5, or 7 works well\n    cnn_dropout=CNN_DROPOUT,                  # (optional; default uses RNN dropout)\n    rnn_type=MODEL\n    ).to(device)\n# recurrent_summary(rnn_model, input_size=input_shape)\n\n# Set up TensorBoard logging and save model architecture\nprefix = \"bi_\" if BIDIRECTIONAL else \"\"\nexperiment_name = prefix + MODEL.lower()\nwriter = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n# x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n# writer.add_graph(rnn_model, x)\n\n# Define optimizer with L2 regularization\noptimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n# scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=8, verbose=True, min_lr=1e-6)\nscheduler = CosineAnnealingWarmRestarts(\n    optimizer, \n    T_0=15,        # Restart every 15 epochs\n    T_mult=2,      # Double restart period each time\n    eta_min=1e-6   # Minimum LR\n)\n\n# Enable mixed precision training for GPU acceleration\nscaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:05.632176Z","iopub.execute_input":"2025-11-13T22:12:05.632419Z","iopub.status.idle":"2025-11-13T22:12:09.247206Z","shell.execute_reply.started":"2025-11-13T22:12:05.632394Z","shell.execute_reply":"2025-11-13T22:12:09.246586Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"%%time\n# Train model and track training history\nrnn_model, training_history = fit(\n    model=rnn_model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    epochs=EPOCHS,\n    train_criterion=train_criterion,\n    val_criterion=val_criterion,\n    optimizer=optimizer,\n    scaler=scaler,\n    scheduler=scheduler,\n    device=device,\n    writer=writer,\n    verbose=1,\n    experiment_name=MODEL.lower(),\n    patience=PATIENCE\n    )\n\n# Update best model if current performance is superior\nif training_history['val_f1'][-1] > best_performance:\n    best_model = rnn_model\n    best_performance = training_history['val_f1'][-1]\n\nbest_epoch_number = training_history['best_epoch']\nprint(f\"Best epoch was: {best_epoch_number}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:12:09.247919Z","iopub.execute_input":"2025-11-13T22:12:09.248519Z"}},"outputs":[{"name":"stdout","text":"Training 150 epochs...\nEpoch   1/150 | Train: Loss=1.1017, F1 Score=0.6424 | Val: Loss=0.4253, F1 Score=0.7107\nEpoch   2/150 | Train: Loss=0.8730, F1 Score=0.7720 | Val: Loss=0.4479, F1 Score=0.7910\nEpoch   3/150 | Train: Loss=0.7729, F1 Score=0.8257 | Val: Loss=0.4048, F1 Score=0.7911\nEpoch   4/150 | Train: Loss=0.7174, F1 Score=0.8596 | Val: Loss=0.3866, F1 Score=0.8073\nEpoch   5/150 | Train: Loss=0.6843, F1 Score=0.8741 | Val: Loss=0.3373, F1 Score=0.8309\nEpoch   6/150 | Train: Loss=0.6459, F1 Score=0.8951 | Val: Loss=0.3702, F1 Score=0.8160\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Experiments","metadata":{}},{"cell_type":"code","source":"# @title Plot Hitory\n# Create a figure with two side-by-side subplots (two columns)\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n\n# Plot of training and validation loss on the first axis\nax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\nax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\nax1.set_title('Loss')\nax1.legend()\nax1.grid(alpha=0.3)\n\n# Plot of training and validation accuracy on the second axis\nax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\nax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\nax2.set_title('F1 Score')\nax2.legend()\nax2.grid(alpha=0.3)\n\n# Adjust the layout and display the plot\nplt.tight_layout()\nplt.subplots_adjust(right=0.85)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Plot Confusion Matrix\n# Collect predictions and ground truth labels\nval_preds, val_targets = [], []\nwindow_to_sample = []  # Track which windows belong to which sample\n\nwith torch.no_grad():  # Disable gradient computation for inference\n    for xb, yb in val_loader:\n        xb = xb.to(device)\n        \n        # Forward pass: get model predictions\n        logits = rnn_model(xb)\n        preds = logits.argmax(dim=1).cpu().numpy()\n        \n        # Store batch results (these are still per-window)\n        val_preds.append(preds)\n        val_targets.append(yb.numpy())\n\n# Combine all batches into single arrays (still per-window)\nval_preds_windows = np.concatenate(val_preds)\nval_targets_windows = np.concatenate(val_targets)\n\n# ============= AGGREGATE WINDOWS TO SEQUENCES =============\n# Reconstruct mapping: each sequence produces (160 - W) // S + 1 windows\nn_windows_per_seq = (160 - W_SUGG) // S_SUGG + 1\n\n# Map window predictions back to sample_index\nunique_samples = X_val[\"sample_index\"].unique()\nsequence_preds = {}\nsequence_targets = {}\n\nfor idx, sid in enumerate(unique_samples):\n    # Extract windows for this sequence\n    start_idx = idx * n_windows_per_seq\n    end_idx = start_idx + n_windows_per_seq\n    \n    window_preds = val_preds_windows[start_idx:end_idx]\n    window_targets = val_targets_windows[start_idx:end_idx]\n    \n    # Aggregate strategy: MAJORITY VOTE\n    from collections import Counter\n    vote_counts = Counter(window_preds)\n    final_pred = vote_counts.most_common(1)[0][0]\n    \n    # Target should be same across all windows (sanity check)\n    assert len(np.unique(window_targets)) == 1, f\"Sample {sid} has inconsistent labels!\"\n    final_target = window_targets[0]\n    \n    sequence_preds[sid] = final_pred\n    sequence_targets[sid] = final_target\n\n# Convert to arrays for metrics\nval_preds = np.array([sequence_preds[sid] for sid in unique_samples])\nval_targets = np.array([sequence_targets[sid] for sid in unique_samples])\n# ============= END AGGREGATION =============\n\n# Calculate overall validation metrics (now sequence-level)\nval_acc = accuracy_score(val_targets, val_preds)\nval_prec = precision_score(val_targets, val_preds, average='macro')\nval_rec = recall_score(val_targets, val_preds, average='macro')\nval_f1 = f1_score(val_targets, val_preds, average='macro')\nprint(f\"Accuracy over the validation set: {val_acc:.4f}\")\nprint(f\"Precision over the validation set: {val_prec:.4f}\")\nprint(f\"Recall over the validation set: {val_rec:.4f}\")\nprint(f\"F1 score over the validation set: {val_f1:.4f}\")\n\n# Generate confusion matrix for detailed error analysis\ncm = confusion_matrix(val_targets, val_preds)\n\n# Create numeric labels for heatmap annotation\nlabels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n\n# Visualise confusion matrix\nplt.figure(figsize=(8, 7))\nsns.heatmap(cm, annot=labels, fmt='',\n            cmap='Blues')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix â€” Validation Set (Sequence-Level)')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# choose output directory manually\n\n# --- Kaggle ---\n# OUT_DIR = \"/kaggle/working\"\n\n# --- Cluster (Westworld / Elysium) ---\n# OUT_DIR = \"/home/cristiano.battistini/storage/an2dl_outputs\"\n\n# --- Docker / local environment ---\nOUT_DIR = os.path.join(os.getcwd(), \"outputs\")\n\n# --- Create directory if it doesn't exist ---\nos.makedirs(OUT_DIR, exist_ok=True)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_experiment_output(\n    model_name: str,\n    hyperparams: dict,\n    X_test_seq: np.ndarray,\n    label_mapping: dict,\n    sample_indices: list,\n    output_dir: str,\n    model=None,\n    batch_size: int = 256,\n    window_size: int = None,\n    stride: int = None\n):\n    \"\"\"\n    Run inference on the test set, save predictions and hyperparameters.\n\n    Args:\n        model_name (str): Name of the experiment (e.g. 'lstm', 'bilstm', 'ffn').\n        hyperparams (dict): Dict containing all hyperparameters and training config.\n        X_test_seq (np.ndarray): Test sequences of shape (N_windows, W, F) â€” windowed data.\n        label_mapping (dict): Mapping from label string to class index.\n        sample_indices (list): List of sample_index identifiers (as strings).\n        output_dir (str): Folder where submission and metadata are saved.\n        model (torch.nn.Module): Trained model for inference.\n        batch_size (int): Inference batch size.\n        window_size (int): Window size used to create X_test_seq.\n        stride (int): Stride used to create X_test_seq.\n    \"\"\"\n\n    os.makedirs(output_dir, exist_ok=True)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Reverse mapping\n    idx2label = {v: k for k, v in label_mapping.items()}\n\n    # --- Inference on windows ---\n    model.eval().to(device)\n    with torch.inference_mode():\n        logits = []\n        for i in range(0, len(X_test_seq), batch_size):\n            xb = torch.from_numpy(X_test_seq[i:i+batch_size]).to(device)\n            logits.append(model(xb).cpu().numpy())\n        logits = np.concatenate(logits, axis=0)  # (N_windows, num_classes)\n\n    # Get per-window predictions\n    pred_idx_windows = logits.argmax(axis=1)  # (N_windows,)\n\n    # --- Aggregate windows to sequences ---\n    # Calculate how many windows per sequence (assuming T=160)\n    if window_size is None or stride is None:\n        # Try to infer from hyperparams if not provided\n        window_size = hyperparams.get('window', 12)\n        stride = hyperparams.get('stride', 3)\n    \n    n_windows_per_seq = (160 - window_size) // stride + 1\n\n    # ============= FIX: SORT SAMPLE INDICES =============\n    # CRITICAL: X_test_seq windows are created in the order that sample_indices\n    # appear in X_test. We must match this order for aggregation.\n    # If build_sequences processes sample_indices in sorted order, sort here too.\n    sample_indices = sorted(sample_indices)  # Ensure alignment\n    # ============= END FIX =============\n    \n    # Group predictions by sample_index using majority vote\n    from collections import Counter\n    sequence_predictions = []\n    \n    for idx in range(len(sample_indices)):\n        # Extract windows for this sequence\n        start_idx = idx * n_windows_per_seq\n        end_idx = start_idx + n_windows_per_seq\n        \n        window_preds = pred_idx_windows[start_idx:end_idx]\n        \n        # Majority vote across windows\n        vote_counts = Counter(window_preds)\n        final_pred_idx = vote_counts.most_common(1)[0][0]\n        \n        sequence_predictions.append(final_pred_idx)\n    \n    # Convert indices to labels\n    pred_labels = [idx2label[int(i)] for i in sequence_predictions]\n\n    # --- Build submission DataFrame ---\n    submission = pd.DataFrame({\n        \"sample_index\": [str(sid).zfill(3) for sid in sample_indices],\n        \"label\": pred_labels\n    })\n\n    # --- Build file names ---\n    run_name = f\"{model_name}_exp\"\n    csv_path = os.path.join(output_dir, f\"{run_name}_submission.csv\")\n    json_path = os.path.join(output_dir, f\"{run_name}_config.json\")\n\n    # --- Save submission ---\n    submission.to_csv(csv_path, index=False)\n\n    # --- Save hyperparameters as JSON ---\n    with open(json_path, \"w\") as f:\n        json.dump(hyperparams, f, indent=4)\n\n    print(f\"Saved submission at: {csv_path}\")\n    print(f\"Saved hyperparameters at: {json_path}\")\n    return submission\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define your hyperparameters as a dict\nhyperparams = {\n    \"m\": MODEL,\n    \"lr\": LEARNING_RATE,\n    \"epochs\": EPOCHS,\n    \"pat\": PATIENCE,\n    \"hl\": HIDDEN_LAYERS,\n    \"hs\": HIDDEN_SIZE,\n    \"dr\": DROPOUT_RATE,\n    \"l1\": L1_LAMBDA,\n    \"l2\": L2_LAMBDA,\n    'bi': BIDIRECTIONAL,\n    \"cnn_channels\":CNN_CHANNELS,                  # enables CNN1D preprocessing\n    \"cnn_kernel_size\":CNN_KERNEL_SIZE,                # usually 3, 5, or 7 works well\n    \"cnn_dropout\":CNN_DROPOUT                 # (optional; default uses RNN dropout)\n}\n\nmodel = best_model if \"best_model\" in globals() else rnn_model\nmodel_name = f\"{MODEL.lower()}_bi\" if BIDIRECTIONAL else f\"{MODEL.lower()}_f\"\n\n# Run and save output\nsubmission = save_experiment_output(\n    model_name=model_name,\n    hyperparams=hyperparams,\n    X_test_seq=X_te_win,\n    label_mapping={'no_pain': 0, 'low_pain': 1, 'high_pain': 2},\n    sample_indices=X_test[\"sample_index\"].unique(),\n    output_dir=OUT_DIR,\n    model=model,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# FINAL TRAINING ON THE FULL DATASET + SUBMISSION GENERATION\n# ============================================================\nRNN_TYPE='GRU'\n\n# 1. Preprocess full training data\nDF, _ = preprocess_joints(X_TRAIN.copy())\nX_train_full, _ = dataset_conversion_type_embed_ready(DF)\ny_full = Y_TRAIN.copy()\n\n\nlabels_full = y_full[\"label\"].map(label_mapping)\ny_full_np = labels_full.to_numpy()\n\nnum_classes = np.max(y_full_np) + 1  # For 0-indexed labels: [0, 1, 2] â†’ num_classes=3\nclass_counts = np.bincount(y_full_np, minlength=num_classes)\ntotal = len(y_full_np)\n\nprint(\"\\nTraining class distribution:\")\nfor i, name in enumerate(['no_pain', 'low_pain', 'high_pain']):\n    print(f\"  {name} ({i}): {class_counts[i]} ({class_counts[i]/total*100:.1f}%)\")\n\nfinal_epochs = best_epoch_number\nprint('final epochs:', final_epochs)\n\n\n# 3. Merge features and labels\ntrain_merged = X_train_full.merge(y_full, on=\"sample_index\")\n\n# 4. Encode labels numerically BEFORE building sequences\nlabel_mapping = {\"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2}\ntrain_merged[\"label\"] = train_merged[\"label\"].map(label_mapping)\n\n# 5. Normalise feature values\n\nscale_columns = [col for col in train_merged.columns if col.startswith(\"joint_\")]\n# calculate the minimum and maximum values from the training data only\nmins = X_train[scale_columns].min()\nmaxs = X_train[scale_columns].max()\n\n# apply normalisation to the specified columns in all datasets (training and validation)\nfor column in scale_columns:\n\n    # normalise the training set\n    train_merged[column] = (train_merged[column] - mins[column]) / (maxs[column] - mins[column])\n\n# 6. Build full sequences\nX_train_seq, y_train_seq = build_sequences(train_merged, train_merged[[\"sample_index\", \"label\"]], window=W_SUGG, stride=S_SUGG)\n\n\n# 7. DataLoader\ntrain_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\ntrain_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n\n# 8. Initialize model with tuned hyperparameters\nmodel = RecurrentClassifier(\n    input_size=X_train_seq.shape[2],\n    hidden_size=HIDDEN_SIZE,\n    num_layers=HIDDEN_LAYERS,\n    num_classes=len(label_mapping),\n    rec_dropout_rate = RECURRENT_DROPOUT,\n    dropout_rate=DROPOUT_RATE,\n    bidirectional=BIDIRECTIONAL,\n    cnn_channels=CNN_CHANNELS,                  # enables CNN1D preprocessing\n    cnn_kernel_size=CNN_KERNEL_SIZE,                # usually 3, 5, or 7 works well\n    cnn_dropout=CNN_DROPOUT,                  # (optional; default uses RNN dropout)\n    rnn_type=RNN_TYPE\n).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\nscaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n\n# 9. Train model on the entire dataset\nmodel, history = fit(\n    model=model,\n    train_loader=train_loader,\n    val_loader=None,\n    epochs=final_epochs,\n    train_criterion=train_criterion,\n    val_criterion=None,\n    optimizer=optimizer,\n    scaler=scaler,\n    device=device,\n    patience=PATIENCE,\n    verbose=True,\n    evaluation_metric=\"val_f1\",  # ignored since no validation\n    mode=\"max\",\n    restore_best_weights=False,\n    experiment_name=\"final_full_train\"\n)\n\n# 10. Prepare test set for inference\nX_test = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\nDF_test, _ = preprocess_joints(X_test.copy())\nX_test, _ = dataset_conversion_type_embed_ready(DF_test)\n\nfor column in scale_columns:\n    # normalise the test set\n    X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])\n\n# Build windowed sequences (this creates N_windows, not N_sequences)\nX_test_seq, _ = build_sequences(X_test, None, window=W_SUGG, stride=S_SUGG)\n\n# 11. Save predictions and configuration\nOUT_DIR = \"results_FULL_model\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nhyperparams = {\n    \"m\": MODEL,\n    \"lr\": LEARNING_RATE,\n    \"epochs\": EPOCHS,\n    \"pat\": PATIENCE,\n    \"hl\": HIDDEN_LAYERS,\n    \"hs\": HIDDEN_SIZE,\n    \"dr\": DROPOUT_RATE,\n    \"l1\": L1_LAMBDA,\n    \"l2\": L2_LAMBDA,\n    'bi': BIDIRECTIONAL,\n    'w': W_SUGG,\n    's': S_SUGG\n}\n\n# MODIFIED: Pass window_size and stride for aggregation\nsubmission = save_experiment_output(\n    model_name=RNN_TYPE.lower(),\n    hyperparams=hyperparams,\n    X_test_seq=X_test_seq,  # Windowed data (N_windows, W, F)\n    label_mapping=label_mapping,\n    output_dir=OUT_DIR,\n    sample_indices=X_test[\"sample_index\"].unique(),\n    model=model,\n    window_size=W_SUGG,  # NEW: pass for aggregation\n    stride=S_SUGG  # NEW: pass for aggregation\n)\n\nprint(\"\\nâœ… Final model trained and submission saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"erro.toBreak()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **K-Shuffle-Split Cross Validation**","metadata":{}},{"cell_type":"code","source":"def k_shuffle_split_cross_validation_round_rnn(df, y, epochs, device, k, batch_size, window, stride,\n                                               hidden_layers, hidden_size, learning_rate, rec_dropout_rate, \n                                               dropout_rate, rnn_type, bidirectional, cnn_channels, cnn_kernel_size, cnn_dropout,\n                            l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n                            restore_best_weights=True, writer=None, verbose=10, seed=SEED, experiment_name=\"\"):\n    \"\"\"\n    Perform K-fold shuffle split cross-validation with user-based splitting for time series data.\n\n    Args:\n        df: DataFrame with columns\n        epochs: Number of training epochs\n        device: torch.device for computation\n        k: Number of cross-validation splits\n        n_val_idxs: Number of indexes for validation set\n        batch_size: Batch size for training\n        hidden_layers: Number of recurrent layers\n        hidden_size: Hidden state dimensionality\n        learning_rate: Learning rate for optimizer\n        dropout_rate: Dropout rate\n        rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n        bidirectional: Whether to use bidirectional RNN\n        l1_lambda: L1 regularization coefficient (if used)\n        l2_lambda: L2 regularization coefficient (weight_decay)\n        patience: Early stopping patience\n        evaluation_metric: Metric to monitor for early stopping\n        mode: 'max' or 'min' for evaluation metric\n        restore_best_weights: Whether to restore best weights after training\n        writer: TensorBoard writer\n        verbose: Verbosity level\n        seed: Random seed\n        experiment_name: Name for experiment logging\n\n    Returns:\n        fold_losses: Dict with validation losses for each split\n        fold_metrics: Dict with validation F1 scores for each split\n        best_scores: Dict with best F1 score for each split plus mean and std\n    \"\"\"\n\n    # Initialise containers for results across all splits\n    fold_losses = {}\n    fold_metrics = {}\n    best_scores = {}\n    best_epochs_per_fold = {}\n\n    DF, _ = preprocess_joints(X_TRAIN.copy())\n    X_train, _  = dataset_conversion_type_embed_ready(DF)\n    y = Y_TRAIN.copy()\n\n    # Step 1. temporary merge X_train + y_train to create splits ---\n    train_merged = X_train.merge(y, on=\"sample_index\", how=\"left\")\n\n    # Step 2. Retrieve unique indexes ---\n    unique_samples = train_merged['sample_index'].unique()\n\n    num_classes = len(train_merged['label'].unique())\n\n    # Prepare stratified K-fold based on label per sample_index\n    # ---------------------------------------------------------------\n    # Extract one label per sample_index\n    label_per_sample = train_merged.groupby(\"sample_index\")[\"label\"].first().map({\n        \"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2\n    }).values\n\n    # Create stratified splitter\n    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n    all_splits = list(skf.split(unique_samples, label_per_sample))\n    # ---------------------------------------------------------------\n\n    # Store initial weights to reset model for each split\n    initial_state = None\n\n    # Iterate through K random splits\n    for split_idx, (train_idx, val_idx) in enumerate(all_splits):\n\n        if verbose > 0:\n            print(f\"Split {split_idx+1}/{k}\")\n\n        # stratified split indices\n        train_idxs = unique_samples[train_idx]\n        val_idxs   = unique_samples[val_idx]\n\n        # Split the dataset into training, validation, and test sets based on user IDs\n        df_train = train_merged[train_merged['sample_index'].isin(train_idxs)].copy()\n        df_val = train_merged[train_merged['sample_index'].isin(val_idxs)].copy()\n\n        # X: only features\n        X_train = df_train.drop(columns=['label'])\n        X_val   = df_val.drop(columns=['label'])\n\n        # y: un'etichetta per ogni sequenza\n        y_train = df_train.groupby(\"sample_index\")[\"label\"].first().values\n        y_val   = df_val.groupby(\"sample_index\")[\"label\"].first().values\n\n        # Define mapping once\n        label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n        inv_label_mapping = {v: k for k, v in label_mapping.items()}\n\n        # Convert y_train/y_val from string â†’ int\n        y_train = np.array([label_mapping[l] for l in y_train])\n        y_val   = np.array([label_mapping[l] for l in y_val])\n\n        # ============= COMPUTE CLASS WEIGHTS FROM TRAINING FOLD =============\n        # Calculate class distribution for this fold's training set\n        num_classes_fold = np.max(y_train) + 1\n        class_counts_fold = np.bincount(y_train, minlength=num_classes_fold)\n        \n        if verbose > 0:\n            total_train = len(y_train)\n            print(f\"  Training fold class distribution:\")\n            for i, name in enumerate(['no_pain', 'low_pain', 'high_pain']):\n                print(f\"    {name} ({i}): {class_counts_fold[i]} ({class_counts_fold[i]/total_train*100:.1f}%)\")\n        \n        # Compute class weights using the 'maximum count' rule\n        max_count_fold = class_counts_fold.max()\n        class_weights_fold = max_count_fold / class_counts_fold\n        \n        # Use SQRT dampening for weights (more balanced)\n        class_weights_fold_dampened = np.sqrt(class_weights_fold)\n        class_weights = torch.tensor(class_weights_fold_dampened, dtype=torch.float32)\n\n        if verbose > 0:\n            print(f\"  Class weights (raw): {class_weights_fold}\")\n            print(f\"  Class weights (sqrt dampened): {class_weights_fold_dampened}\")\n        \n        # Define training criterion with FocalLabelSmoothing (NEW)\n        train_criterion = WeightLabelLoss(\n            num_classes=num_classes_fold,\n            class_weights=class_weights_fold_dampened,  # Use sqrt dampened weights\n        ).to(device)\n        \n\n        # ============= END CLASS WEIGHTS COMPUTATION =============\n\n        # Normalise features using training set statistics\n        scale_columns = [col for col in X_train.columns if col.startswith(\"joint_\")]\n\n        train_max = X_train[scale_columns].max()\n        train_min = X_train[scale_columns].min()\n\n        X_train[scale_columns] = (X_train[scale_columns] - train_min) / (train_max - train_min + 1e-8)\n        X_val[scale_columns] = (X_val[scale_columns] - train_min) / (train_max - train_min + 1e-8)\n\n        if verbose > 0:\n            print(f\"  Training set shape: {X_train.shape}\")\n            print(f\"  Validation set shape: {X_val.shape}\")\n\n        y_train_df = pd.DataFrame({\n            \"sample_index\": X_train[\"sample_index\"].unique(),\n            \"label\": y_train\n        })\n\n        X_train_seq, y_train_seq = build_sequences(X_train, y_train_df, window, stride)\n\n        y_val_df = pd.DataFrame({\n            \"sample_index\": X_val[\"sample_index\"].unique(),\n            \"label\": y_val\n        })\n\n        X_val_seq, y_val_seq = build_sequences(X_val, y_val_df, window, stride)\n\n        if verbose > 0:\n            print(f\"  Training sequences shape: {X_train_seq.shape}\")\n            print(f\"  Validation sequences shape: {X_val_seq.shape}\")\n\n        input_shape = X_train_seq.shape[1:] # extract the shape of a single sequence\n        num_classes = len(np.unique(y_train)) # how many unique pain level exists\n\n        if verbose > 0:\n            print(f\"  Input shape: {input_shape}\")\n            print(f\"  Num classes: {num_classes}\")\n\n        # Create PyTorch datasets\n        train_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\n        val_ds   = TensorDataset(torch.from_numpy(X_val_seq), torch.from_numpy(y_val_seq))\n\n        # Create data loaders\n        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n        val_loader   = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n\n        # Initialise model architecture\n        model = RecurrentClassifier(\n            input_size=input_shape[-1],\n            hidden_size=hidden_size,\n            num_layers=hidden_layers,\n            num_classes=num_classes,\n            dropout_rate=dropout_rate,\n            rec_dropout_rate=rec_dropout_rate,\n            bidirectional=bidirectional,\n            cnn_channels=cnn_channels,                  # enables CNN1D preprocessing\n            cnn_kernel_size=cnn_kernel_size,                # usually 3, 5, or 7 works well\n            cnn_dropout=cnn_dropout,                  # (optional; default uses RNN dropout)\n            rnn_type=rnn_type\n        ).to(device)\n\n        # 3. save initial state at 1st split, reset in the following splits\n        if initial_state is None:\n            # the first split (split_idx == 0)\n            # save initial random weights\n            initial_state = copy.deepcopy(model.state_dict())\n        else:\n            # Questo Ã¨ uno split successivo (1, 2, ...)\n            # Resetta il modello ai pesi iniziali salvati\n            model.load_state_dict(initial_state)\n        \n        # Define optimizer with L2 regularization\n        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n\n        # Enable mixed precision training for GPU acceleration\n        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n\n        # scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=7, verbose=True, min_lr=1e-6)\n        scheduler = CosineAnnealingWarmRestarts(\n            optimizer, \n            T_0=15,        # Restart every 15 epochs\n            T_mult=2,      # Double restart period each time\n            eta_min=1e-6   # Minimum LR\n        )\n        # Create directory for model checkpoints\n        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n\n        # Validation criterion (standard CrossEntropy without smoothing)\n        val_criterion = nn.CrossEntropyLoss()to(device)\n\n        # Train model on current split\n        model, training_history = fit(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            epochs=epochs,\n            train_criterion=train_criterion,  # Use fold-specific FocalLabelSmoothing\n            val_criterion=val_criterion,\n            optimizer=optimizer,\n            scaler=split_scaler,\n            scheduler=scheduler,\n            device=device,\n            writer=writer,\n            patience=patience,\n            verbose=verbose,\n            evaluation_metric=evaluation_metric,\n            mode=mode,\n            restore_best_weights=restore_best_weights,\n            experiment_name=experiment_name+\"/split_\"+str(split_idx)\n        )\n\n        # Store results for this split\n        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n        fold_metrics[f\"split_{split_idx}\"] = training_history['val_f1']\n        best_scores[f\"split_{split_idx}\"] = max(training_history['val_f1'])\n\n        best_epoch_idx = training_history['val_f1'].index(max(training_history['val_f1']))\n        best_epochs_per_fold[f\"split_{split_idx}\"] = best_epoch_idx\n\n\n    # Compute mean and standard deviation of best scores across splits\n    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n    best_scores[\"mean_best_epoch\"] = np.mean([\n        best_epochs_per_fold[k] for k in best_epochs_per_fold.keys() if k.startswith(\"split_\")\n    ])\n\n    if verbose > 0:\n        print(f\"Best score: {best_scores['mean']:.4f}Â±{best_scores['std']:.4f}\")\n\n    return fold_losses, fold_metrics, best_scores\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Hyperparameters Tuning**","metadata":{}},{"cell_type":"code","source":"import pickle\n\ndef grid_search_cv_rnn(df, y, param_grid, fixed_params, cv_params, verbose=True, n_iter=60, \n                       checkpoint_every=10, early_prune_threshold=0.8):\n    \"\"\"\n    Execute grid search with K-shuffle-split cross-validation for RNN models on time series data.\n\n    Args:\n        df: DataFrame with columns \n        param_grid: Dict of parameters to test, e.g. {'batch_size': [16, 32], 'rnn_type': ['LSTM', 'GRU']}\n        fixed_params: Dict of fixed hyperparameters (hidden_size, learning_rate, window_size, stride, etc.)\n        cv_params: Dict of CV settings (epochs, k, patience, criterion, scaler, device, etc.)\n        verbose: Print progress for each configuration\n        checkpoint_every: Save results every N iterations (default 10)\n        early_prune_threshold: Stop config if score < best_score * threshold after 2 folds (default 0.7)\n\n    Returns:\n        results: Dict with scores for each configuration\n        best_config: Dict with best hyperparameter combination\n        best_score: Best mean F1 score achieved\n        best_config_epochs: Mean best epoch from best configuration\n    \"\"\"\n    # Generate all parameter combinations\n    param_names = list(param_grid.keys())\n    param_values = list(param_grid.values())\n    all_combinations = list(product(*param_values))\n\n    total_possible = len(all_combinations)\n\n    \n    results = {}\n    best_score = -np.inf\n    best_config = None\n    best_config_epochs = None\n\n    # If n_iter is None, set it to total_possible to run a full grid search\n    if n_iter is None:\n        n_iter = total_possible\n\n    # Se n_iter Ã¨ minore del totale, scegli N combinazioni a caso\n    if n_iter < total_possible:\n        print(f\"--- Eseguendo RANDOM SEARCH ---\")\n        print(f\"Selezionate {n_iter} combinazioni casuali su {total_possible} possibili.\")\n        # Use seed for reproducibility\n        random.seed(cv_params.get('seed', 42))\n        combinations = random.sample(all_combinations, n_iter)\n    else:\n        print(f\"--- Eseguendo GRID SEARCH ---\")\n        print(f\"Testando tutte le {total_possible} combinazioni.\")\n        combinations = all_combinations\n\n    for idx, combo in enumerate(combinations, 1):\n        # Create current configuration dict\n        current_config = dict(zip(param_names, combo))\n        config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n\n        if verbose:\n            if n_iter < total_possible:\n                print(f\"\\nConfiguration {idx}/{n_iter}:\")\n            else:\n                 print(f\"\\nConfiguration {idx}/{total_possible}:\")\n            for param, value in current_config.items():\n                print(f\"  {param}: {value}\")\n\n        # Merge current config with fixed parameters\n        run_params = {**fixed_params, **current_config}\n\n        # Execute cross-validation \n        _, _, fold_scores = k_shuffle_split_cross_validation_round_rnn(\n            df=df,\n            y=y,\n            experiment_name=config_str,\n            **run_params,\n            **cv_params\n        )\n\n        # Early pruning: skip config if performance is too poor after initial folds\n        if best_score > -np.inf:  # Only prune after we have a baseline\n            partial_scores = fold_scores.get('scores', [])\n            if len(partial_scores) >= 2:\n                partial_mean = np.mean(partial_scores[:2])\n                if partial_mean < best_score * early_prune_threshold:\n                    if verbose:\n                        print(f\"  [PRUNED] Score {partial_mean:.4f} < {best_score * early_prune_threshold:.4f} (threshold), skipping.\")\n                    results[config_str] = fold_scores  # Still save for analysis\n                    continue\n\n        # Store results\n        results[config_str] = fold_scores\n\n        # Track best configuration\n        if fold_scores[\"mean\"] > best_score:\n            best_score = fold_scores[\"mean\"]\n            best_config = current_config.copy()\n            best_config_epochs = fold_scores[\"mean_best_epoch\"]\n            if verbose:\n                print(\"  NEW BEST SCORE!\")\n\n        if verbose:\n            print(f\"  F1 Score: {fold_scores['mean']:.4f}Â±{fold_scores['std']:.4f}\")\n\n        # Save checkpoint periodically\n        if idx % checkpoint_every == 0:\n            checkpoint_path = f'grid_search_checkpoint_{idx}.pkl'\n            with open(checkpoint_path, 'wb') as f:\n                pickle.dump({\n                    'results': results,\n                    'best_config': best_config,\n                    'best_score': best_score,\n                    'best_config_epochs': best_config_epochs,\n                    'completed_idx': idx,\n                    'total': n_iter if n_iter < total_possible else total_possible\n                }, f)\n            if verbose:\n                print(f\"  [CHECKPOINT] Salvato in {checkpoint_path}\")\n\n    # Final save\n    final_path = 'grid_search_final.pkl'\n    with open(final_path, 'wb') as f:\n        pickle.dump({\n            'results': results,\n            'best_config': best_config,\n            'best_score': best_score,\n            'best_config_epochs': best_config_epochs\n        }, f)\n    print(f\"\\n[COMPLETATO] Risultati salvati in {final_path}\")\n\n    return results, best_config, best_score, best_config_epochs\n\n\ndef plot_top_configurations_rnn(results, k_splits, top_n=5, figsize=(14, 7)):\n    \"\"\"\n    Visualise top N RNN configurations with boxplots of F1 scores across CV splits.\n\n    Args:\n        results: Dict of results from grid_search_cv_rnn\n        k_splits: Number of CV splits used\n        top_n: Number of top configurations to display\n        figsize: Figure size tuple\n    \"\"\"\n    # Sort by mean score\n    config_scores = {name: data['mean'] for name, data in results.items()}\n    sorted_configs = sorted(config_scores.items(), key=lambda x: x[1], reverse=True)\n\n    # Select top N\n    top_configs = sorted_configs[:min(top_n, len(sorted_configs))]\n\n    # Prepare boxplot data\n    boxplot_data = []\n    labels = []\n\n    # Define a dictionary for replacements, ordered to handle prefixes correctly\n    replacements = {\n        'window_':'W=',\n        'stride_':'S=',\n        'batch_size_': 'BS=',\n        'learning_rate_': '\\nLR=',\n        'hidden_layers_': '\\nHL=',\n        'hidden_size_': '\\nHS=',\n        'dropout_rate_': '\\nDR=',\n        'rnn_type_': '\\nRNN=',\n        'bidirectional_': '\\nBIDIR=',\n        'l1_lambda_': '\\nL1=',\n        'l2_lambda_': '\\nL2='\n    }\n\n    # Replacements for separators\n    separator_replacements = {\n        '_window_':'\\nW=',\n        '_stride_':'\\nS=',\n        '_learning_rate_': '\\nLR=',\n        '_hidden_layers_': '\\nHL=',\n        '_hidden_size_': '\\nHS=',\n        '_dropout_rate_': '\\nDR=',\n        '_rnn_type_': '\\nRNN=',\n        '_bidirectional_': '\\nBIDIR=',\n        '_l1_lambda_': '\\nL1=',\n        '_l2_lambda_': '\\nL2=',\n        '_': ''\n    }\n\n    for config_name, mean_score in top_configs:\n        # Extract best score from each split (auto-detect number of splits)\n        split_scores = []\n        for i in range(k_splits):\n            if f'split_{i}' in results[config_name]:\n                split_scores.append(results[config_name][f'split_{i}'])\n        boxplot_data.append(split_scores)\n\n        # Verify we have the expected number of splits\n        if len(split_scores) != k_splits:\n            print(f\"Warning: Config {config_name} has {len(split_scores)} splits, expected {k_splits}\")\n\n        # Create readable label using the replacements dictionary\n        readable_label = config_name\n        for old, new in replacements.items():\n            readable_label = readable_label.replace(old, new)\n\n        # Apply separator replacements\n        for old, new in separator_replacements.items():\n             readable_label = readable_label.replace(old, new)\n\n        labels.append(f\"{readable_label}\\n(Î¼={mean_score:.3f})\")\n\n    # Create plot\n    fig, ax = plt.subplots(figsize=figsize)\n    bp = ax.boxplot(boxplot_data, labels=labels, patch_artist=True,\n                    showmeans=True, meanline=True)\n\n    # Styling\n    for patch in bp['boxes']:\n        patch.set_facecolor('lightblue')\n        patch.set_alpha(0.7)\n\n    # Highlight best configuration\n    ax.get_xticklabels()[0].set_fontweight('bold')\n\n    ax.set_ylabel('F1 Score')\n    ax.set_xlabel('Configuration')\n    ax.set_title(f'Top {len(top_configs)} RNN Configurations - F1 Score Distribution Across {k_splits} Splits')\n    ax.grid(alpha=0.3, axis='y')\n\n    plt.xticks(rotation=0, ha='center')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DF, _ = preprocess_joints(X_TRAIN.copy())\nX_train, _ = dataset_conversion_type_embed_ready(DF)\ny = Y_TRAIN.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"K = 5\nVAL_FRAC = 0.20  # 20% sequences for validation in each split\nN = X_train['sample_index'].nunique()  # Returns integer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\n# Define parameters to search\nparam_grid_phase1 = {\n    'batch_size': [64, 128, 256],\n    'learning_rate': [3e-4, 5e-4, 7e-4],\n    \n    # Fixed to your proven baseline\n    'hidden_size': [192],\n    'hidden_layers': [2],\n    'dropout_rate': [0.5],\n    'l2_lambda': [1e-4],\n    'window': [12],\n    'stride': [3],\n    'rnn_type': ['GRU'],\n    'bidirectional': [True],\n    'epochs': [150],\n    'patience': [10],\n}\n\n\n# Fixed hyperparameters (not being tuned)\nfixed_params = {\n    'l1_lambda': L1_LAMBDA,\n    'cnn_channels':CNN_CHANNELS,                  # enables CNN1D preprocessing\n    'cnn_kernel_size'=:CNN_KERNEL_SIZE,                # usually 3, 5, or 7 works well\n    'cnn_dropout': CNN_DROPOUT,                  # (optional; default uses RNN dropout)\n}\n\n\n# Cross-validation settings\ncv_params = {\n    'device': device,\n    'k': K,\n    'verbose': 0,\n    'seed': SEED\n}\n\n# Run aggressive random search\n# Total possible: 3Ã—3Ã—3Ã—3Ã—3Ã—4Ã—2Ã—2 = 3,888 combinations\n# Testing: 60 random samples (~1.5% of total)\nresults, best_config, best_score, best_epochs = grid_search_cv_rnn(\n    df=df_train,\n    y=y_train,\n    param_grid=param_grid_phase1,\n    fixed_params=fixed_params,\n    cv_params=cv_params,\n    verbose=True,\n    n_iter=None,  # Sample 60 random configurations\n    checkpoint_every=10,  # Save every 10 iterations\n    early_prune_threshold=0.70  # Skip if score < 70% of best\n)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"BEST CONFIGURATION:\")\nfor k, v in best_config.items():\n    print(f\"  {k}: {v}\")\nprint(f\"Best F1 Score: {best_score:.4f}\")\nprint(f\"Best Epochs: {best_epochs:.1f}\")\nprint(f\"{'='*60}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualise results\nplot_top_configurations_rnn(results, k_splits=K, top_n=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nparam_grid_phase2 = {\n    'batch_size': [best_config['batch_size']],\n    'learning_rate': [best_config['learning_rate']],\n    \n    # Now explore architecture\n    'hidden_size': [128, 192, 256],\n    'hidden_layers': [2, 3],\n    'dropout_rate': [0.4, 0.5],\n    \n    # Fixed\n    'l2_lambda': [1e-4],\n    'window': [12],\n    'stride': [3],\n    'rnn_type': ['GRU'],\n    'bidirectional': [True],\n    'epochs': [150],\n    'patience': [10],\n}\n\n# Fixed hyperparameters (not being tuned)\nfixed_params = {\n    'l1_lambda': L1_LAMBDA,\n    'dropout_rate': RECURRENT_DROPOUT\n}\n\n# Cross-validation settings\ncv_params = {\n    'device': device,\n    'k': K,\n    'verbose': 0,\n    'seed': SEED\n}\n\n# Run aggressive random search\n# Total possible: 3Ã—3Ã—3Ã—3Ã—3Ã—4Ã—2Ã—2 = 3,888 combinations\n# Testing: 60 random samples (~1.5% of total)\nresults, best_config, best_score, best_epochs = grid_search_cv_rnn(\n    df=df_train,\n    y=y_train,\n    param_grid=param_grid_phase2,\n    fixed_params=fixed_params,\n    cv_params=cv_params,\n    verbose=True,\n    n_iter=None,  # Sample 60 random configurations\n    checkpoint_every=10,  # Save every 10 iterations\n    early_prune_threshold=0.70  # Skip if score < 70% of best\n)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"BEST CONFIGURATION:\")\nfor k, v in best_config.items():\n    print(f\"  {k}: {v}\")\nprint(f\"Best F1 Score: {best_score:.4f}\")\nprint(f\"Best Epochs: {best_epochs:.1f}\")\nprint(f\"{'='*60}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualise results\nplot_top_configurations_rnn(results, k_splits=K, top_n=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nparam_grid_phase3 = {\n    'batch_size': [best_config['batch_size']],\n    'learning_rate': [best_config['learning_rate']],\n    'hidden_size': [best_config['hidden_size']],\n    'hidden_layers': [best_config['hidden_layers']],\n    'dropout_rate': [best_config['dropout_rate']],\n    \n    # Now explore window/stride/L2\n    'window': [12, 16, 20],\n    'stride': [2, 3, 4],\n    'l2_lambda': [3e-5, 1e-4, 3e-4],\n    \n    'rnn_type': ['GRU'],\n    'bidirectional': [True],\n    'epochs': [150],\n    'patience': [10],\n}\n\n# Fixed hyperparameters (not being tuned)\nfixed_params = {\n    'l1_lambda': L1_LAMBDA,\n}\n\n# Cross-validation settings\ncv_params = {\n    'device': device,\n    'k': K,\n    'verbose': 0,\n    'seed': SEED\n}\n\n# Run aggressive random search\n# Total possible: 3Ã—3Ã—3Ã—3Ã—3Ã—4Ã—2Ã—2 = 3,888 combinations\n# Testing: 60 random samples (~1.5% of total)\nresults, best_config, best_score, best_epochs = grid_search_cv_rnn(\n    df=df_train,\n    y=y_train,\n    param_grid=param_grid_phase3,\n    fixed_params=fixed_params,\n    cv_params=cv_params,\n    verbose=True,\n    n_iter=13,  # Sample 60 random configurations\n    checkpoint_every=10,  # Save every 10 iterations\n    early_prune_threshold=0.70  # Skip if score < 70% of best\n)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"BEST CONFIGURATION:\")\nfor k, v in best_config.items():\n    print(f\"  {k}: {v}\")\nprint(f\"Best F1 Score: {best_score:.4f}\")\nprint(f\"Best Epochs: {best_epochs:.1f}\")\nprint(f\"{'='*60}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualise results\nplot_top_configurations_rnn(results, k_splits=K, top_n=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# FINAL TRAINING ON THE FULL DATASET + SUBMISSION GENERATION\n# ============================================================\n\n# 1. Preprocess full training data\nDF, _ = preprocess_joints(X_TRAIN.copy())\nX_train_full, _ = dataset_conversion_type_embed_ready(DF)\ny_full = Y_TRAIN.copy()\n\n\nlabels_full = y_full[\"label\"].map(label_mapping)\ny_full_np = labels_full.to_numpy()\n\nnum_classes = np.max(y_full_np) + 1  # For 0-indexed labels: [0, 1, 2] â†’ num_classes=3\nclass_counts = np.bincount(y_full_np, minlength=num_classes)\ntotal = len(y_full_np)\n\nprint(\"\\nTraining class distribution:\")\nfor i, name in enumerate(['no_pain', 'low_pain', 'high_pain']):\n    print(f\"  {name} ({i}): {class_counts[i]} ({class_counts[i]/total*100:.1f}%)\")\n\n# Compute class weights using the 'maximum count' rule (makes the most common class weight=1.0)\nmax_count = class_counts.max()\nclass_weights = max_count / class_counts\nprint(f\"Class weights: {class_weights}\")\n\n# For use in PyTorch or your label smoothing class:\nclass_weights_dampened = np.sqrt(class_weights)\nclass_weights = torch.tensor(class_weights_dampened, dtype=torch.float32)\n\n# Define training criterion with class-aware label smoothing\ntrain_criterion = WeightLabelLoss(\n    num_classes=3,\n    class_weights=class_weights              # Use automatic weights\n).to(device)\n\n\n# 2. Combine the best hyperparameters (found in grid search)\nfinal_best_params = {**fixed_params, **best_config}\n\n# Use mean best epoch from CV + 20% buffer (more data â†’ slightly more epochs)\nfinal_epochs = int(best_epochs * 1)\nprint(f\"\\nðŸ“Š CV found best performance at epoch {best_epochs:.1f} (average)\")\nprint(f\"   Final training will use {final_epochs} epochs (20% buffer for full dataset)\")\n\nfinal_best_params['epochs'] = final_epochs\n\n\nprint(\"Training final model with best configuration:\")\nfor k, v in final_best_params.items():\n    print(f\"  {k}: {v}\")\n\n# 3. Merge features and labels\ntrain_merged = X_train_full.merge(y_full, on=\"sample_index\")\n\n# 4. Encode labels numerically BEFORE building sequences\nlabel_mapping = {\"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2}\ntrain_merged[\"label\"] = train_merged[\"label\"].map(label_mapping)\n\n\n\n# 5. Normalise feature values\n\nscale_columns = [col for col in train_merged.columns if col.startswith(\"joint_\")]\n# calculate the minimum and maximum values from the training data only\nmins = X_train[scale_columns].min()\nmaxs = X_train[scale_columns].max()\n\n# apply normalisation to the specified columns in all datasets (training and validation)\nfor column in scale_columns:\n\n    # normalise the training set\n    train_merged[column] = (train_merged[column] - mins[column]) / (maxs[column] - mins[column])\n\n# 6. Build full sequences\nX_train_seq, y_train_seq = build_sequences(train_merged, train_merged[[\"sample_index\", \"label\"]], window=final_best_params[\"window\"], stride=final_best_params[\"stride\"])\n\n\n# 7. DataLoader\ntrain_ds = TensorDataset(torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq))\ntrain_loader = make_loader(train_ds, batch_size=final_best_params[\"batch_size\"], shuffle=True, drop_last=False)\n\n# 8. Initialize model with tuned hyperparameters\nmodel = RecurrentClassifier(\n    input_size=X_train_seq.shape[2],\n    hidden_size=final_best_params[\"hidden_size\"],\n    num_layers=final_best_params[\"hidden_layers\"],\n    num_classes=len(label_mapping),\n    rec_dropout_rate = RECURRENT_DROPOUT,\n    dropout_rate=final_best_params[\"dropout_rate\"],\n    bidirectional=final_best_params[\"bidirectional\"],\n    rnn_type=final_best_params[\"rnn_type\"],\n    cnn_channels=final_best_params[\"cnn_channels\"],                  # enables CNN1D preprocessing\n    cnn_kernel_size=final_best_params[\"cnn_kernel_size\"],                # usually 3, 5, or 7 works well\n    cnn_dropout=final_best_params[\"cnn_dropout\"],                  # (optional; default uses RNN dropout)\n).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=final_best_params[\"learning_rate\"], weight_decay=final_best_params['l2_lambda'])\nscaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n\n# 9. Train model on the entire dataset\nmodel, history = fit(\n    model=model,\n    train_loader=train_loader,\n    val_loader=None,\n    epochs=final_epochs,\n    train_criterion=train_criterion,\n    val_criterion=None,\n    optimizer=optimizer,\n    scaler=scaler,\n    device=device,\n    patience=final_best_params[\"patience\"],\n    verbose=True,\n    evaluation_metric=\"val_f1\",  # ignored since no validation\n    mode=\"max\",\n    restore_best_weights=False,\n    experiment_name=\"final_full_train\"\n)\n\n# 10. Prepare test set for inference\nX_test = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\nDF_test, _ = preprocess_joints(X_test.copy())\nX_test, _ = dataset_conversion_type_embed_ready(DF_test)\n\nfor column in scale_columns:\n    # normalise the test set\n    X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])\n\n# Build windowed sequences (this creates N_windows, not N_sequences)\nX_test_seq, _ = build_sequences(X_test, None, window=final_best_params[\"window\"], stride=final_best_params[\"stride\"])\n\n# 11. Save predictions and configuration\nOUT_DIR = \"results_best_model\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nhyperparams = final_best_params.copy()\nhyperparams.update({\n    \"best_cv_f1\": best_score\n})\n\n# MODIFIED: Pass window_size and stride for aggregation\nsubmission = save_experiment_output(\n    model_name=final_best_params[\"rnn_type\"].lower(),\n    hyperparams=hyperparams,\n    X_test_seq=X_test_seq,  # Windowed data (N_windows, W, F)\n    label_mapping=label_mapping,\n    output_dir=OUT_DIR,\n    sample_indices=X_test[\"sample_index\"].unique(),\n    model=model,\n    window_size=final_best_params[\"window\"],  # NEW: pass for aggregation\n    stride=final_best_params[\"stride\"]  # NEW: pass for aggregation\n)\n\nprint(\"\\nâœ… Final model trained and submission saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}