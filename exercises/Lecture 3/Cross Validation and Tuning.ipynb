{"cells":[{"cell_type":"markdown","metadata":{"id":"X5i0sMFDr5jO"},"source":["# **Artificial Neural Networks and Deep Learning**\n","\n","---\n","\n","## **Lecture 3: Cross Validation and Tuning**\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=13uEsx8hOr-fJQ7EtjQgDynZ8QvHuTTBb\" width=\"500\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"omSLbdLvhDRx"},"source":["## üåê **Google Drive Connection**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AoaLQpvChLpb"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/gdrive\")\n","current_dir = \"/gdrive/My\\\\ Drive/[2025-2026]\\\\ AN2DL/Lecture\\\\ 3\"\n","%cd $current_dir"]},{"cell_type":"markdown","metadata":{"id":"l3FoTyRa9pLu"},"source":["## ‚öôÔ∏è **Libraries Import**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_sOaV1Y8NsL"},"outputs":[],"source":["# Set seed for reproducibility\n","SEED = 42\n","\n","# Import necessary libraries\n","import os\n","\n","# Set environment variables before importing modules\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n","\n","# Suppress warnings\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","# Import necessary modules\n","import logging\n","import random\n","import numpy as np\n","\n","# Set seeds for random number generators in NumPy and Python\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","# Import PyTorch\n","import torch\n","torch.manual_seed(SEED)\n","from torch import nn\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import TensorDataset, DataLoader\n","logs_dir = \"tensorboard\"\n","!pkill -f tensorboard\n","%load_ext tensorboard\n","!mkdir -p models\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.benchmark = True\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Device: {device}\")\n","\n","# Import other libraries\n","import copy\n","import shutil\n","from itertools import product\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","\n","# Configure plot display settings\n","sns.set(font_scale=1.4)\n","sns.set_style('white')\n","plt.rc('font', size=14)\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"TCaGhb5ttXGc"},"source":["## ‚è≥ **Data Loading**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxEwCjfpt_6W"},"outputs":[],"source":["# Load the Glass dataset from CSV file\n","os.environ[\"DATASET_NAME\"] = \"glass_dataset.csv\"\n","os.environ[\"DATASET_URL\"] = \"1xyZvjIw2nR5QtlfN9vumuPUjr3SFZLob\"\n","if not os.path.exists(os.environ[\"DATASET_NAME\"]):\n","    print(\"Downloading data...\")\n","    ! gdown -q ${DATASET_URL}\n","    print(\"Download completed\")\n","else:\n","    print(\"Data already downloaded. Using cached data...\")\n","data = pd.read_csv('glass_dataset.csv')"]},{"cell_type":"markdown","metadata":{"id":"kTu9Ca0L1Du2"},"source":["## üîé **Exploration and Data Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P485pmX1uvBf"},"outputs":[],"source":["# Display the first 10 rows of the Glass dataset\n","data.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBZgisH81IK9"},"outputs":[],"source":["# Print the shape of the Glass dataset\n","print('Glass dataset shape', data.shape)\n","\n","# Generate summary statistics for the Glass dataset\n","data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtUiSOuL1liZ"},"outputs":[],"source":["# Get the target values from the Glass dataset\n","target = data['Glass Class'].values\n","print('Target shape', target.shape)\n","\n","# Calculate the unique target labels and their counts\n","unique, count = np.unique(target, return_counts=True)\n","print('Target labels:', unique)\n","for i in range(len(unique)):\n","    print(f'Class {unique[i]} has {count[i]} samples')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_AV2Ce81lf1"},"outputs":[],"source":["# Plot pairwise relationships between features colored by glass class\n","sns.pairplot(\n","    data=data,\n","    hue='Glass Class',\n","    corner=True,        # Display only lower triangle for efficiency\n","    palette='tab10'     # Color palette suitable for 6 classes\n",")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xj6jQOLc33eb"},"source":["## üîÑ **Data Preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDG_wvir1lc8"},"outputs":[],"source":["# Prepare features and labels as float32 and int64 arrays\n","X = data.drop('Glass Class', axis=1).astype(np.float32).values\n","y = target.astype(np.int64)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxswApXm1lL3"},"outputs":[],"source":["def make_loader(ds, batch_size, shuffle, drop_last):\n","    # Determine optimal number of worker processes for data loading\n","    cpu_cores = os.cpu_count() or 2\n","    num_workers = max(2, min(4, cpu_cores))\n","\n","    # Create DataLoader with performance optimizations\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers,\n","        pin_memory=True,  # Faster GPU transfer\n","        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n","        prefetch_factor=4,  # Load 4 batches ahead\n","    )"]},{"cell_type":"markdown","metadata":{"id":"Yq6ADZrD4xrA"},"source":["## üõ†Ô∏è **Model Building**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJpcWRGb1lDU"},"outputs":[],"source":["# Define a simple feedforward neural network\n","class FeedForwardNet(nn.Module):\n","    def __init__(self, in_features, hidden_layers, hidden_size, dropout_rate, num_classes):\n","        super().__init__()\n","        modules = []\n","        # First layer\n","        modules.append(nn.Linear(in_features, hidden_size))\n","        if dropout_rate > 0 :\n","            modules.append(nn.Dropout(dropout_rate))\n","        modules.append(nn.ReLU())\n","\n","        # Additional hidden layers\n","        for _ in range(hidden_layers):\n","            modules.append(nn.Linear(hidden_size, hidden_size))\n","            if dropout_rate > 0 :\n","                modules.append(nn.Dropout(dropout_rate))\n","            modules.append(nn.ReLU())\n","\n","        # Output layer\n","        modules.append(nn.Linear(hidden_size, num_classes))\n","        self.net = nn.Sequential(*modules)\n","\n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"markdown","metadata":{"id":"_SeAVVtC47KT"},"source":["## üßÆ **Network Parameters**"]},{"cell_type":"markdown","metadata":{"id":"hb7P-CSm4-Jg"},"source":["## üß† **Model Training**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5yhvl7I1k6f","cellView":"form"},"outputs":[],"source":["# @title train_one_epoch()\n","def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n","    \"\"\"\n","    Perform one complete training epoch through the entire training dataset.\n","\n","    Args:\n","        model (nn.Module): The neural network model to train\n","        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n","        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n","        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n","        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","        l1_lambda (float): Lambda for L1 regularization\n","        l2_lambda (float): Lambda for L2 regularization\n","\n","    Returns:\n","        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n","    \"\"\"\n","    model.train()  # Set model to training mode\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_targets = []\n","\n","    # Iterate through training batches\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        # Move data to device (GPU/CPU)\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Clear gradients from previous step\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        # Forward pass with mixed precision (if CUDA available)\n","        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","            logits = model(inputs)\n","            loss = criterion(logits, targets)\n","\n","            # Add L1 and L2 regularization\n","            l1_norm = sum(p.abs().sum() for p in model.parameters())\n","            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n","            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n","\n","\n","        # Backward pass with gradient scaling\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # Accumulate metrics\n","        running_loss += loss.item() * inputs.size(0)\n","        predictions = logits.argmax(dim=1)\n","        all_predictions.append(predictions.cpu().numpy())\n","        all_targets.append(targets.cpu().numpy())\n","\n","    # Calculate epoch metrics\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    epoch_f1 = f1_score(\n","        np.concatenate(all_targets),\n","        np.concatenate(all_predictions),\n","        average='weighted'\n","    )\n","\n","    return epoch_loss, epoch_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kd-4dZt-1k3n","cellView":"form"},"outputs":[],"source":["# @title validate_one_epoch()\n","def validate_one_epoch(model, val_loader, criterion, device):\n","    \"\"\"\n","    Perform one complete validation epoch through the entire validation dataset.\n","\n","    Args:\n","        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n","        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n","        criterion (nn.Module): Loss function used to calculate validation loss\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","\n","    Returns:\n","        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n","\n","    Note:\n","        This function automatically sets the model to evaluation mode and disables\n","        gradient computation for efficiency during validation.\n","    \"\"\"\n","    model.eval()  # Set model to evaluation mode\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_targets = []\n","\n","    # Disable gradient computation for validation\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            # Move data to device\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            # Forward pass with mixed precision (if CUDA available)\n","            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","                logits = model(inputs)\n","                loss = criterion(logits, targets)\n","\n","            # Accumulate metrics\n","            running_loss += loss.item() * inputs.size(0)\n","            predictions = logits.argmax(dim=1)\n","            all_predictions.append(predictions.cpu().numpy())\n","            all_targets.append(targets.cpu().numpy())\n","\n","    # Calculate epoch metrics\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    epoch_accuracy = f1_score(\n","        np.concatenate(all_targets),\n","        np.concatenate(all_predictions),\n","        average='weighted'\n","    )\n","\n","    return epoch_loss, epoch_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTSWCMxi1k1H","cellView":"form"},"outputs":[],"source":["# @title log_metrics_to_tensorboard()\n","def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n","    \"\"\"\n","    Log training metrics and model parameters to TensorBoard for visualization.\n","\n","    Args:\n","        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n","        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n","        train_loss (float): Training loss for this epoch\n","        train_f1 (float): Training f1 score for this epoch\n","        val_loss (float): Validation loss for this epoch\n","        val_f1 (float): Validation f1 score for this epoch\n","        model (nn.Module): The neural network model (for logging weights/gradients)\n","\n","    Note:\n","        This function logs scalar metrics (loss/f1 score) and histograms of model\n","        parameters and gradients, which helps monitor training progress and detect\n","        issues like vanishing/exploding gradients.\n","    \"\"\"\n","    # Log scalar metrics\n","    writer.add_scalar('Loss/Training', train_loss, epoch)\n","    writer.add_scalar('Loss/Validation', val_loss, epoch)\n","    writer.add_scalar('F1/Training', train_f1, epoch)\n","    writer.add_scalar('F1/Validation', val_f1, epoch)\n","\n","    # Log model parameters and gradients\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            if param.numel() > 0:\n","                writer.add_histogram(f'{name}/weights', param.data, epoch)\n","            if param.grad is not None:\n","                if param.grad.numel() > 0:\n","                    writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"]},{"cell_type":"code","source":["# @title fit()\n","def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n","        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n","        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n","    \"\"\"\n","    Train the neural network model on the training data and validate on the validation data.\n","\n","    Args:\n","        model (nn.Module): The neural network model to train\n","        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n","        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n","        epochs (int): Number of training epochs\n","        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n","        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n","        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","        l1_lambda (float): L1 regularization coefficient (default: 0)\n","        l2_lambda (float): L2 regularization coefficient (default: 0)\n","        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n","        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n","        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n","        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n","        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n","        verbose (int, optional): Frequency of printing training progress (default: 10)\n","        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n","\n","    Returns:\n","        tuple: (model, training_history) - Trained model and metrics history\n","    \"\"\"\n","\n","    # Initialize metrics tracking\n","    training_history = {\n","        'train_loss': [], 'val_loss': [],\n","        'train_f1': [], 'val_f1': []\n","    }\n","\n","    # Configure early stopping if patience is set\n","    if patience > 0:\n","        patience_counter = 0\n","        best_metric = float('-inf') if mode == 'max' else float('inf')\n","        best_epoch = 0\n","\n","    # Main training loop: iterate through epochs\n","    for epoch in range(1, epochs + 1):\n","\n","        # Forward pass through training data, compute gradients, update weights\n","        train_loss, train_f1 = train_one_epoch(\n","            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n","        )\n","\n","        # Evaluate model on validation data without updating weights\n","        val_loss, val_f1 = validate_one_epoch(\n","            model, val_loader, criterion, device\n","        )\n","\n","        # Store metrics for plotting and analysis\n","        training_history['train_loss'].append(train_loss)\n","        training_history['val_loss'].append(val_loss)\n","        training_history['train_f1'].append(train_f1)\n","        training_history['val_f1'].append(val_f1)\n","\n","        # Write metrics to TensorBoard for visualization\n","        if writer is not None:\n","            log_metrics_to_tensorboard(\n","                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n","            )\n","\n","        # Print progress every N epochs or on first epoch\n","        if verbose > 0:\n","            if epoch % verbose == 0 or epoch == 1:\n","                print(f\"Epoch {epoch:3d}/{epochs} | \"\n","                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n","                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n","\n","        # Early stopping logic: monitor metric and save best model\n","        if patience > 0:\n","            current_metric = training_history[evaluation_metric][-1]\n","            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n","\n","            if is_improvement:\n","                best_metric = current_metric\n","                best_epoch = epoch\n","                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= patience:\n","                    if verbose > 0:\n","                        print(f\"Early stopping triggered after {epoch} epochs.\")\n","                    break\n","\n","    # Restore best model weights if early stopping was used\n","    if restore_best_weights and patience > 0:\n","        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n","        if verbose > 0:\n","            print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n","\n","    # Save final model if no early stopping\n","    if patience == 0:\n","        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n","\n","    # Close TensorBoard writer\n","    if writer is not None:\n","        writer.close()\n","\n","    return model, training_history"],"metadata":{"id":"u7OpSai1FNPU","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def k_shuffle_split_cross_validation_round(X, y, epochs, criterion, scaler, device,\n","                            k, test_size, batch_size, hidden_layers, hidden_size, learning_rate, dropout_rate,\n","                            l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n","                            restore_best_weights=True, writer=None, verbose=10, seed=SEED, experiment_name=\"\"):\n","\n","    # Initialise containers for results across all splits\n","    fold_losses = {}\n","    fold_metrics = {}\n","    best_scores = {}\n","\n","    # Initialise model architecture\n","    in_features = X.shape[1]\n","    num_classes = len(np.unique(y))\n","    model = FeedForwardNet(in_features, hidden_layers=hidden_layers, hidden_size=hidden_size,\n","                           dropout_rate=dropout_rate, num_classes=num_classes).to(device)\n","\n","    # Store initial weights to reset model for each split\n","    initial_state = copy.deepcopy(model.state_dict())\n","\n","    # Iterate through K random splits\n","    for split_idx in range(k):\n","\n","        if verbose > 0:\n","            print(f\"Split {split_idx+1}/{k}\")\n","\n","        # Create train-val-test split with stratification\n","        X_train_val, X_test, y_train_val, y_test = train_test_split(\n","            X, y,\n","            test_size=test_size,\n","            random_state=SEED+split_idx,\n","            stratify=y\n","        )\n","        X_train, X_val, y_train, y_val = train_test_split(\n","            X_train_val, y_train_val,\n","            test_size=test_size,\n","            random_state=SEED+split_idx,\n","            stratify=y_train_val\n","        )\n","\n","        # Normalise features using training set statistics\n","        train_max = X_train.max(axis=0)\n","        train_min = X_train.min(axis=0)\n","        X_train = (X_train - train_min) / (train_max - train_min + 1e-8)\n","        X_val = (X_val - train_min) / (train_max - train_min + 1e-8)\n","        X_test = (X_test - train_min) / (train_max - train_min + 1e-8)\n","\n","        # Create PyTorch datasets\n","        train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","        val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n","        test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n","\n","        # Create data loaders\n","        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n","        val_loader   = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n","        test_loader  = make_loader(test_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n","\n","        # Reset model to initial weights for fair comparison across splits\n","        model.load_state_dict(initial_state)\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n","\n","        # Create directory for model checkpoints\n","        !mkdir -p models/{experiment_name}\n","\n","        # Train model on current split\n","        model, training_history = fit(\n","            model=model,\n","            train_loader=train_loader,\n","            val_loader=val_loader,\n","            epochs=epochs,\n","            criterion=criterion,\n","            optimizer=optimizer,\n","            scaler=scaler,\n","            device=device,\n","            writer=writer,\n","            patience=patience,\n","            verbose=verbose,\n","            experiment_name=experiment_name+\"/split_\"+str(split_idx)\n","        )\n","\n","        # Store results for this split\n","        fold_losses[f\"split_{split_idx}\"] = training_history['val_loss']\n","        fold_metrics[f\"split_{split_idx}\"] = training_history['val_f1']\n","        best_scores[f\"split_{split_idx}\"] = max(training_history['val_f1'])\n","\n","    # Compute mean and standard deviation of best scores across splits\n","    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys()])\n","    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys()])\n","\n","    if verbose > 0:\n","        print(f\"Best score: {best_scores['mean']:.4f}¬±{best_scores['std']:.4f}\")\n","\n","    return fold_losses, fold_metrics, best_scores"],"metadata":{"id":"k5FFBYRpU1TV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cross-validation\n","K = 5                    # Number of splits (5 and 10 are considered good values)\n","TEST_SIZE = 0.2          # Validation/test proportion\n","\n","# Training\n","EPOCHS = 100              # Maximum epochs (increase to improve performance)\n","PATIENCE = 20             # Early stopping patience (increase to improve performance)\n","VERBOSE = 10             # Print frequency\n","\n","# Optimisation\n","LEARNING_RATE = 1e-3     # Learning rate\n","BATCH_SIZE = 64          # Batch size\n","\n","# Architecture\n","HIDDEN_LAYERS = 2        # Hidden layers\n","HIDDEN_SIZE = 128        # Neurons per layer\n","\n","# Regularisation\n","DROPOUT_RATE = 0         # Dropout probability\n","L1_LAMBDA = 0            # L1 penalty\n","L2_LAMBDA = 0            # L2 penalty\n","\n","# Training utilities\n","scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"4wevD9tXhc8K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Baseline**"],"metadata":{"id":"E5fY9uFixhZH"}},{"cell_type":"code","source":["%%time\n","# Execute K-fold cross-validation with baseline configuration\n","losses, metrics, best_scores = k_shuffle_split_cross_validation_round(\n","    X=X,\n","    y=y,\n","    epochs=EPOCHS,\n","    criterion=criterion,\n","    scaler=scaler,\n","    device=device,\n","    k=K,\n","    test_size=TEST_SIZE,\n","    batch_size=BATCH_SIZE,\n","    hidden_layers=HIDDEN_LAYERS,\n","    hidden_size=HIDDEN_SIZE,\n","    learning_rate=LEARNING_RATE,\n","    dropout_rate=DROPOUT_RATE,\n","    l1_lambda=L1_LAMBDA,\n","    l2_lambda=L2_LAMBDA,\n","    verbose=VERBOSE,\n","    patience=PATIENCE,\n","    seed=SEED,\n","    experiment_name=\"baseline\"\n",")"],"metadata":{"id":"2NnrW7Krsaqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Hitory\n","# Create figure with two subplots sharing x axis\n","fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 5), sharex=True)\n","\n","# Color palette for K splits\n","colors = plt.cm.get_cmap('tab10', K)\n","\n","# Plot validation loss for each split\n","for split in range(K):\n","    axes[0].plot(losses[f'split_{split}'][:-PATIENCE], label=f'Split {split+1}',\n","                 color=colors(split), alpha=0.6)\n","axes[0].set_title('Validation Loss per Split')\n","axes[0].set_ylabel('Loss')\n","axes[0].set_xlabel('Epoch')\n","axes[0].grid(alpha=0.3)\n","\n","# Plot validation F1 score for each split\n","for split in range(K):\n","    axes[1].plot(metrics[f'split_{split}'][:-PATIENCE], label=f'Split {split+1}',\n","                 color=colors(split), alpha=0.6)\n","axes[1].set_title('Validation F1 Score per Split')\n","axes[1].set_ylabel('F1 Score')\n","axes[1].set_xlabel('Epoch')\n","axes[1].grid(alpha=0.3)\n","\n","# Add shared legend on the right\n","handles, labels = axes[0].get_legend_handles_labels()\n","fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n","\n","# Adjust layout\n","plt.tight_layout()\n","plt.subplots_adjust(right=0.975)\n","plt.show()"],"metadata":{"cellView":"form","id":"dYU3mTMfFrxX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Hyperparameters Tuning**"],"metadata":{"id":"ybJgNcvXOCFu"}},{"cell_type":"code","source":["def grid_search_cv(X, y, param_grid, fixed_params, cv_params, verbose=True):\n","    \"\"\"\n","    Execute grid search with K-fold cross-validation.\n","\n","    Args:\n","        X, y: Training data and labels\n","        param_grid: Dict of parameters to test, e.g. {'batch_size': [16, 32, 64]}\n","        fixed_params: Dict of fixed hyperparameters (hidden_size, learning_rate, etc.)\n","        cv_params: Dict of CV settings (epochs, k, patience, criterion, scaler, device, etc.)\n","        verbose: Print progress for each configuration\n","\n","    Returns:\n","        results: Dict with scores for each configuration\n","        best_config: Dict with best hyperparameter combination\n","        best_score: Best mean F1 score achieved\n","    \"\"\"\n","    from itertools import product\n","\n","    # Generate all parameter combinations\n","    param_names = list(param_grid.keys())\n","    param_values = list(param_grid.values())\n","    combinations = list(product(*param_values))\n","\n","    results = {}\n","    best_score = -np.inf\n","    best_config = None\n","\n","    total = len(combinations)\n","\n","    for idx, combo in enumerate(combinations, 1):\n","        # Create current configuration dict\n","        current_config = dict(zip(param_names, combo))\n","        config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n","\n","        if verbose:\n","            print(f\"\\nConfiguration {idx}/{total}:\")\n","            for param, value in current_config.items():\n","                print(f\"  {param}: {value}\")\n","\n","        # Merge current config with fixed parameters\n","        run_params = {**fixed_params, **current_config}\n","\n","        # Execute cross-validation\n","        _, _, fold_scores = k_shuffle_split_cross_validation_round(\n","            X=X, y=y,\n","            experiment_name=config_str,\n","            **run_params,\n","            **cv_params\n","        )\n","\n","        # Store results\n","        results[config_str] = fold_scores\n","\n","        # Track best configuration\n","        if fold_scores[\"mean\"] > best_score:\n","            best_score = fold_scores[\"mean\"]\n","            best_config = current_config.copy()\n","            if verbose:\n","                print(\"  NEW BEST SCORE!\")\n","\n","        if verbose:\n","            print(f\"  F1 Score: {fold_scores['mean']:.4f}¬±{fold_scores['std']:.4f}\")\n","\n","    return results, best_config, best_score\n","\n","\n","def plot_top_configurations(results, k_splits, top_n=5, figsize=(12, 6)):\n","    \"\"\"\n","    Visualise top N configurations with boxplots of F1 scores across CV splits.\n","\n","    Args:\n","        results: Dict of results from grid_search_cv\n","        k_splits: Number of CV splits used\n","        top_n: Number of top configurations to display\n","        figsize: Figure size tuple\n","    \"\"\"\n","    # Sort by mean score\n","    config_scores = {name: data['mean'] for name, data in results.items()}\n","    sorted_configs = sorted(config_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","    # Select top N\n","    top_configs = sorted_configs[:min(top_n, len(sorted_configs))]\n","\n","    # Prepare boxplot data\n","    boxplot_data = []\n","    labels = []\n","\n","    # Define a dictionary for replacements, ordered to handle prefixes correctly\n","    replacements = {\n","        'batch_size_': 'BS=',\n","        'learning_rate_': '\\nLR=',\n","        'hidden_layers_': '\\nHL=',\n","        'hidden_size_': '\\nHS=',\n","        'dropout_rate_': '\\nDR=',\n","        'l1_lambda_': '\\nL1=',\n","        'l2_lambda_': '\\nL2='\n","    }\n","\n","    # Replacements for separators\n","    separator_replacements = {\n","        '_learning_rate_': '\\nLR=',\n","        '_hidden_layers_': '\\nHL=',\n","        '_hidden_size_': '\\nHS=',\n","        '_dropout_rate_': '\\nDR=',\n","        '_l1_lambda_': '\\nL1=',\n","        '_l2_lambda_': '\\nL2=',\n","        '_': ''\n","    }\n","\n","\n","    for config_name, mean_score in top_configs:\n","        # Extract best score from each split (auto-detect number of splits)\n","        split_scores = []\n","        for i in range(k_splits):\n","            if f'split_{i}' in results[config_name]:\n","                split_scores.append(results[config_name][f'split_{i}'])\n","        boxplot_data.append(split_scores)\n","\n","        # Verify we have the expected number of splits\n","        if len(split_scores) != k_splits:\n","            print(f\"Warning: Config {config_name} has {len(split_scores)} splits, expected {k_splits}\")\n","\n","        # Create readable label using the replacements dictionary\n","        readable_label = config_name\n","        for old, new in replacements.items():\n","            readable_label = readable_label.replace(old, new)\n","\n","        # Apply separator replacements\n","        for old, new in separator_replacements.items():\n","             readable_label = readable_label.replace(old, new)\n","\n","        labels.append(f\"{readable_label}\\n(Œº={mean_score:.3f})\")\n","\n","    # Create plot\n","    fig, ax = plt.subplots(figsize=figsize)\n","    bp = ax.boxplot(boxplot_data, labels=labels, patch_artist=True,\n","                    showmeans=True, meanline=True)\n","\n","    # Styling\n","    for patch in bp['boxes']:\n","        patch.set_facecolor('lightblue')\n","        patch.set_alpha(0.7)\n","\n","    # Highlight best configuration\n","    ax.get_xticklabels()[0].set_fontweight('bold')\n","\n","    ax.set_ylabel('F1 Score')\n","    ax.set_xlabel('Configuration')\n","    ax.set_title(f'Top {len(top_configs)} Configurations - F1 Score Distribution Across {k_splits} Splits')\n","    ax.grid(alpha=0.3, axis='y')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"MK8jVnuI2Sld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VERBOSE = 0             # Print frequency"],"metadata":{"id":"c31PEutsF4-d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Batch Size**"],"metadata":{"id":"IjA-9Cosexji"}},{"cell_type":"code","source":["%%time\n","# Define parameters to search\n","param_grid = {\n","    'batch_size': [4, 16, 32, 64, 128]\n","}\n","\n","# Fixed hyperparameters (not being tuned)\n","fixed_params = {\n","    'hidden_layers': HIDDEN_LAYERS,\n","    'hidden_size': HIDDEN_SIZE,\n","    'learning_rate': LEARNING_RATE,\n","    'dropout_rate': DROPOUT_RATE,\n","    'l1_lambda': L1_LAMBDA,\n","    'l2_lambda': L2_LAMBDA\n","}\n","\n","# Cross-validation settings\n","cv_params = {\n","    'epochs': EPOCHS,\n","    'criterion': criterion,\n","    'scaler': scaler,\n","    'device': device,\n","    'k': K,\n","    'test_size': TEST_SIZE,\n","    'patience': PATIENCE,\n","    'verbose': VERBOSE,\n","    'seed': SEED\n","}\n","\n","# Execute search\n","results, best_config, best_score = grid_search_cv(\n","    X=X, y=y,\n","    param_grid=param_grid,\n","    fixed_params=fixed_params,\n","    cv_params=cv_params\n",")"],"metadata":{"id":"xGs5tr8H2bwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the results\n","print(results)"],"metadata":{"id":"iuY_U3gi7hrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the best configuration found\n","print(best_config)"],"metadata":{"id":"RRTtTsIu7iJa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the best performance achieved\n","print(best_score)"],"metadata":{"id":"Xy174JO77iG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualise results\n","plot_top_configurations(results, k_splits=K, top_n=5)"],"metadata":{"id":"A9-Ixz-76A80"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Learning Rate**"],"metadata":{"id":"FMqSO8bxBORl"}},{"cell_type":"code","source":["%%time\n","# Define parameters to search\n","param_grid = {\n","    'learning_rate': [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n","}\n","\n","# Fixed hyperparameters (not being tuned)\n","fixed_params = {\n","    'hidden_layers': HIDDEN_LAYERS,\n","    'hidden_size': HIDDEN_SIZE,\n","    'batch_size': BATCH_SIZE,\n","    'dropout_rate': DROPOUT_RATE,\n","    'l1_lambda': L1_LAMBDA,\n","    'l2_lambda': L2_LAMBDA\n","}\n","\n","# Cross-validation settings\n","cv_params = {\n","    'epochs': EPOCHS,\n","    'criterion': criterion,\n","    'scaler': scaler,\n","    'device': device,\n","    'k': K,\n","    'test_size': TEST_SIZE,\n","    'patience': PATIENCE,\n","    'verbose': VERBOSE,\n","    'seed': SEED\n","}\n","\n","# Execute search\n","results, best_config, best_score = grid_search_cv(\n","    X=X, y=y,\n","    param_grid=param_grid,\n","    fixed_params=fixed_params,\n","    cv_params=cv_params\n",")"],"metadata":{"id":"UINFAFWm7ROa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualise results\n","plot_top_configurations(results, k_splits=K, top_n=4)"],"metadata":{"id":"g1sA096B9c4l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Model Complexity**"],"metadata":{"id":"1sQAQ6k90DDv"}},{"cell_type":"code","source":["%%time\n","# Define parameters to search\n","param_grid = {\n","    'hidden_layers': [1, 2],\n","    'hidden_size': [32, 128, 512]\n","}\n","\n","# Fixed hyperparameters (not being tuned)\n","fixed_params = {\n","    'learning_rate': LEARNING_RATE,\n","    'batch_size': BATCH_SIZE,\n","    'dropout_rate': DROPOUT_RATE,\n","    'l1_lambda': L1_LAMBDA,\n","    'l2_lambda': L2_LAMBDA\n","}\n","\n","# Cross-validation settings\n","cv_params = {\n","    'epochs': EPOCHS,\n","    'criterion': criterion,\n","    'scaler': scaler,\n","    'device': device,\n","    'k': K,\n","    'test_size': TEST_SIZE,\n","    'patience': PATIENCE,\n","    'verbose': VERBOSE,\n","    'seed': SEED\n","}\n","\n","# Execute search\n","results, best_config, best_score = grid_search_cv(\n","    X=X, y=y,\n","    param_grid=param_grid,\n","    fixed_params=fixed_params,\n","    cv_params=cv_params\n",")"],"metadata":{"id":"d3Need769uyw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualise results\n","plot_top_configurations(results, k_splits=K, top_n=3)"],"metadata":{"id":"4DEIcs2I9uwP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Batch Size vs Learning Rate vs Model Complexity**"],"metadata":{"id":"9x2oKpcfftDe"}},{"cell_type":"code","source":["%%time\n","# Define parameters to search\n","param_grid = {\n","    'batch_size': [16, 32],\n","    'learning_rate': [1e-3, 5e-3],\n","    'hidden_layers': [1, 2],\n","    'hidden_size': [128, 512]\n","}\n","\n","# Fixed hyperparameters (not being tuned)\n","fixed_params = {\n","    'dropout_rate': DROPOUT_RATE,\n","    'l1_lambda': L1_LAMBDA,\n","    'l2_lambda': L2_LAMBDA\n","}\n","\n","# Cross-validation settings\n","cv_params = {\n","    'epochs': EPOCHS,\n","    'criterion': criterion,\n","    'scaler': scaler,\n","    'device': device,\n","    'k': K,\n","    'test_size': TEST_SIZE,\n","    'patience': PATIENCE,\n","    'verbose': VERBOSE,\n","    'seed': SEED\n","}\n","\n","# Execute search\n","results, best_config, best_score = grid_search_cv(\n","    X=X, y=y,\n","    param_grid=param_grid,\n","    fixed_params=fixed_params,\n","    cv_params=cv_params\n",")"],"metadata":{"id":"9Ic-gTJO-Wtj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualise results\n","plot_top_configurations(results, k_splits=K, top_n=5)"],"metadata":{"id":"nhPhkunT-Wog"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Model Regularisation**\n","\n"],"metadata":{"id":"l8i8yY-88bvK"}},{"cell_type":"code","source":["%%time\n","# Define parameters to search\n","param_grid = {\n","    'dropout_rate': [0, 0.2, 0.5],\n","    'l1_lambda': [0, 1e-4, 1e-2],\n","    'l2_lambda': [0, 1e-4, 1e-2]\n","}\n","\n","# Fixed hyperparameters (not being tuned)\n","fixed_params = {\n","    'batch_size': 16,\n","    'learning_rate': 0.005,\n","    'hidden_layers': 2,\n","    'hidden_size': 512\n","}\n","\n","# Cross-validation settings\n","cv_params = {\n","    'epochs': EPOCHS,\n","    'criterion': criterion,\n","    'scaler': scaler,\n","    'device': device,\n","    'k': K,\n","    'test_size': TEST_SIZE,\n","    'patience': PATIENCE,\n","    'verbose': VERBOSE,\n","    'seed': SEED\n","}\n","\n","# Execute search\n","results, best_config, best_score = grid_search_cv(\n","    X=X, y=y,\n","    param_grid=param_grid,\n","    fixed_params=fixed_params,\n","    cv_params=cv_params\n",")"],"metadata":{"id":"bkLafrp1DMoA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualise results\n","plot_top_configurations(results, k_splits=K, top_n=5)"],"metadata":{"id":"Zt724GBUDMlV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Batch Size vs Learning Rate vs Model Complexity vs Model Regularisation**"],"metadata":{"id":"0nf4YT8m-vFY"}},{"cell_type":"code","source":["%%time\n","# Define parameters to search\n","param_grid = {\n","    'batch_size': [16, 32],\n","    'learning_rate': [1e-3, 5e-3],\n","    'hidden_layers': [1, 2],\n","    'hidden_size': [128, 512],\n","    'dropout_rate': [0, 0.2, 0.5],\n","    'l1_lambda': [0, 1e-4, 1e-2],\n","    'l2_lambda': [0, 1e-4, 1e-2]\n","}\n","\n","# Fixed hyperparameters (not being tuned)\n","fixed_params = {}\n","\n","# Cross-validation settings\n","cv_params = {\n","    'epochs': EPOCHS,\n","    'criterion': criterion,\n","    'scaler': scaler,\n","    'device': device,\n","    'k': K,\n","    'test_size': TEST_SIZE,\n","    'patience': PATIENCE,\n","    'verbose': VERBOSE,\n","    'seed': SEED\n","}\n","\n","# Execute search\n","results, best_config, best_score = grid_search_cv(\n","    X=X, y=y,\n","    param_grid=param_grid,\n","    fixed_params=fixed_params,\n","    cv_params=cv_params\n",")"],"metadata":{"id":"Gb0EwmT1aekZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualise results\n","plot_top_configurations(results, k_splits=K, top_n=5)"],"metadata":{"id":"cQjdbYHjaeh5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üïπÔ∏è **Inference**"],"metadata":{"id":"B5myzy4k-3yK"}},{"cell_type":"code","source":["\"\"\"\n","Configuration 143/432:\n","  batch_size: 16\n","  learning_rate: 0.005\n","  hidden_layers: 1\n","  hidden_size: 512\n","  dropout_rate: 0\n","  l1_lambda: 0.01\n","  l2_lambda: 0.0001\n","  F1 Score: 0.7582¬±0.0602\n","\"\"\"\n","\n","best_configuration = {}\n","best_configuration['batch_size'] = 16\n","best_configuration['learning_rate'] = 0.005\n","best_configuration['hidden_layers'] = 1\n","best_configuration['hidden_size'] = 512\n","best_configuration['dropout_rate'] = 0\n","best_configuration['l1_lambda'] = 0.01\n","best_configuration['l2_lambda'] = 0.0001"],"metadata":{"id":"6yKse15QsilR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Retrieve the best hyperparameters\n","best_batch_size = best_configuration['batch_size']\n","best_learning_rate = best_configuration['learning_rate']\n","best_hidden_layers = best_configuration['hidden_layers']\n","best_hidden_size = best_configuration['hidden_size']\n","best_dropout_rate = best_configuration['dropout_rate']\n","best_l1_lambda = best_configuration['l1_lambda']\n","best_l2_lambda = best_configuration['l2_lambda']\n","\n","# Initialize lists to store metrics for each split on the test set\n","test_accuracies = []\n","test_precisions = []\n","test_recall_scores = [] # Corrected typo\n","test_f1_scores = []\n","all_test_targets = []\n","all_test_preds = []\n","\n","\n","for split in range(K):\n","    print(f\"Evaluating Split {split+1}/{K}\")\n","\n","    # Regenerate the data splits to ensure the correct test set for this split\n","    X_train_val, X_test, y_train_val, y_test = train_test_split(\n","        X,\n","        y,\n","        test_size=TEST_SIZE,\n","        random_state=SEED+split,\n","        stratify=y\n","    )\n","    # Further split train_val to get train and validation sets (needed for normalization)\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_train_val,\n","        y_train_val,\n","        test_size=TEST_SIZE,\n","        random_state=SEED+split, # Use same random state for consistent splitting\n","        stratify=y_train_val\n","    )\n","\n","    # Apply the same normalization learned from the training set of this split to the test set\n","    max_df = X_train.max(axis=0)\n","    min_df = X_train.min(axis=0)\n","    # Add a small epsilon to avoid division by zero in case of constant features\n","    X_test_normalized = (X_test - min_df) / (max_df - min_df + 1e-8)\n","\n","\n","    # Create a TensorDataset and DataLoader for the test set\n","    test_ds  = TensorDataset(torch.from_numpy(X_test_normalized), torch.from_numpy(y_test))\n","    test_loader  = make_loader(test_ds, batch_size=best_batch_size, shuffle=False, drop_last=False)\n","\n","    # Initialize the model with the best hyperparameters found\n","    model = FeedForwardNet(in_features=X.shape[1],\n","                           hidden_layers=best_hidden_layers,\n","                           hidden_size=best_hidden_size,\n","                           dropout_rate=best_dropout_rate,\n","                           num_classes=len(np.unique(y))).to(device)\n","\n","    # Construct the path to the saved model weights for this specific split and best configuration\n","    config_name = f\"batch_size_{best_batch_size}_learning_rate_{best_learning_rate}_hidden_layers_{best_hidden_layers}_hidden_size_{best_hidden_size}_dropout_rate_{best_dropout_rate}_l1_lambda_{best_l1_lambda}_l2_lambda_{best_l2_lambda}\"\n","    model_path = f\"models/{config_name}/split_{split}_model.pt\"\n","\n","    # Load the model weights\n","    model.load_state_dict(torch.load(model_path))\n","    model.eval() # Set model to evaluation mode\n","\n","\n","    # Make predictions on the test set for this split\n","    split_test_preds, split_test_targets = [], []\n","    with torch.no_grad():  # Disable gradient computation for inference\n","        for xb, yb in test_loader:\n","            xb = xb.to(device)\n","\n","            # Forward pass: get model predictions\n","            logits = model(xb)\n","            preds = logits.argmax(dim=1).cpu().numpy()\n","\n","            # Store batch results\n","            split_test_preds.append(preds)\n","            split_test_targets.append(yb.numpy())\n","\n","    # Combine all batches into single arrays for this split\n","    split_test_preds = np.concatenate(split_test_preds)\n","    split_test_targets = np.concatenate(split_test_targets)\n","\n","    # Calculate metrics for this split's test set\n","    split_test_acc = accuracy_score(split_test_targets, split_test_preds)\n","    split_test_prec = precision_score(split_test_targets, split_test_preds, average='weighted')\n","    split_test_rec = recall_score(split_test_targets, split_test_preds, average='weighted')\n","    split_test_f1 = f1_score(split_test_targets, split_test_preds, average='weighted')\n","\n","    # Print F1 score for the current split\n","    print(f\"  Test F1 Score for Split {split+1}: {split_test_f1:.4f}\")\n","\n","\n","    # Store metrics for calculating average later\n","    test_accuracies.append(split_test_acc)\n","    test_precisions.append(split_test_prec)\n","    test_recall_scores.append(split_test_rec)\n","    test_f1_scores.append(split_test_f1)\n","\n","    # Extend the overall lists for the confusion matrix\n","    all_test_targets.extend(split_test_targets)\n","    all_test_preds.extend(split_test_preds)\n","\n","\n","# Calculate and print average metrics across all splits on the test set\n","print(\"\\nAverage metrics across all splits on the test set:\")\n","print(f\"Mean Accuracy: {np.mean(test_accuracies):.4f} ¬± {np.std(test_accuracies):.4f}\")\n","print(f\"Mean Precision: {np.mean(test_precisions):.4f} ¬± {np.std(test_precisions):.4f}\")\n","print(f\"Mean Recall: {np.mean(test_recall_scores):.4f} ¬± {np.std(test_recall_scores):.4f}\")\n","print(f\"Mean F1 score: {np.mean(test_f1_scores):.4f} ¬± {np.std(test_f1_scores):.4f}\")\n","\n","\n","# Generate confusion matrix for the concatenated test sets (across all splits)\n","cm = confusion_matrix(all_test_targets, all_test_preds)\n","\n","# Create numeric labels for heatmap annotation\n","labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n","\n","# Visualise confusion matrix\n","plt.figure(figsize=(8, 7))\n","sns.heatmap(cm, annot=labels, fmt='',\n","            cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Aggregated Confusion Matrix ‚Äî Test Sets Across Splits')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"RLiotKiDzztT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_models_directory(scores, best_configuration):\n","    \"\"\"\n","    Deletes all model directories and files in the 'models' directory\n","    except for the directory containing the best model based on the\n","    best_configuration.\n","\n","    Args:\n","        scores (dict): Dictionary containing the results of the hyperparameter search.\n","        best_configuration (dict): Dictionary containing the hyperparameters\n","                                   of the best performing configuration.\n","    \"\"\"\n","    models_dir = \"models\"\n","    if not os.path.exists(models_dir):\n","        print(f\"Models directory '{models_dir}' not found.\")\n","        return\n","\n","    # Construct the expected directory name for the best configuration\n","    best_config_dir_name = f\"bs_{best_configuration['batch_size']}_lr_{best_configuration['learning_rate']}_hl_{best_configuration['hidden_layers']}_hs_{best_configuration['hidden_size']}_dr_{best_configuration['dropout_rate']}_l1_{best_configuration['l1_lambda']}_l2_{best_configuration['l2_lambda']}\"\n","    best_config_path = os.path.join(models_dir, best_config_dir_name)\n","\n","    # Add a check to ensure the best configuration directory exists\n","    if not os.path.exists(best_config_path):\n","        print(f\"Error: Best model directory '{best_config_path}' not found. Cannot clean directory safely.\")\n","        return\n","\n","    print(f\"Keeping the best model directory: {best_config_path}\")\n","\n","    # Iterate through all items in the models directory\n","    for item in os.listdir(models_dir):\n","        item_path = os.path.join(models_dir, item)\n","\n","        # If the item is a file or a directory and not the best configuration directory, delete it\n","        if item_path != best_config_path:\n","            if os.path.isdir(item_path):\n","                print(f\"Deleting directory: {item_path}\")\n","                shutil.rmtree(item_path)\n","            elif os.path.isfile(item_path):\n","                print(f\"Deleting file: {item_path}\")\n","                os.remove(item_path)\n","\n","    print(\"Models directory cleaned.\")\n","\n","# clean_models_directory(scores, best_configuration)"],"metadata":{"id":"IR_JdhGezzvY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOZkZVOlzz0e"},"source":["#  \n","<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n","\n","##### Connect with us:\n","- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"14\"> **LinkedIn:**  [AIRLab Polimi](https://www.linkedin.com/company/airlab-polimi/)\n","- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"14\"> **Instagram:** [airlab_polimi](https://www.instagram.com/airlab_polimi/)\n","\n","##### Contributors:\n","- **Eugenio Lomurno**: eugenio.lomurno@polimi.it\n","- **Alberto Archetti**: alberto.archetti@polimi.it\n","- **Roberto Basla**: roberto.basla@polimi.it\n","- **Carlo Sgaravatti**: carlo.sgaravatti@polimi.it\n","\n","```\n","   Copyright 2025 Eugenio Lomurno, Alberto Archetti, Roberto Basla, Carlo Sgaravatti\n","\n","   Licensed under the Apache License, Version 2.0 (the \"License\");\n","   you may not use this file except in compliance with the License.\n","   You may obtain a copy of the License at\n","\n","       http://www.apache.org/licenses/LICENSE-2.0\n","\n","   Unless required by applicable law or agreed to in writing, software\n","   distributed under the License is distributed on an \"AS IS\" BASIS,\n","   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","   See the License for the specific language governing permissions and\n","   limitations under the License.\n","```\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"12ODGsMkMPxFykDXp0w1dWj_F1FDU65sh","timestamp":1633524062268}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}