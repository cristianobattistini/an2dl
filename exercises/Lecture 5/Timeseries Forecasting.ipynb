{"cells":[{"cell_type":"markdown","metadata":{"id":"Z1O12-Gwx3fw"},"source":["# **Artificial Neural Networks and Deep Learning**\n","\n","---\n","\n","## **Lecture 5: Timeseries Forecasting**\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1Co7J09_1WY1kmurHm8SJPkxZw2_5s_tL\" width=\"500\"/>"]},{"cell_type":"markdown","metadata":{"id":"nmKMLFLdfdhL"},"source":["## üåê **Google Drive Connection**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIPnkn7fcGLp"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/gdrive\")\n","current_dir = \"/gdrive/My\\\\ Drive/[2025-2026]\\\\ AN2DL/Lecture\\\\ 5\"\n","%cd $current_dir"]},{"cell_type":"markdown","metadata":{"id":"ap3o8JayfgEM"},"source":["## ‚öôÔ∏è **Libraries Import**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aR-w0eZ-ABIT"},"outputs":[],"source":["# Set seed for reproducibility\n","SEED = 42\n","\n","# Import necessary libraries\n","import os\n","\n","# Set environment variables before importing modules\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n","\n","# Suppress warnings\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","# Import necessary modules\n","import logging\n","import random\n","import numpy as np\n","\n","# Set seeds for random number generators in NumPy and Python\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","# Import PyTorch\n","import torch\n","torch.manual_seed(SEED)\n","from torch import nn\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import TensorDataset, DataLoader\n","logs_dir = \"tensorboard\"\n","!pkill -f tensorboard\n","%load_ext tensorboard\n","!mkdir -p models\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.benchmark = True\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Device: {device}\")\n","\n","# Import other libraries\n","import copy\n","import shutil\n","from datetime import datetime\n","from itertools import product\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Configure plot display settings\n","sns.set(font_scale=1.4)\n","sns.set_style('white')\n","plt.rc('font', size=14)\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"a49TrZ82D-Fl"},"source":["## ‚è≥ **Data Loading**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh_MCOFHxzQ3"},"outputs":[],"source":["# Set environment variables for Air Quality dataset\n","os.environ[\"DATASET_NAME\"] = \"air_quality.csv\"\n","os.environ[\"DATASET_URL\"] = \"1RQZbTbSsj-rL2Xc960VRge4cDmV0ByNf\"\n","\n","# Check if Air Quality dataset exists, download and unzip if not\n","if not os.path.exists(os.environ[\"DATASET_NAME\"]):\n","    print(\"Downloading Air Quality dataset...\")\n","    !gdown -q ${DATASET_URL} -O ${DATASET_NAME}\n","    print(\"Air Quality dataset downloaded!\")\n","else:\n","    print(\"Air Quality dataset already downloaded. Using cached data.\")"]},{"cell_type":"markdown","metadata":{"id":"ZVNvxWmG0OHH"},"source":["## üîé **Exploration and Data Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YB1vxLkYyfFQ"},"outputs":[],"source":["# Load the dataset from a CSV file\n","dataset = pd.read_csv('air_quality.csv')\n","\n","# Print the shape of the dataset\n","print(f\"Dataset shape: {dataset.shape}\")\n","\n","# Display the first few rows of the dataset\n","dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f75FMAA1CNA2"},"outputs":[],"source":["# Display a summary of the dataset's structure and data types\n","dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXpSLGgC902P"},"outputs":[],"source":["# Define a function to plot and inspect specified columns of a dataframe\n","def inspect_dataframe(df, columns):\n","    # Create subplots for each column\n","    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17, 10))\n","\n","    # Iterate through the specified columns and plot each\n","    for i, col in enumerate(columns):\n","        axs[i].plot(df[col])\n","        axs[i].set_title(col)\n","\n","    # Display the plots\n","    plt.show()\n","\n","# Call the function to inspect all columns of the dataset\n","inspect_dataframe(dataset, dataset.columns)"]},{"cell_type":"markdown","metadata":{"id":"EnhcDiwV0OHI"},"source":["## üîÑ **Data Preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-B7nZOTphuJP"},"outputs":[],"source":["# Convert 'dew' column to float32\n","dataset['dew'] = dataset['dew'].astype(np.float32)\n","\n","# Convert 'temperature' column to float32\n","dataset['temperature'] = dataset['temperature'].astype(np.float32)\n","\n","# Convert 'pression' column to float32\n","dataset['pression'] = dataset['pression'].astype(np.float32)\n","\n","# Display updated dataset information\n","dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wB4jRVsPJw25"},"outputs":[],"source":["# Define sizes for validation and test sets\n","val_size = 2850\n","test_size = 2850\n","\n","# Split the dataset into training, validation, and test sets\n","X_train_raw = dataset.iloc[:-val_size-test_size]\n","X_val_raw = dataset.iloc[-val_size-test_size:-test_size]\n","X_test_raw = dataset.iloc[-test_size:]\n","\n","# Print the shapes of the split datasets\n","print(f\"Train set shape: {X_train_raw.shape}\")\n","print(f\"Validation set shape: {X_val_raw.shape}\")\n","print(f\"Test set shape: {X_test_raw.shape}\")\n","\n","# Normalise data using training set statistics\n","X_min = X_train_raw.min()\n","X_max = X_train_raw.max()\n","\n","# Apply min-max normalisation\n","X_train_raw = (X_train_raw - X_min) / (X_max - X_min)\n","X_val_raw = (X_val_raw - X_min) / (X_max - X_min)\n","X_test_raw = (X_test_raw - X_min) / (X_max - X_min)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-J4bF6bIA_do"},"outputs":[],"source":["# Define target labels as the column names of the dataset\n","TARGET_LABELS = dataset.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcUR2mxDBT32"},"outputs":[],"source":["# Define a function to inspect and compare data splits\n","def inspect_splits(X_train, X_val, X_test, columns):\n","    # Create subplots for each column\n","    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17, 10))\n","\n","    # Plot train, validation, and test data for each column\n","    for i, col in enumerate(columns):\n","        axs[i].plot(X_train[col], label='Train')\n","        axs[i].plot(X_val[col], label='Validation')\n","        axs[i].plot(X_test[col], label='Test')\n","        axs[i].set_title(col)\n","\n","    # Add a single legend below all subplots\n","    handles, labels = axs[0].get_legend_handles_labels()\n","    figs.legend(handles, labels, loc='center', bbox_to_anchor=(0.5, 0.02),\n","                ncol=3, borderaxespad=0.)\n","\n","    # Adjust layout to prevent legend overlap\n","    plt.tight_layout()\n","    plt.subplots_adjust(bottom=0.1)\n","    plt.show()\n","\n","# Inspect splits across all target labels\n","inspect_splits(X_train_raw, X_val_raw, X_test_raw, TARGET_LABELS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5U5QeKoWJP9X"},"outputs":[],"source":["# Define window size for time-series data\n","WINDOW_SIZE = 800\n","\n","# Define stride length for window movement\n","STRIDE = 5\n","\n","# Define telescope size for prediction horizon\n","TELESCOPE_SIZE = 50"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0ckE6LaO68r"},"outputs":[],"source":["# Extract the last 'WINDOW_SIZE' rows of the dataset for future prediction\n","future = dataset[-WINDOW_SIZE:]\n","\n","# Normalise the future data using training set statistics\n","future = (future - X_min) / (X_max - X_min)\n","\n","# Expand dimensions to prepare for model input\n","future = np.expand_dims(future, axis=0)\n","\n","# Print the shape of the prepared future data\n","future.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFoJa5kMkq2g"},"outputs":[],"source":["# Define a function to build sequences and corresponding labels from a dataframe\n","def build_sequences(df, target_labels=['temperature'], window=200, stride=20, telescope=100):\n","    # Ensure the window size is compatible with the stride\n","    assert window % stride == 0\n","\n","    # Initialise containers for dataset sequences and labels\n","    dataset = []\n","    labels = []\n","\n","    # Copy dataframe values for processing\n","    temp_df = df.copy().values\n","    temp_label = df[target_labels].copy().values\n","\n","    # Check for and handle padding requirement\n","    padding_check = len(df) % window\n","    if padding_check != 0:\n","        # Compute padding length\n","        padding_len = window - len(df) % window\n","\n","        # Add zero-padding to features and labels\n","        padding = np.zeros((padding_len, temp_df.shape[1]), dtype='float32')\n","        temp_df = np.concatenate((padding, temp_df))\n","\n","        padding = np.zeros((padding_len, temp_label.shape[1]), dtype='float32')\n","        temp_label = np.concatenate((padding, temp_label))\n","\n","        # Verify that the padded data length is divisible by the window size\n","        assert len(temp_df) % window == 0\n","\n","    # Create sequences and corresponding labels\n","    for idx in np.arange(0, len(temp_df) - window - telescope, stride):\n","        dataset.append(temp_df[idx:idx + window])\n","        labels.append(temp_label[idx + window:idx + window + telescope])\n","\n","    # Convert lists to numpy arrays\n","    dataset = np.array(dataset)\n","    labels = np.array(labels)\n","\n","    # Return the dataset and labels\n","    return dataset, labels"]},{"cell_type":"markdown","metadata":{"id":"LgY5g6xUDswZ"},"source":["## üõ†Ô∏è **Multivariate Forecating (Direct)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHNFduwNy2h1"},"outputs":[],"source":["# Define direct telescope size for prediction horizon\n","direct_telescope = TELESCOPE_SIZE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_dsZ1Mb41DH"},"outputs":[],"source":["# Build sequences and labels for training, validation, and test datasets\n","X_train, y_train = build_sequences(X_train_raw, TARGET_LABELS, WINDOW_SIZE, STRIDE, direct_telescope)\n","X_val, y_val = build_sequences(X_val_raw, TARGET_LABELS, WINDOW_SIZE, 1, direct_telescope)\n","X_test, y_test = build_sequences(X_test_raw, TARGET_LABELS, WINDOW_SIZE, 1, direct_telescope)\n","\n","# Print the shapes of the resulting datasets\n","X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruqsimHHmqzJ"},"outputs":[],"source":["# Define a function to inspect multivariate sequences and predictions\n","def inspect_multivariate(X, y, columns, telescope, idx=None):\n","    # Select a random index if none is provided\n","    if idx is None:\n","        idx = np.random.randint(0, len(X))\n","\n","    # Create subplots for visualisation\n","    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17, 10))\n","\n","    # Iterate through each column and plot the sequence and target predictions\n","    for i, col in enumerate(columns):\n","        axs[i].plot(np.arange(len(X[0, :, i])), X[idx, :, i])\n","        axs[i].scatter(np.arange(len(X[0, :, i]), len(X[0, :, i]) + telescope),\n","                       y[idx, :, i], color='orange')\n","        axs[i].set_title(col)\n","        axs[i].set_ylim(0, 1)\n","\n","    # Display the plots\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKVNgYSmDxp4"},"outputs":[],"source":["# Inspect multivariate sequences and predictions using the training data\n","inspect_multivariate(X_train, y_train, TARGET_LABELS, direct_telescope)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXtMw_8-0OHK"},"outputs":[],"source":["# Define the input shape based on the training data\n","input_shape = X_train.shape[1:]\n","\n","# Define the output shape based on the training labels\n","output_shape = y_train.shape[1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QauHepiR0OHK"},"outputs":[],"source":["# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n","train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n","test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnJ3kprO0OHK"},"outputs":[],"source":["# Define the batch size, which is the number of samples in each batch\n","BATCH_SIZE = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Um623fqN0OHK"},"outputs":[],"source":["def make_loader(ds, batch_size, shuffle, drop_last):\n","    # Determine optimal number of worker processes for data loading\n","    cpu_cores = os.cpu_count() or 2\n","    num_workers = max(2, min(4, cpu_cores))\n","\n","    # Create DataLoader with performance optimizations\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers,\n","        pin_memory=True,  # Faster GPU transfer\n","        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n","        prefetch_factor=4,  # Load 4 batches ahead\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovHWYKlS0OHK"},"outputs":[],"source":["# Create data loaders with different settings for each phase\n","train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n","test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ous3qIUa0OHK"},"outputs":[],"source":["# Get one batch from the training data loader\n","for xb, yb in train_loader:\n","    print(\"Features batch shape:\", xb.shape)\n","    print(\"Labels batch shape:\", yb.shape)\n","    break # Stop after getting one batch"]},{"cell_type":"markdown","metadata":{"id":"KD1RAmtz0OHK"},"source":["## üõ†Ô∏è **Model Building**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4oF3AxG0OHK","cellView":"form"},"outputs":[],"source":["# @title recurrent_summary()\n","def recurrent_summary(model, input_size):\n","    \"\"\"\n","    Custom summary function that emulates torchinfo's output while correctly\n","    counting parameters for RNN/GRU/LSTM layers.\n","\n","    This function is designed for models whose direct children are\n","    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n","\n","    Args:\n","        model (nn.Module): The model to analyze.\n","        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n","    \"\"\"\n","\n","    # Dictionary to store output shapes captured by forward hooks\n","    output_shapes = {}\n","    # List to track hook handles for later removal\n","    hooks = []\n","\n","    def get_hook(name):\n","        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n","        def hook(module, input, output):\n","            # Handle RNN layer outputs (returns a tuple)\n","            if isinstance(output, tuple):\n","                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n","                shape1 = list(output[0].shape)\n","                shape1[0] = -1  # Replace batch dimension with -1\n","\n","                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n","                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n","                    shape2 = list(output[1][0].shape)  # Extract h_n only\n","                else:  # RNN/GRU case: h_n only\n","                    shape2 = list(output[1].shape)\n","\n","                # Replace batch dimension (middle position) with -1\n","                shape2[1] = -1\n","\n","                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n","\n","            # Handle standard layer outputs (e.g., Linear)\n","            else:\n","                shape = list(output.shape)\n","                shape[0] = -1  # Replace batch dimension with -1\n","                output_shapes[name] = f\"{shape}\"\n","        return hook\n","\n","    # 1. Determine the device where model parameters reside\n","    try:\n","        device = next(model.parameters()).device\n","    except StopIteration:\n","        device = torch.device(\"cpu\")  # Fallback for models without parameters\n","\n","    # 2. Create a dummy input tensor with batch_size=1\n","    dummy_input = torch.randn(1, *input_size).to(device)\n","\n","    # 3. Register forward hooks on target layers\n","    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n","    for name, module in model.named_children():\n","        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n","            # Register the hook and store its handle for cleanup\n","            hook_handle = module.register_forward_hook(get_hook(name))\n","            hooks.append(hook_handle)\n","\n","    # 4. Execute a dummy forward pass in evaluation mode\n","    model.eval()\n","    with torch.no_grad():\n","        try:\n","            model(dummy_input)\n","        except Exception as e:\n","            print(f\"Error during dummy forward pass: {e}\")\n","            # Clean up hooks even if an error occurs\n","            for h in hooks:\n","                h.remove()\n","            return\n","\n","    # 5. Remove all registered hooks\n","    for h in hooks:\n","        h.remove()\n","\n","    # --- 6. Print the summary table ---\n","\n","    print(\"-\" * 79)\n","    # Column headers\n","    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n","    print(\"=\" * 79)\n","\n","    total_params = 0\n","    total_trainable_params = 0\n","\n","    # Iterate through modules again to collect and display parameter information\n","    for name, module in model.named_children():\n","        if name in output_shapes:\n","            # Count total and trainable parameters for this module\n","            module_params = sum(p.numel() for p in module.parameters())\n","            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n","\n","            total_params += module_params\n","            total_trainable_params += trainable_params\n","\n","            # Format strings for display\n","            layer_name = f\"{name} ({type(module).__name__})\"\n","            output_shape_str = str(output_shapes[name])\n","            params_str = f\"{trainable_params:,}\"\n","\n","            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n","\n","    print(\"=\" * 79)\n","    print(f\"Total params: {total_params:,}\")\n","    print(f\"Trainable params: {total_trainable_params:,}\")\n","    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n","    print(\"-\" * 79)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fV2C-AdU0OHL"},"outputs":[],"source":["class RecurrentForecaster(nn.Module):\n","    \"\"\"\n","    Generic RNN model (RNN, LSTM, GRU) for forecasting.\n","    Uses the last hidden state of the RNN to predict a future sequence.\n","    (Many-to-One -> One-to-Many approach)\n","    \"\"\"\n","    def __init__(\n","            self,\n","            input_size,       # Number of features in the input (e.g., 6 for air quality)\n","            hidden_size,      # Hidden state size\n","            num_layers,       # Number of stacked RNN layers\n","            output_timesteps, # Number of future timesteps to predict (e.g., 12)\n","            output_features,  # Number of features to predict (e.g., 1 for 'PM2.5' only, 6 for all)\n","            rnn_type='GRU',   # 'RNN', 'LSTM', or 'GRU'\n","            bidirectional=False,\n","            dropout_rate=0.2\n","            ):\n","        super().__init__()\n","\n","        self.rnn_type = rnn_type\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.bidirectional = bidirectional\n","        self.output_timesteps = output_timesteps\n","        self.output_features = output_features\n","\n","        # Map string name to PyTorch RNN class\n","        rnn_map = {\n","            'RNN': nn.RNN,\n","            'LSTM': nn.LSTM,\n","            'GRU': nn.GRU\n","        }\n","\n","        if rnn_type not in rnn_map:\n","            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n","\n","        rnn_module = rnn_map[rnn_type]\n","\n","        # Dropout is only applied between layers (if num_layers > 1)\n","        dropout_val = dropout_rate if num_layers > 1 else 0\n","\n","        # --- Recurrent Block (GRU) ---\n","        self.rnn = rnn_module(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,      # Input shape: (batch, seq_len, features)\n","            bidirectional=bidirectional,\n","            dropout=dropout_val\n","        )\n","\n","        # Calculate input size for the final MLP\n","        if self.bidirectional:\n","            mlp_input_size = hidden_size * 2 # Concat fwd + bwd\n","        else:\n","            mlp_input_size = hidden_size\n","\n","        # --- MLP Block (Forecaster) ---\n","        # This layer projects the last hidden state onto the entire future sequence\n","        self.forecaster = nn.Linear(mlp_input_size, output_timesteps * output_features)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x shape: (batch_size, seq_length, input_size)\n","        \"\"\"\n","\n","        # rnn_out shape: (batch_size, seq_len, hidden_size * num_directions)\n","        # hidden shape: (num_layers * num_directions, batch_size, hidden_size)\n","        rnn_out, hidden = self.rnn(x)\n","\n","        # If it's an LSTM, hidden is a tuple (h_n, c_n), we only need h_n\n","        if self.rnn_type == 'LSTM':\n","            hidden = hidden[0]\n","\n","        # Extract the last hidden state from the last layer\n","        if self.bidirectional:\n","            # Reshape to (num_layers, 2, batch_size, hidden_size)\n","            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n","            # Concatenate the last fwd (hidden[-1, 0, ...]) and bwd (hidden[-1, 1, ...])\n","            # Final shape: (batch_size, hidden_size * 2)\n","            rnn_last_state = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n","        else:\n","            # Take the hidden state from the last layer\n","            # Final shape: (batch_size, hidden_size)\n","            rnn_last_state = hidden[-1]\n","\n","        # --- Projection to Future ---\n","        # Pass the last state (the \"summary\" of the sequence) through the MLP\n","        # prediction shape: (batch_size, output_timesteps * output_features)\n","        prediction = self.forecaster(rnn_last_state)\n","\n","        # Reshape the output to match the target (e.g., [B, 12, 1] or [B, 12, 6])\n","        batch_size = x.size(0)\n","        prediction = prediction.view(batch_size, self.output_timesteps, self.output_features)\n","\n","        return prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFWAHMFl0OHL"},"outputs":[],"source":["forecast_model = RecurrentForecaster(\n","    input_size=input_shape[1],              # Number of features in the input\n","    hidden_size=128,                        # Equivalent to 128 units in Keras\n","    num_layers=1,                           # Equivalent to the two LSTM layers\n","    output_timesteps=output_shape[0],\n","    output_features=output_shape[1],        # We predict all features\n","    dropout_rate=0.2,                       # Dropout between GRU layers (if num_layers > 1)\n","    rnn_type='LSTM',\n","    bidirectional=False\n",").to(device)\n","\n","# Show summary (requires torchinfo)\n","# (batch_size, seq_len, input_size)\n","recurrent_summary(forecast_model, input_size=(input_shape[0], input_shape[1]))"]},{"cell_type":"markdown","metadata":{"id":"Gnoe30SU0OHL"},"source":["## üßÆ **Network and Training Hyperparameters**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAlsr6lp0OHL"},"outputs":[],"source":["# Training configuration\n","LEARNING_RATE = 1e-3\n","EPOCHS = 500\n","PATIENCE = 50\n","\n","# Architecture\n","HIDDEN_LAYERS = 1        # Hidden layers\n","HIDDEN_SIZE = 256        # Neurons per layer\n","MODEL_TYPE = 'LSTM'      # RNN, LSTM, or GRU\n","BIDIRECTIONAL = False    # Use bidirectional RNN layers\n","\n","# Regularisation\n","DROPOUT_RATE = 0         # Dropout probability\n","L1_LAMBDA = 0            # L1 penalty\n","L2_LAMBDA = 0            # L2 penalty\n","\n","# Set up loss function and optimizer\n","criterion = nn.MSELoss()"]},{"cell_type":"markdown","metadata":{"id":"PWFUzIMw0OHL"},"source":["## üß† **Model Training**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXXnGRua0OHL"},"outputs":[],"source":["def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n","    \"\"\"\n","    Perform one complete training epoch through the entire training dataset.\n","    Adapted for Regression (Forecasting).\n","    Calculates and reports RMSE.\n","    Optimizes on the provided criterion (assumed to be MSELoss).\n","\n","    Args:\n","        model (nn.Module): The neural network model to train\n","        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n","        criterion (nn.Module): Loss function (e.g., MSELoss)\n","        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n","        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","        l1_lambda (float): Lambda for L1 regularization\n","        l2_lambda (float): Lambda for L2 regularization\n","\n","    Returns:\n","        float: average_rmse - Training RMSE for this epoch\n","    \"\"\"\n","    model.train()  # Set model to training mode\n","\n","    running_mse_loss = 0.0\n","\n","    # Iterate through training batches\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        # Move data to device (GPU/CPU)\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Clear gradients from previous step\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        # Forward pass with mixed precision (if CUDA available)\n","        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","            predictions = model(inputs)\n","            # Calculate the loss (e.G., MSE)\n","            loss = criterion(predictions, targets)\n","\n","            # Add L1 and L2 regularization\n","            l1_norm = sum(p.abs().sum() for p in model.parameters())\n","            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n","            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n","\n","        # Backward pass with gradient scaling\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # Accumulate metrics\n","        # We store the *squared* error (MSE) from the loss function\n","        running_mse_loss += loss.item() * inputs.size(0)\n","\n","    # Calculate epoch metrics\n","    epoch_mse = running_mse_loss / len(train_loader.dataset)\n","    epoch_rmse = np.sqrt(epoch_mse) # Convert final MSE to RMSE for reporting\n","\n","    return epoch_rmse\n","\n","\n","def validate_one_epoch(model, val_loader, criterion, device):\n","    \"\"\"\n","    Perform one complete validation epoch through the entire validation dataset.\n","    Adapted for Regression (Forecasting).\n","    Calculates and reports RMSE.\n","\n","    Args:\n","        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n","        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n","        criterion (nn.Module): Loss function used to calculate validation loss (e.g., MSELoss)\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","\n","    Returns:\n","        float: average_rmse - Validation RMSE for this epoch\n","    \"\"\"\n","    model.eval()  # Set model to evaluation mode\n","\n","    running_mse_loss = 0.0\n","\n","    # Disable gradient computation for validation\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            # Move data to device\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            # Forward pass with mixed precision (if CUDA available)\n","            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","                predictions = model(inputs)\n","                # Calculate the loss (e.g., MSE)\n","                loss = criterion(predictions, targets)\n","\n","            # Accumulate metrics\n","            running_mse_loss += loss.item() * inputs.size(0)\n","\n","    # Calculate epoch metrics\n","    epoch_mse = running_mse_loss / len(val_loader.dataset)\n","    epoch_rmse = np.sqrt(epoch_mse) # Convert final MSE to RMSE for reporting\n","\n","    return epoch_rmse\n","\n","\n","def log_metrics_to_tensorboard(writer, epoch, train_rmse, val_rmse, model):\n","    \"\"\"\n","    Log training metrics and model parameters to TensorBoard for visualization.\n","    Adapted for Regression (Forecasting) metrics (RMSE only).\n","\n","    Args:\n","        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n","        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n","        train_rmse (float): Training RMSE for this epoch\n","        val_rmse (float): Validation RMSE for this epoch\n","        model (nn.Module): The neural network model (for logging weights/gradients)\n","    \"\"\"\n","    # Log scalar metrics\n","    writer.add_scalar('RMSE/Training', train_rmse, epoch)\n","    writer.add_scalar('RMSE/Validation', val_rmse, epoch)\n","\n","    # Log model parameters and gradients\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            # Check if the tensor is not empty before adding a histogram\n","            if param.numel() > 0:\n","                writer.add_histogram(f'{name}/weights', param.data, epoch)\n","            if param.grad is not None:\n","                # Check if the gradient tensor is not empty before adding a histogram\n","                if param.grad.numel() > 0:\n","                    if torch.isfinite(param.grad).all():\n","                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)\n","\n","\n","def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n","        l1_lambda=0, l2_lambda=0, patience=0, scheduler=None, # Added scheduler parameter\n","        evaluation_metric=\"val_rmse\", mode='min', # Monitors val_rmse and minimizes\n","        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n","    \"\"\"\n","    Train the neural network model on the training data and validate on the validation data.\n","    Adapted for Regression (Forecasting), using RMSE as the sole metric.\n","\n","    Args:\n","        model (nn.Module): The neural network model to train\n","        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n","        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n","        epochs (int): Number of training epochs\n","        criterion (nn.Module): Loss function (e.g., MSELoss)\n","        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n","        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","        l1_lambda (float): L1 regularization coefficient (default: 0)\n","        l2_lambda (float): L2 regularization coefficient (default: 0)\n","        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n","        scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler (default: None)\n","        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_rmse\")\n","        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'min')\n","        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n","        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n","        verbose (int, optional): Frequency of printing training progress (default: 10)\n","        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n","\n","    Returns:\n","        tuple: (model, training_history) - Trained model and metrics history\n","    \"\"\"\n","\n","    # Ensure models directory exists\n","    if (patience > 0 or restore_best_weights) and not os.path.exists(\"models\"):\n","        os.makedirs(\"models\")\n","        print(\"Created 'models' directory for saving model checkpoints.\")\n","\n","    model_path = os.path.join(\"models\", f\"{experiment_name}_model.pt\")\n","\n","    # Initialize metrics tracking\n","    training_history = {\n","        'train_rmse': [], 'val_rmse': [], 'lr': []\n","    }\n","\n","    # Configure early stopping if patience is set\n","    if patience > 0:\n","        patience_counter = 0\n","        best_metric = float('inf') if mode == 'min' else float('-inf')\n","        best_epoch = 0\n","\n","    print(f\"Training {epochs} epochs...\")\n","\n","    # Main training loop: iterate through epochs\n","    for epoch in range(1, epochs + 1):\n","\n","        # Forward pass through training data, compute gradients, update weights\n","        train_rmse = train_one_epoch(\n","            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n","        )\n","\n","        # Evaluate model on validation data without updating weights\n","        val_rmse = validate_one_epoch(\n","            model, val_loader, criterion, device\n","        )\n","\n","        # Step the scheduler if provided (typically after validation)\n","        if scheduler is not None:\n","            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n","                scheduler.step(val_rmse)\n","            else:\n","                scheduler.step()\n","\n","\n","        # Store metrics for plotting and analysis\n","        training_history['train_rmse'].append(train_rmse)\n","        training_history['val_rmse'].append(val_rmse)\n","        training_history['lr'].append(optimizer.param_groups[0]['lr'])\n","\n","\n","        # Write metrics to TensorBoard for visualization\n","        if writer is not None:\n","            log_metrics_to_tensorboard(\n","                writer, epoch, train_rmse, val_rmse, model\n","            )\n","            # Log learning rate\n","            writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n","\n","\n","        # Print progress every N epochs or on first epoch\n","        if verbose > 0:\n","            if epoch % verbose == 0 or epoch == 1:\n","                print(f\"Epoch {epoch:3d}/{epochs} | \"\n","                      f\"Train: RMSE={train_rmse:.4f} | \"\n","                      f\"Val: RMSE={val_rmse:.4f} | \"\n","                      f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n","\n","\n","        # Early stopping logic: monitor metric and save best model\n","        if patience > 0:\n","            # We monitor the metric specified in 'evaluation_metric' (default: 'val_rmse')\n","            current_metric = training_history[evaluation_metric][-1]\n","            is_improvement = (current_metric < best_metric) if mode == 'min' else (current_metric > best_metric)\n","\n","            if is_improvement:\n","                best_metric = current_metric\n","                best_epoch = epoch\n","                torch.save(model.state_dict(), model_path)\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= patience:\n","                    print(f\"Early stopping triggered after {epoch} epochs.\")\n","                    break\n","\n","    # Restore best model weights if early stopping was used\n","    if restore_best_weights and patience > 0:\n","        try:\n","            model.load_state_dict(torch.load(model_path))\n","            print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n","        except FileNotFoundError:\n","            print(f\"Warning: Could not find best model checkpoint at {model_path}. Using last model.\")\n","\n","    # Save final model if no early stopping\n","    if patience == 0:\n","        torch.save(model.state_dict(), model_path)\n","\n","    # Close TensorBoard writer\n","    if writer is not None:\n","        writer.close()\n","\n","    return model, training_history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgVEh_mH0OHO"},"outputs":[],"source":["rnn_model = RecurrentForecaster(\n","    input_size=input_shape[1],\n","    hidden_size=HIDDEN_SIZE,\n","    num_layers=HIDDEN_LAYERS,\n","    output_timesteps=output_shape[0],\n","    output_features=output_shape[1],\n","    dropout_rate=DROPOUT_RATE,\n","    rnn_type=MODEL_TYPE,\n","    bidirectional=BIDIRECTIONAL\n",").to(device)\n","recurrent_summary(rnn_model, input_size=input_shape)\n","\n","# Set up TensorBoard logging and save model architecture\n","experiment_name = \"direct_lstm_forecaster_50\"\n","writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n","x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n","writer.add_graph(rnn_model, x)\n","\n","# Define optimizer with L2 regularization\n","optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n","\n","# Enable mixed precision training for GPU acceleration\n","scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVqKtK250OHO"},"outputs":[],"source":["%%time\n","# Train model and track training history\n","# Define the learning rate scheduler\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='min',        # Monitor a metric that should be minimized (RMSE)\n","    factor=0.1,        # Factor by which the learning rate will be reduced\n","    patience=max(10,PATIENCE//2),       # Number of epochs with no improvement after which learning rate will be reduced\n","    min_lr=1e-6        # Minimum learning rate\n",")\n","\n","rnn_model, training_history = fit(\n","    model=rnn_model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    epochs=EPOCHS,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scaler=scaler,\n","    device=device,\n","    l1_lambda=L1_LAMBDA,\n","    l2_lambda=L2_LAMBDA,\n","    patience=PATIENCE,\n","    evaluation_metric=\"val_rmse\",\n","    mode='min',\n","    restore_best_weights=True,\n","    writer=writer,  # Set to writer if you want TensorBoard logging\n","    verbose=1,\n","    experiment_name=\"direct_lstm_forecaster_50\",\n","    scheduler=scheduler # Pass the scheduler to the fit function\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q12Y35sQ0OHO"},"outputs":[],"source":["# @title Plot History\n","# Create a figure with two side-by-side subplots (two columns)\n","fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 10))\n","\n","# Plot of training and validation RMSE\n","ax1.plot(training_history['train_rmse'], label='Training RMSE', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax1.plot(training_history['val_rmse'], label='Validation RMSE', alpha=0.9, color='#ff7f0e')\n","ax1.set_title('Root Mean Squared Error')\n","ax1.set_ylabel('RMSE')\n","ax1.legend()\n","ax1.grid(alpha=0.3)\n","\n","# Plot of Learning Rate\n","ax2.plot(training_history['lr'], label='Learning Rate', color='#1f77b4')\n","ax2.set_title('Learning Rate')\n","ax2.set_xlabel('Epoch')\n","ax2.set_ylabel('LR')\n","ax2.legend()\n","ax2.grid(alpha=0.3)\n","\n","# Adjust the layout and display the plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQtbJ1Xj0OHO"},"outputs":[],"source":["# Define a function to inspect multivariate predictions versus ground truth\n","def inspect_multivariate_prediction(X, y, pred, columns, telescope, idx=None):\n","    # Select a random index if none is provided\n","    if idx is None:\n","        idx = np.random.randint(0, len(X))\n","\n","    # Prepare prediction and ground truth by concatenating the last known point\n","    pred = np.concatenate([np.expand_dims(X[:, -1, :], axis=1), pred], axis=1)\n","    y = np.concatenate([np.expand_dims(X[:, -1, :], axis=1), y], axis=1)\n","\n","    # Create subplots for visualisation\n","    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17, 10))\n","\n","    # Iterate through each column and plot actual, true, and predicted values\n","    for i, col in enumerate(columns):\n","        axs[i].plot(np.arange(len(X[0, :, i])), X[idx, :, i])  # Plot historical data\n","        axs[i].plot(np.arange(len(X[0, :, i]) - 1, len(X[0, :, i]) + telescope),\n","                    y[idx, :, i], color='orange', label='Ground Truth')  # Plot ground truth\n","        axs[i].plot(np.arange(len(X[0, :, i]) - 1, len(X[0, :, i]) + telescope),\n","                    pred[idx, :, i], color='green', label='Prediction')  # Plot predictions\n","        axs[i].set_title(col)\n","\n","    # Display the plots\n","    plt.show()\n","\n","# Make predictions on validation set\n","rnn_model.eval()\n","val_predictions = []\n","with torch.no_grad():\n","    for inputs, _ in val_loader:\n","        inputs = inputs.to(device)\n","        preds = rnn_model(inputs)\n","        val_predictions.append(preds.cpu().numpy())\n","\n","# Concatenate all predictions\n","val_predictions = np.concatenate(val_predictions, axis=0)\n","\n","# Calculate validation performance\n","val_mse = np.mean((val_predictions - y_val) ** 2)\n","val_rmse = np.sqrt(val_mse)\n","\n","print(f\"Validation Performance:\")\n","print(f\"  RMSE: {val_rmse:.6f}\")\n","\n","# Inspect multivariate predictions versus ground truth using the validation data\n","inspect_multivariate_prediction(X_val, y_val, val_predictions, TARGET_LABELS, direct_telescope)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLkgtmmD0OHO"},"outputs":[],"source":["# ========================================\n","# Predictions on test set using Direct Forecasting model\n","# ========================================\n","X_test_direct, y_test_direct = build_sequences(\n","    X_test_raw, TARGET_LABELS, WINDOW_SIZE, STRIDE, direct_telescope\n",")\n","test_ds_direct = TensorDataset(torch.from_numpy(X_test_direct), torch.from_numpy(y_test_direct))\n","test_loader_direct = make_loader(test_ds_direct, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n","\n","rnn_model.eval()\n","test_predictions = []\n","\n","with torch.no_grad():\n","    for inputs, _ in test_loader_direct:\n","        inputs = inputs.to(device)\n","        preds = rnn_model(inputs)\n","        test_predictions.append(preds.cpu().numpy())\n","\n","test_predictions = np.concatenate(test_predictions, axis=0)\n","\n","# ========================================\n","# Compute RMSE per timestep and feature\n","# ========================================\n","rmses = []\n","for i in range(test_predictions.shape[1]):\n","    ft_rmses = []\n","    for j in range(test_predictions.shape[2]):\n","        mse = np.mean((y_test_direct[:, i, j] - test_predictions[:, i, j]) ** 2)\n","        rmse = np.sqrt(mse)\n","        ft_rmses.append(rmse)\n","    rmses.append(np.array(ft_rmses))\n","\n","rmses = np.array(rmses)\n","\n","# ========================================\n","# Generate future predictions\n","# ========================================\n","future_predictions = rnn_model(torch.from_numpy(future).to(device))\n","future_predictions = future_predictions.cpu().detach().numpy()\n","\n","future_predictions = np.concatenate(\n","    [np.expand_dims(future[:, -1, :], axis=1), future_predictions],\n","    axis=1\n",")\n","\n","rmses = np.concatenate([np.array([[0, 0, 0]]), rmses], axis=0)\n","\n","# ========================================\n","# Visualize future predictions with uncertainty bounds\n","# ========================================\n","figs, axs = plt.subplots(len(TARGET_LABELS), 1, sharex=True, figsize=(17, 10))\n","\n","for i, col in enumerate(TARGET_LABELS):\n","    axs[i].plot(\n","        np.arange(len(future[0, :, i])),\n","        future[0, :, i],\n","        label='Historical',\n","        color='#1f77b4',\n","        linewidth=2\n","    )\n","\n","    axs[i].plot(\n","        np.arange(len(future[0, :, i]) - 1, len(future[0, :, i]) + direct_telescope),\n","        future_predictions[0, :, i],\n","        color='orange',\n","        label='Prediction',\n","        linewidth=2\n","    )\n","\n","    axs[i].fill_between(\n","        np.arange(len(future[0, :, i]) - 1, len(future[0, :, i]) + direct_telescope),\n","        future_predictions[0, :, i] + rmses[:, i],\n","        future_predictions[0, :, i] - rmses[:, i],\n","        color='orange',\n","        alpha=0.3,\n","        label='Uncertainty (¬±RMSE)'\n","    )\n","\n","    axs[i].set_title(col, fontsize=14, fontweight='bold')\n","    axs[i].set_ylim(0, 1)\n","    axs[i].grid(alpha=0.3)\n","    axs[i].legend(loc='upper left')\n","\n","plt.xlabel('Timestep', fontsize=12)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"se0ajIhe0OHO"},"source":["## üõ†Ô∏è **Multivariate Forecating (Autoregressive)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHI0Eqzg0OHP"},"outputs":[],"source":["# Define autoregressive telescope size for prediction horizon\n","autoregressive_telescope = 10\n","\n","assert autoregressive_telescope <= WINDOW_SIZE, \"Telescope size must be less than or equal to window size.\""]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"ruby"},"id":"hTo94F6F0OHP"},"outputs":[],"source":["# Build sequences and labels for training, validation, and test datasets\n","X_train, y_train = build_sequences(X_train_raw, TARGET_LABELS, WINDOW_SIZE, STRIDE, autoregressive_telescope)\n","X_val, y_val = build_sequences(X_val_raw, TARGET_LABELS, WINDOW_SIZE, 1, autoregressive_telescope)\n","X_test, y_test = build_sequences(X_test_raw, TARGET_LABELS, WINDOW_SIZE, 1, autoregressive_telescope)\n","\n","# Print the shapes of the resulting datasets\n","X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"ruby"},"id":"-q6-un0z0OHP"},"outputs":[],"source":["# Inspect multivariate sequences and predictions using the training data\n","inspect_multivariate(X_train, y_train, TARGET_LABELS, autoregressive_telescope)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"ruby"},"id":"pzMOrsbS0OHP"},"outputs":[],"source":["# Define the output shape based on the training labels\n","output_shape = y_train.shape[1:]\n","\n","# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n","train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n","test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n","\n","# Create data loaders with different settings for each phase\n","train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n","test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n","\n","# Get one batch from the training data loader\n","for xb, yb in train_loader:\n","    print(\"Features batch shape:\", xb.shape)\n","    print(\"Labels batch shape:\", yb.shape)\n","    break # Stop after getting one batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuJrcEyh0OHP"},"outputs":[],"source":["rnn_model = RecurrentForecaster(\n","    input_size=input_shape[1],  # Number of features in the input\n","    hidden_size=HIDDEN_SIZE,          # Equivalent to 128 units in Keras\n","    num_layers=HIDDEN_LAYERS,             # Equivalent to the two LSTM layers\n","    output_timesteps=output_shape[0],\n","    output_features=output_shape[1], # We predict all features\n","    dropout_rate=DROPOUT_RATE,         # Dropout between GRU layers (if num_layers > 1)\n","    rnn_type=MODEL_TYPE,\n","    bidirectional=BIDIRECTIONAL\n",").to(device)\n","recurrent_summary(rnn_model, input_size=input_shape)\n","\n","# Set up TensorBoard logging and save model architecture\n","experiment_name = \"autoregressive_lstm_forecaster_50\"\n","writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n","x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n","writer.add_graph(rnn_model, x)\n","\n","# Define optimizer with L2 regularization\n","optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n","\n","# Enable mixed precision training for GPU acceleration\n","scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AG05vMFU0OHP"},"outputs":[],"source":["%%time\n","# Train model and track training history\n","# Define the learning rate scheduler\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='min',        # Monitor a metric that should be minimized (RMSE)\n","    factor=0.1,        # Factor by which the learning rate will be reduced\n","    patience=max(10,PATIENCE//2),       # Number of epochs with no improvement after which learning rate will be reduced\n","    min_lr=1e-6        # Minimum learning rate\n",")\n","\n","rnn_model, training_history = fit(\n","    model=rnn_model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    epochs=EPOCHS,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scaler=scaler,\n","    device=device,\n","    l1_lambda=L1_LAMBDA,\n","    l2_lambda=L2_LAMBDA,\n","    patience=PATIENCE,\n","    evaluation_metric=\"val_rmse\",\n","    mode='min',\n","    restore_best_weights=True,\n","    writer=writer,  # Set to writer if you want TensorBoard logging\n","    verbose=1,\n","    experiment_name=\"autoregressive_lstm_forecaster_50\",\n","    scheduler=scheduler # Pass the scheduler to the fit function\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAZFNRQC0OHP"},"outputs":[],"source":["# @title Plot History\n","# Create a figure with two side-by-side subplots (two columns)\n","fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 10))\n","\n","# Plot of training and validation RMSE\n","ax1.plot(training_history['train_rmse'], label='Training RMSE', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax1.plot(training_history['val_rmse'], label='Validation RMSE', alpha=0.9, color='#ff7f0e')\n","ax1.set_title('Root Mean Squared Error')\n","ax1.set_ylabel('RMSE')\n","ax1.legend()\n","ax1.grid(alpha=0.3)\n","\n","# Plot of Learning Rate\n","ax2.plot(training_history['lr'], label='Learning Rate', color='#1f77b4')\n","ax2.set_title('Learning Rate')\n","ax2.set_xlabel('Epoch')\n","ax2.set_ylabel('LR')\n","ax2.legend()\n","ax2.grid(alpha=0.3)\n","\n","\n","# Adjust the layout and display the plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubGUdM7-0OHP"},"outputs":[],"source":["# Autoregressive prediction function\n","def autoregressive_predict(model, initial_sequence, num_steps, step_size, device):\n","    model.eval()\n","    predictions = []\n","    current_input = initial_sequence.clone()\n","\n","    with torch.no_grad():\n","        for _ in range(0, num_steps, step_size):\n","            pred = model(current_input)\n","            predictions.append(pred.cpu().numpy())\n","\n","            current_input = torch.cat([\n","                current_input[:, step_size:, :],\n","                pred\n","            ], dim=1)\n","\n","    all_predictions = np.concatenate(predictions, axis=1)\n","    return all_predictions[:, :num_steps, :]\n","\n","# Build validation sequences for 50-step prediction\n","X_val_direct, y_val_direct = build_sequences(\n","    X_val_raw,\n","    TARGET_LABELS,\n","    WINDOW_SIZE,\n","    STRIDE,\n","    direct_telescope\n",")\n","\n","# Perform autoregressive prediction\n","val_predictions = []\n","\n","for i in range(0, len(X_val_direct), BATCH_SIZE):\n","    batch_end = min(i + BATCH_SIZE, len(X_val_direct))\n","    inputs = torch.from_numpy(X_val_direct[i:batch_end]).to(device)\n","\n","    preds = autoregressive_predict(\n","        model=rnn_model,\n","        initial_sequence=inputs,\n","        num_steps=direct_telescope,\n","        step_size=autoregressive_telescope,\n","        device=device\n","    )\n","    val_predictions.append(preds)\n","\n","val_predictions = np.concatenate(val_predictions, axis=0)\n","\n","# Calculate metrics\n","mse = np.mean((val_predictions - y_val_direct) ** 2)\n","rmse = np.sqrt(mse)\n","\n","print(f\"Validation Performance:\")\n","print(f\"  RMSE: {rmse:.6f}\")\n","\n","# Visualize results\n","inspect_multivariate_prediction(\n","    X_val_direct,\n","    y_val_direct,\n","    val_predictions,\n","    TARGET_LABELS,\n","    direct_telescope\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-FtqfXF0OHP"},"outputs":[],"source":["# ========================================\n","# Predictions on test set using Autoregressive Forecasting model\n","# ========================================\n","X_test_direct, y_test_direct = build_sequences(\n","    X_test_raw, TARGET_LABELS, WINDOW_SIZE, STRIDE, direct_telescope\n",")\n","\n","# Perform autoregressive prediction\n","test_predictions = []\n","\n","for i in range(0, len(X_test_direct), BATCH_SIZE):\n","    batch_end = min(i + BATCH_SIZE, len(X_test_direct))\n","    inputs = torch.from_numpy(X_test_direct[i:batch_end]).to(device)\n","\n","    preds = autoregressive_predict(\n","        model=rnn_model,\n","        initial_sequence=inputs,\n","        num_steps=direct_telescope,\n","        step_size=autoregressive_telescope,\n","        device=device\n","    )\n","    test_predictions.append(preds)\n","\n","test_predictions = np.concatenate(test_predictions, axis=0)\n","\n","# ========================================\n","# Compute RMSE per timestep and feature\n","# ========================================\n","rmses = []\n","for i in range(test_predictions.shape[1]):\n","    ft_rmses = []\n","    for j in range(test_predictions.shape[2]):\n","        mse = np.mean((y_test_direct[:, i, j] - test_predictions[:, i, j]) ** 2)\n","        rmse = np.sqrt(mse)\n","        ft_rmses.append(rmse)\n","    rmses.append(np.array(ft_rmses))\n","\n","rmses = np.array(rmses)\n","\n","# ========================================\n","# Generate future predictions\n","# ========================================\n","future_predictions = autoregressive_predict(\n","    model=rnn_model,\n","    initial_sequence=torch.from_numpy(future).to(device),\n","    num_steps=direct_telescope,\n","    step_size=autoregressive_telescope,\n","    device=device\n",")\n","\n","future_predictions = np.concatenate(\n","    [np.expand_dims(future[:, -1, :], axis=1), future_predictions],\n","    axis=1\n",")\n","\n","rmses = np.concatenate([np.array([[0, 0, 0]]), rmses], axis=0)\n","\n","# ========================================\n","# Visualize future predictions with uncertainty bounds\n","# ========================================\n","figs, axs = plt.subplots(len(TARGET_LABELS), 1, sharex=True, figsize=(17, 10))\n","\n","for i, col in enumerate(TARGET_LABELS):\n","    axs[i].plot(\n","        np.arange(len(future[0, :, i])),\n","        future[0, :, i],\n","        label='Historical',\n","        color='#1f77b4',\n","        linewidth=2\n","    )\n","\n","    axs[i].plot(\n","        np.arange(len(future[0, :, i]) - 1, len(future[0, :, i]) + direct_telescope),\n","        future_predictions[0, :, i],\n","        color='orange',\n","        label='Prediction',\n","        linewidth=2\n","    )\n","\n","    axs[i].fill_between(\n","        np.arange(len(future[0, :, i]) - 1, len(future[0, :, i]) + direct_telescope),\n","        future_predictions[0, :, i] + rmses[:, i],\n","        future_predictions[0, :, i] - rmses[:, i],\n","        color='orange',\n","        alpha=0.3,\n","        label='Uncertainty (¬±RMSE)'\n","    )\n","\n","    axs[i].set_title(col, fontsize=14, fontweight='bold')\n","    axs[i].set_ylim(0, 1)\n","    axs[i].grid(alpha=0.3)\n","    axs[i].legend(loc='upper left')\n","\n","plt.xlabel('Timestep', fontsize=12)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6SceyvIJ0OHP"},"source":["## üõ†Ô∏è **Multivariate Sequence-To-Sequence Forecating (Autoregressive)**\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1v4slQEVlkAkDoRUH8q28ftdOs-fQ9vd-\" width=\"800\"/>\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zw2vwpbr0OHQ"},"outputs":[],"source":["# Build sequences and labels for training, validation, and test datasets\n","X_train, y_train = build_sequences(X_train_raw, TARGET_LABELS, WINDOW_SIZE, STRIDE, direct_telescope)\n","X_val, y_val = build_sequences(X_val_raw, TARGET_LABELS, WINDOW_SIZE, 1, direct_telescope)\n","X_test, y_test = build_sequences(X_test_raw, TARGET_LABELS, WINDOW_SIZE, 1, direct_telescope)\n","\n","# Print the shapes of the resulting datasets\n","X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8WuRzc70OHQ"},"outputs":[],"source":["# Define the output shape based on the training labels\n","output_shape = y_train.shape[1:]\n","\n","# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n","train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n","test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n","\n","# Create data loaders with different settings for each phase\n","train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n","test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n","\n","# Get one batch from the training data loader\n","for xb, yb in train_loader:\n","    print(\"Features batch shape:\", xb.shape)\n","    print(\"Labels batch shape:\", yb.shape)\n","    break # Stop after getting one batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g57EspDV0OHQ"},"outputs":[],"source":["class Seq2SeqForecaster(nn.Module):\n","    \"\"\"\n","    Sequence-to-Sequence model with attention for forecasting.\n","    Encoder and Decoder use the same RNN type (RNN, LSTM, or GRU).\n","    Trained with teacher forcing during training.\n","    \"\"\"\n","    def __init__(\n","            self,\n","            input_size,       # Number of features in the input\n","            hidden_size,      # Hidden state size\n","            num_layers,       # Number of stacked RNN layers\n","            output_timesteps, # Number of future timesteps to predict\n","            output_features,  # Number of features to predict\n","            rnn_type='GRU',   # 'RNN', 'LSTM', or 'GRU'\n","            dropout_rate=0.2,\n","            bidirectional=False  # Enable bidirectional RNN\n","            ):\n","        super().__init__()\n","\n","        self.rnn_type = rnn_type\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.output_timesteps = output_timesteps\n","        self.output_features = output_features\n","        self.bidirectional = bidirectional\n","        self.num_directions = 2 if bidirectional else 1\n","\n","        # Map string name to PyTorch RNN class\n","        rnn_map = {\n","            'RNN': nn.RNN,\n","            'LSTM': nn.LSTM,\n","            'GRU': nn.GRU\n","        }\n","\n","        if rnn_type not in rnn_map:\n","            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n","\n","        rnn_module = rnn_map[rnn_type]\n","        dropout_val = dropout_rate if num_layers > 1 else 0\n","\n","        # --- Encoder ---\n","        self.encoder = rnn_module(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout_val,\n","            bidirectional=bidirectional\n","        )\n","\n","        # Encoder output size (accounting for bidirectional)\n","        encoder_output_size = hidden_size * self.num_directions\n","\n","        # --- Attention Layer ---\n","        self.attention = nn.Linear(encoder_output_size + hidden_size, 1)\n","\n","        # --- Decoder (always unidirectional for autoregressive generation) ---\n","        self.decoder = rnn_module(\n","            input_size=output_features + encoder_output_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout_val,\n","            bidirectional=False\n","        )\n","\n","        # --- Output Layer ---\n","        self.fc_out = nn.Linear(hidden_size, output_features)\n","\n","        # Project bidirectional encoder hidden to decoder hidden if needed\n","        if bidirectional:\n","            self.hidden_projection = nn.Linear(hidden_size * 2, hidden_size)\n","\n","    def _project_encoder_hidden(self, hidden):\n","        \"\"\"\n","        Project bidirectional encoder hidden state to unidirectional decoder hidden state.\n","        \"\"\"\n","        if not self.bidirectional:\n","            return hidden\n","\n","        if self.rnn_type == 'LSTM':\n","            # hidden is tuple (h, c)\n","            h, c = hidden\n","            # h, c shape: (num_layers * 2, batch_size, hidden_size)\n","            # Reshape to (num_layers, 2, batch_size, hidden_size)\n","            h = h.view(self.num_layers, 2, -1, self.hidden_size)\n","            c = c.view(self.num_layers, 2, -1, self.hidden_size)\n","            # Concatenate forward and backward, then project\n","            h = self.hidden_projection(torch.cat([h[:, 0], h[:, 1]], dim=-1))\n","            c = self.hidden_projection(torch.cat([c[:, 0], c[:, 1]], dim=-1))\n","            return (h, c)\n","        else:\n","            # hidden shape: (num_layers * 2, batch_size, hidden_size)\n","            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n","            # Concatenate forward and backward, then project\n","            hidden = self.hidden_projection(torch.cat([hidden[:, 0], hidden[:, 1]], dim=-1))\n","            return hidden\n","\n","    def attention_mechanism(self, decoder_hidden, encoder_outputs):\n","        \"\"\"\n","        Compute attention weights and context vector.\n","        decoder_hidden: (batch_size, hidden_size) - current decoder hidden state\n","        encoder_outputs: (batch_size, seq_len, encoder_output_size) - all encoder outputs\n","        Returns: context vector (batch_size, encoder_output_size)\n","        \"\"\"\n","        batch_size, seq_len, encoder_output_size = encoder_outputs.size()\n","\n","        # Repeat decoder hidden state for all encoder time steps\n","        decoder_hidden_repeated = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n","\n","        # Concatenate decoder hidden with each encoder output\n","        combined = torch.cat([decoder_hidden_repeated, encoder_outputs], dim=2)\n","\n","        # Compute attention scores\n","        attention_scores = self.attention(combined)\n","\n","        # Apply softmax to get attention weights\n","        attention_weights = torch.softmax(attention_scores, dim=1)\n","\n","        # Compute context vector as weighted sum of encoder outputs\n","        context = torch.sum(attention_weights * encoder_outputs, dim=1)\n","\n","        return context\n","\n","    def forward(self, x, target=None, teacher_forcing_ratio=0.5):\n","        \"\"\"\n","        x: (batch_size, seq_length, input_size) - input sequence\n","        target: (batch_size, output_timesteps, output_features) - target sequence (for teacher forcing)\n","        teacher_forcing_ratio: probability of using teacher forcing during training\n","        \"\"\"\n","        batch_size = x.size(0)\n","\n","        # --- Encode ---\n","        encoder_outputs, hidden = self.encoder(x)\n","        # encoder_outputs: (batch_size, seq_len, hidden_size * num_directions)\n","\n","        # Project encoder hidden state to decoder hidden state\n","        decoder_hidden = self._project_encoder_hidden(hidden)\n","\n","        # --- Prepare decoder initial input ---\n","        decoder_input = x[:, -1:, :self.output_features]\n","\n","        # Initialize outputs tensor\n","        outputs = torch.zeros(batch_size, self.output_timesteps, self.output_features).to(x.device)\n","\n","        # --- Decode ---\n","        for t in range(self.output_timesteps):\n","            # Get decoder hidden state for attention\n","            if self.rnn_type == 'LSTM':\n","                decoder_hidden_state = decoder_hidden[0][-1]\n","            else:\n","                decoder_hidden_state = decoder_hidden[-1]\n","\n","            # Compute attention context\n","            context = self.attention_mechanism(decoder_hidden_state, encoder_outputs)\n","\n","            # Concatenate decoder input with context vector\n","            decoder_input_with_context = torch.cat([decoder_input, context.unsqueeze(1)], dim=2)\n","\n","            # Pass through decoder\n","            decoder_output, decoder_hidden = self.decoder(decoder_input_with_context, decoder_hidden)\n","\n","            # Generate prediction\n","            prediction = self.fc_out(decoder_output.squeeze(1))\n","            outputs[:, t, :] = prediction\n","\n","            # Teacher forcing\n","            if target is not None and torch.rand(1).item() < teacher_forcing_ratio:\n","                decoder_input = target[:, t:t+1, :]\n","            else:\n","                decoder_input = prediction.unsqueeze(1)\n","\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqcD7l220OHQ"},"outputs":[],"source":["def train_one_epoch_seq2seq(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0, teacher_forcing_ratio=0.5):\n","    \"\"\"\n","    Perform one complete training epoch through the entire training dataset.\n","    Adapted for Regression (Forecasting) with Seq2Seq and Teacher Forcing.\n","    Calculates and reports RMSE.\n","    Optimizes on the provided criterion (assumed to be MSELoss).\n","\n","    Args:\n","        model (nn.Module): The neural network model to train (seq2seq)\n","        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n","        criterion (nn.Module): Loss function (e.g., MSELoss)\n","        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n","        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","        l1_lambda (float): Lambda for L1 regularization\n","        l2_lambda (float): Lambda for L2 regularization\n","        teacher_forcing_ratio (float): Probability of using teacher forcing (0.0 to 1.0)\n","\n","    Returns:\n","        float: average_rmse - Training RMSE for this epoch\n","    \"\"\"\n","    model.train()  # Set model to training mode\n","\n","    running_mse_loss = 0.0\n","\n","    # Iterate through training batches\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        # Move data to device (GPU/CPU)\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Clear gradients from previous step\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        # Forward pass with mixed precision (if CUDA available)\n","        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","            # Pass targets for teacher forcing during training\n","            predictions = model(inputs, targets, teacher_forcing_ratio=teacher_forcing_ratio)\n","            # Calculate the loss (e.g., MSE)\n","            loss = criterion(predictions, targets)\n","\n","            # Add L1 and L2 regularization\n","            l1_norm = sum(p.abs().sum() for p in model.parameters())\n","            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n","            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n","\n","        # Backward pass with gradient scaling\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # Accumulate metrics\n","        # We store the *squared* error (MSE) from the loss function\n","        running_mse_loss += loss.item() * inputs.size(0)\n","\n","    # Calculate epoch metrics\n","    epoch_mse = running_mse_loss / len(train_loader.dataset)\n","    epoch_rmse = np.sqrt(epoch_mse) # Convert final MSE to RMSE for reporting\n","\n","    return epoch_rmse\n","\n","\n","def validate_one_epoch_seq2seq(model, val_loader, criterion, device):\n","    \"\"\"\n","    Perform one complete validation epoch through the entire validation dataset.\n","    Adapted for Regression (Forecasting) with Seq2Seq.\n","    Calculates and reports RMSE.\n","    No teacher forcing during validation.\n","\n","    Args:\n","        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n","        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n","        criterion (nn.Module): Loss function used to calculate validation loss (e.g., MSELoss)\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","\n","    Returns:\n","        float: average_rmse - Validation RMSE for this epoch\n","    \"\"\"\n","    model.eval()  # Set model to evaluation mode\n","\n","    running_mse_loss = 0.0\n","\n","    # Disable gradient computation for validation\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            # Move data to device\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            # Forward pass with mixed precision (if CUDA available)\n","            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","                # No teacher forcing during validation (teacher_forcing_ratio=0.0)\n","                predictions = model(inputs, teacher_forcing_ratio=0.0)\n","                # Calculate the loss (e.g., MSE)\n","                loss = criterion(predictions, targets)\n","\n","            # Accumulate metrics\n","            running_mse_loss += loss.item() * inputs.size(0)\n","\n","    # Calculate epoch metrics\n","    epoch_mse = running_mse_loss / len(val_loader.dataset)\n","    epoch_rmse = np.sqrt(epoch_mse) # Convert final MSE to RMSE for reporting\n","\n","    return epoch_rmse\n","\n","\n","def fit_seq2seq(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n","        l1_lambda=0, l2_lambda=0, patience=0, teacher_forcing_ratio=0.5, scheduler=None, # Added scheduler parameter\n","        evaluation_metric=\"val_rmse\", mode='min', # Monitors val_rmse and minimizes\n","        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n","    \"\"\"\n","    Train the neural network model on the training data and validate on the validation data.\n","    Adapted for Regression (Forecasting) with Seq2Seq and Teacher Forcing, using RMSE as the sole metric.\n","\n","    Args:\n","        model (nn.Module): The neural network model to train (seq2seq)\n","        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n","        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n","        epochs (int): Number of training epochs\n","        criterion (nn.Module): Loss function (e.g., MSELoss)\n","        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n","        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","        l1_lambda (float): L1 regularization coefficient (default: 0)\n","        l2_lambda (float): L2 regularization coefficient (default: 0)\n","        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n","        teacher_forcing_ratio (float): Probability of using teacher forcing during training (default: 0.5)\n","        scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler (default: None)\n","        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_rmse\")\n","        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'min')\n","        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n","        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n","        verbose (int, optional): Frequency of printing training progress (default: 10)\n","        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n","\n","    Returns:\n","        tuple: (model, training_history) - Trained model and metrics history\n","    \"\"\"\n","\n","    # Ensure models directory exists\n","    if (patience > 0 or restore_best_weights) and not os.path.exists(\"models\"):\n","        os.makedirs(\"models\")\n","        print(\"Created 'models' directory for saving model checkpoints.\")\n","\n","    model_path = os.path.join(\"models\", f\"{experiment_name}_model.pt\")\n","\n","    # Initialize metrics tracking\n","    training_history = {\n","        'train_rmse': [], 'val_rmse': []\n","    }\n","\n","    # Configure early stopping if patience is set\n","    if patience > 0:\n","        patience_counter = 0\n","        best_metric = float('inf') if mode == 'min' else float('-inf')\n","        best_epoch = 0\n","\n","    print(f\"Training {epochs} epochs with teacher forcing ratio: {teacher_forcing_ratio}...\")\n","\n","    # Main training loop: iterate through epochs\n","    for epoch in range(1, epochs + 1):\n","\n","        # Forward pass through training data, compute gradients, update weights\n","        train_rmse = train_one_epoch_seq2seq(\n","            model, train_loader, criterion, optimizer, scaler, device,\n","            l1_lambda, l2_lambda, teacher_forcing_ratio\n","        )\n","\n","        # Evaluate model on validation data without updating weights\n","        val_rmse = validate_one_epoch_seq2seq(\n","            model, val_loader, criterion, device\n","        )\n","\n","        # Step the scheduler if provided (typically after validation)\n","        if scheduler is not None:\n","            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n","                scheduler.step(val_rmse)\n","            else:\n","                scheduler.step()\n","\n","        # Store metrics for plotting and analysis\n","        training_history['train_rmse'].append(train_rmse)\n","        training_history['val_rmse'].append(val_rmse)\n","\n","        # Write metrics to TensorBoard for visualization\n","        if writer is not None:\n","            log_metrics_to_tensorboard(\n","                writer, epoch, train_rmse, val_rmse, model\n","            )\n","            # Log learning rate\n","            writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n","\n","\n","        # Print progress every N epochs or on first epoch\n","        if verbose > 0:\n","            if epoch % verbose == 0 or epoch == 1:\n","                print(f\"Epoch {epoch:3d}/{epochs} | \"\n","                      f\"Train: RMSE={train_rmse:.4f} | \"\n","                      f\"Val: RMSE={val_rmse:.4f} | \"\n","                      f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n","\n","        # Early stopping logic: monitor metric and save best model\n","        if patience > 0:\n","            # We monitor the metric specified in 'evaluation_metric' (default: 'val_rmse')\n","            current_metric = training_history[evaluation_metric][-1]\n","            is_improvement = (current_metric < best_metric) if mode == 'min' else (current_metric > best_metric)\n","\n","            if is_improvement:\n","                best_metric = current_metric\n","                best_epoch = epoch\n","                torch.save(model.state_dict(), model_path)\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= patience:\n","                    print(f\"Early stopping triggered after {epoch} epochs.\")\n","                    break\n","\n","    # Restore best model weights if early stopping was used\n","    if restore_best_weights and patience > 0:\n","        try:\n","            model.load_state_dict(torch.load(model_path))\n","            print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n","        except FileNotFoundError:\n","            print(f\"Warning: Could not find best model checkpoint at {model_path}. Using last model.\")\n","\n","    # Save final model if no early stopping\n","    if patience == 0:\n","        torch.save(model.state_dict(), model_path)\n","\n","    # Close TensorBoard writer\n","    if writer is not None:\n","        writer.close()\n","\n","    return model, training_history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJEEJSRE0OHQ"},"outputs":[],"source":["seq2seq_model = Seq2SeqForecaster(\n","    input_size=input_shape[1],  # Number of features in the input\n","    hidden_size=HIDDEN_SIZE,          # Equivalent to 128 units in Keras\n","    num_layers=HIDDEN_LAYERS,             # Equivalent to the two LSTM layers\n","    output_timesteps=output_shape[0],\n","    output_features=output_shape[1], # We predict all features\n","    dropout_rate=DROPOUT_RATE,         # Dropout between LSTM layers (if num_layers > 1)\n","    rnn_type=MODEL_TYPE,\n","    bidirectional=BIDIRECTIONAL\n",").to(device)\n","recurrent_summary(seq2seq_model, input_size=input_shape)\n","\n","# Set up TensorBoard logging and save model architecture\n","experiment_name = \"seq2seq_lstm_forecaster\"\n","writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n","x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n","writer.add_graph(seq2seq_model, x)\n","\n","# Define optimizer with L2 regularization\n","optimizer = torch.optim.AdamW(seq2seq_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n","\n","# Enable mixed precision training for GPU acceleration\n","scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1CX_Wcs0OHQ"},"outputs":[],"source":["%%time\n","# Train model and track training history\n","# Define the learning rate scheduler\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='min',        # Monitor a metric that should be minimized (RMSE)\n","    factor=0.1,        # Factor by which the learning rate will be reduced\n","    patience=max(10,PATIENCE//2),       # Number of epochs with no improvement after which learning rate will be reduced\n","    min_lr=1e-6        # Minimum learning rate\n",")\n","\n","seq2seq_model, training_history = fit_seq2seq(\n","    model=seq2seq_model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    epochs=EPOCHS,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scaler=scaler,\n","    device=device,\n","    l1_lambda=L1_LAMBDA,\n","    l2_lambda=L2_LAMBDA,\n","    patience=PATIENCE,\n","    evaluation_metric=\"val_rmse\",\n","    mode='min',\n","    restore_best_weights=True,\n","    writer=writer,  # Set to writer if you want TensorBoard logging\n","    verbose=1,\n","    experiment_name=\"seq2seq_lstm_forecaster\",\n","    teacher_forcing_ratio=0.5,\n","    scheduler=scheduler # Pass the scheduler to the fit function\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhWTYINn0OHR"},"outputs":[],"source":["# @title Plot History\n","# Create a figure with two side-by-side subplots (two columns)\n","fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(12, 5))\n","\n","# Plot of training and validation RMSE\n","ax1.plot(training_history['train_rmse'], label='Training RMSE', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax1.plot(training_history['val_rmse'], label='Validation RMSE', alpha=0.9, color='#ff7f0e')\n","ax1.set_title('Root Mean Squared Error')\n","ax1.set_xlabel('Epoch')\n","ax1.set_ylabel('RMSE')\n","ax1.legend()\n","ax1.grid(alpha=0.3)\n","\n","# Adjust the layout and display the plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ETU1cJi0OHR"},"outputs":[],"source":["# Define a function to inspect multivariate predictions versus ground truth\n","def inspect_multivariate_prediction(X, y, pred, columns, telescope, idx=None):\n","    # Select a random index if none is provided\n","    if idx is None:\n","        idx = np.random.randint(0, len(X))\n","\n","    # Prepare prediction and ground truth by concatenating the last known point\n","    pred = np.concatenate([np.expand_dims(X[:, -1, :], axis=1), pred], axis=1)\n","    y = np.concatenate([np.expand_dims(X[:, -1, :], axis=1), y], axis=1)\n","\n","    # Create subplots for visualisation\n","    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17, 10))\n","\n","    # Iterate through each column and plot actual, true, and predicted values\n","    for i, col in enumerate(columns):\n","        axs[i].plot(np.arange(len(X[0, :, i])), X[idx, :, i])  # Plot historical data\n","        axs[i].plot(np.arange(len(X[0, :, i]) - 1, len(X[0, :, i]) + telescope),\n","                    y[idx, :, i], color='orange', label='Ground Truth')  # Plot ground truth\n","        axs[i].plot(np.arange(len(X[0, :, i]) - 1, len(X[0, :, i]) + telescope),\n","                    pred[idx, :, i], color='green', label='Prediction')  # Plot predictions\n","        axs[i].set_title(col)\n","\n","    # Display the plots\n","    plt.show()\n","\n","# Make predictions on validation set\n","seq2seq_model.eval()\n","val_predictions = []\n","with torch.no_grad():\n","    for inputs, _ in val_loader:\n","        inputs = inputs.to(device)\n","        preds = seq2seq_model(inputs)\n","        val_predictions.append(preds.cpu().numpy())\n","\n","# Concatenate all predictions\n","val_predictions = np.concatenate(val_predictions, axis=0)\n","\n","# Calculate validation performance\n","val_mse = np.mean((val_predictions - y_val) ** 2)\n","val_rmse = np.sqrt(val_mse)\n","\n","print(f\"Validation Performance:\")\n","print(f\"  RMSE: {val_rmse:.6f}\")\n","\n","# Inspect multivariate predictions versus ground truth using the validation data\n","inspect_multivariate_prediction(X_val, y_val, val_predictions, TARGET_LABELS, direct_telescope)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sWqLV_5z0OHR"},"outputs":[],"source":["# ========================================\n","# Predictions on test set using Direct Forecasting model\n","# ========================================\n","X_test_direct, y_test_direct = build_sequences(\n","    X_test_raw, TARGET_LABELS, WINDOW_SIZE, STRIDE, direct_telescope\n",")\n","test_ds_direct = TensorDataset(torch.from_numpy(X_test_direct), torch.from_numpy(y_test_direct))\n","test_loader_direct = make_loader(test_ds_direct, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n","\n","seq2seq_model.eval()\n","test_predictions = []\n","\n","with torch.no_grad():\n","    for inputs, _ in test_loader_direct:\n","        inputs = inputs.to(device)\n","        preds = seq2seq_model(inputs)\n","        test_predictions.append(preds.cpu().numpy())\n","\n","test_predictions = np.concatenate(test_predictions, axis=0)\n","\n","# ========================================\n","# Compute RMSE per timestep and feature\n","# ========================================\n","rmses = []\n","for i in range(test_predictions.shape[1]):\n","    ft_rmses = []\n","    for j in range(test_predictions.shape[2]):\n","        mse = np.mean((y_test_direct[:, i, j] - test_predictions[:, i, j]) ** 2)\n","        rmse = np.sqrt(mse)\n","        ft_rmses.append(rmse)\n","    rmses.append(np.array(ft_rmses))\n","\n","rmses = np.array(rmses)\n","\n","# ========================================\n","# Generate future predictions\n","# ========================================\n","future_predictions = seq2seq_model(torch.from_numpy(future).to(device))\n","future_predictions = future_predictions.cpu().detach().numpy()\n","\n","future_predictions = np.concatenate(\n","    [np.expand_dims(future[:, -1, :], axis=1), future_predictions],\n","    axis=1\n",")\n","\n","rmses = np.concatenate([np.array([[0, 0, 0]]), rmses], axis=0)\n","\n","# ========================================\n","# Visualize future predictions with uncertainty bounds\n","# ========================================\n","figs, axs = plt.subplots(len(TARGET_LABELS), 1, sharex=True, figsize=(17, 10))\n","\n","for i, col in enumerate(TARGET_LABELS):\n","    axs[i].plot(\n","        np.arange(len(future[0, :, i])),\n","        future[0, :, i],\n","        label='Historical',\n","        color='#1f77b4',\n","        linewidth=2\n","    )\n","\n","    axs[i].plot(\n","        np.arange(len(future[0, :, i]) - 1, len(future[0, :, i]) + direct_telescope),\n","        future_predictions[0, :, i],\n","        color='orange',\n","        label='Prediction',\n","        linewidth=2\n","    )\n","\n","    axs[i].fill_between(\n","        np.arange(len(future[0, :, i]) - 1, len(future[0, :, i]) + direct_telescope),\n","        future_predictions[0, :, i] + rmses[:, i],\n","        future_predictions[0, :, i] - rmses[:, i],\n","        color='orange',\n","        alpha=0.3,\n","        label='Uncertainty (¬±RMSE)'\n","    )\n","\n","    axs[i].set_title(col, fontsize=14, fontweight='bold')\n","    axs[i].set_ylim(0, 1)\n","    axs[i].grid(alpha=0.3)\n","    axs[i].legend(loc='upper left')\n","\n","plt.xlabel('Timestep', fontsize=12)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","source":["# Copy TensorBoard logs to accessible location for Colab\n","!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n","\n","# Launch TensorBoard interface\n","%tensorboard --logdir \"/content/\"$logs_dir"],"metadata":{"id":"A5kKuRW6088K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?export=view&id=1gWxnQ1RkH9j9zX_iPUcG4cP_MOfo7PgR\" width=\"700\"/>"],"metadata":{"id":"beswvp9c0XQV"}},{"cell_type":"markdown","metadata":{"id":"Z9F9n52Up53Y"},"source":["#  \n","<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n","\n","##### Connect with us:\n","- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"14\"> **LinkedIn:**  [AIRLab Polimi](https://www.linkedin.com/company/airlab-polimi/)\n","- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"14\"> **Instagram:** [airlab_polimi](https://www.instagram.com/airlab_polimi/)\n","\n","##### Contributors:\n","- **Eugenio Lomurno**: eugenio.lomurno@polimi.it\n","- **Alberto Archetti**: alberto.archetti@polimi.it\n","- **Roberto Basla**: roberto.basla@polimi.it\n","- **Carlo Sgaravatti**: carlo.sgaravatti@polimi.it\n","\n","```\n","   Copyright 2025 Eugenio Lomurno, Alberto Archetti, Roberto Basla, Carlo Sgaravatti\n","\n","   Licensed under the Apache License, Version 2.0 (the \"License\");\n","   you may not use this file except in compliance with the License.\n","   You may obtain a copy of the License at\n","\n","       http://www.apache.org/licenses/LICENSE-2.0\n","\n","   Unless required by applicable law or agreed to in writing, software\n","   distributed under the License is distributed on an \"AS IS\" BASIS,\n","   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","   See the License for the specific language governing permissions and\n","   limitations under the License.\n","```\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.12"}},"nbformat":4,"nbformat_minor":0}