{"cells":[{"cell_type":"markdown","metadata":{"id":"X5i0sMFDr5jO"},"source":["# **Artificial Neural Networks and Deep Learning**\n","\n","---\n","\n","## **Lecture 2: Overfitting and Regularisation**\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1gCCkumuvndbK6kgaVJwy4KyBnMEVhmXK\" width=\"500\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"omSLbdLvhDRx"},"source":["## üåê **Google Drive Connection**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AoaLQpvChLpb"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/gdrive\")\n","current_dir = \"/gdrive/My\\\\ Drive/[2025-2026]\\\\ AN2DL/Lecture\\\\ 2\"\n","%cd $current_dir"]},{"cell_type":"markdown","metadata":{"id":"l3FoTyRa9pLu"},"source":["## ‚öôÔ∏è **Libraries Import**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_sOaV1Y8NsL"},"outputs":[],"source":["# Set seed for reproducibility\n","SEED = 42\n","\n","# Import necessary libraries\n","import os\n","\n","# Set environment variables before importing modules\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n","\n","# Suppress warnings\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","# Import necessary modules\n","import logging\n","import random\n","import numpy as np\n","\n","# Set seeds for random number generators in NumPy and Python\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","# Import PyTorch\n","import torch\n","torch.manual_seed(SEED)\n","from torch import nn\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import TensorDataset, DataLoader\n","logs_dir = \"tensorboard\"\n","!pkill -f tensorboard\n","%load_ext tensorboard\n","!mkdir -p models\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.benchmark = True\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Device: {device}\")\n","\n","# Import other libraries\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","\n","# Configure plot display settings\n","sns.set(font_scale=1.4)\n","sns.set_style('white')\n","plt.rc('font', size=14)\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"TCaGhb5ttXGc"},"source":["## ‚è≥ **Data Loading**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxEwCjfpt_6W"},"outputs":[],"source":["# Load the Glass dataset from CSV file\n","os.environ[\"DATASET_NAME\"] = \"glass_dataset.csv\"\n","os.environ[\"DATASET_URL\"] = \"1xyZvjIw2nR5QtlfN9vumuPUjr3SFZLob\"\n","if not os.path.exists(os.environ[\"DATASET_NAME\"]):\n","    print(\"Downloading data...\")\n","    ! gdown -q ${DATASET_URL}\n","    print(\"Download completed\")\n","else:\n","    print(\"Data already downloaded. Using cached data...\")\n","data = pd.read_csv('glass_dataset.csv')"]},{"cell_type":"markdown","metadata":{"id":"kTu9Ca0L1Du2"},"source":["## üîé **Exploration and Data Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P485pmX1uvBf"},"outputs":[],"source":["# Display the first 10 rows of the Glass dataset\n","data.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBZgisH81IK9"},"outputs":[],"source":["# Print the shape of the Glass dataset\n","print('Glass dataset shape', data.shape)\n","\n","# Generate summary statistics for the Glass dataset\n","data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtUiSOuL1liZ"},"outputs":[],"source":["# Get the target values from the Glass dataset\n","target = data['Glass Class'].values\n","print('Target shape', target.shape)\n","\n","# Calculate the unique target labels and their counts\n","unique, count = np.unique(target, return_counts=True)\n","print('Target labels:', unique)\n","for i in range(len(unique)):\n","    print(f'Class {unique[i]} has {count[i]} samples')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_AV2Ce81lf1"},"outputs":[],"source":["# Plot pairwise relationships between features colored by glass class\n","sns.pairplot(\n","    data=data,\n","    hue='Glass Class',\n","    corner=True,        # Display only lower triangle for efficiency\n","    palette='tab10'     # Color palette suitable for 6 classes\n",")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDG_wvir1lc8"},"outputs":[],"source":["# Determine the number of features\n","input_features = data.shape[1] - 1\n","print(f'Number of input features: {input_features}')\n","\n","# Determine the number of classes\n","num_classes = len(np.unique(target))\n","print(f'Number of classes: {num_classes}')"]},{"cell_type":"markdown","source":["## ‚öñÔ∏è **Unbalanced Classification**\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1_uFkyOJTe-LoiNEQAfqNJPdDfYz62Ner\" width=\"500\"/>"],"metadata":{"id":"rVLiRDvPuonp"}},{"cell_type":"markdown","source":["**Binary Classification Metrics**\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1dUVBtRp6yJmfr1-cwUmWtKX2UfgozNaj\" width=\"250\"/>\n","\n","$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$\n","\n","$\\text{Precision} = \\frac{TP}{TP + FP}$\n","\n","$\\text{Recall} = \\frac{TP}{TP + FN}$\n","\n","$F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n","\n","&nbsp;\n","\n","---\n","\n","**Multiclass Classification Metrics**\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1yDfrbcjHPTqFx9P5ZPhi07S5XVcXjd6g\" width=\"300\"/>\n","\n","$\\text{Accuracy} = \\sum_{i=1}^{N} \\frac{TP_i}{TP_i + TN_i + FP_i + FN_i}$\n","\n","$\\text{Precision}_i = \\frac{TP_i}{TP_i + FP_i}$\n","\n","$\\text{Recall}_i = \\frac{TP_i}{TP_i + FN_i}$\n","\n","$F1_i = 2 \\cdot \\frac{\\text{Precision}_i \\cdot \\text{Recall}_i}{\\text{Precision}_i + \\text{Recall}_i}$\n","\n","&nbsp;\n","\n","**Macro Averaging**\n","\n","$\\text{Precision}_{macro} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Precision}_i$\n","\n","$\\text{Recall}_{macro} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Recall}_i$\n","\n","$F1_{macro} = \\frac{1}{N} \\sum_{i=1}^{N} F1_i$\n","\n","&nbsp;\n","\n","**Weighted Macro Averaging**\n","\n","$\\text{Precision}_{weighted} = \\sum_{i=1}^{N} \\left( \\frac{N_i}{N} \\times \\text{Precision}_i \\right)$\n","\n","$\\text{Recall}_{weighted} = \\sum_{i=1}^{N} \\left( \\frac{N_i}{N} \\times \\text{Recall}_i \\right)$\n","\n","$F1_{weighted} = \\sum_{i=1}^{N} \\left( \\frac{N_i}{N} \\times F1_i \\right)$"],"metadata":{"id":"2Nwq7hVyst5X"}},{"cell_type":"markdown","metadata":{"id":"xj6jQOLc33eb"},"source":["## üîÑ **Data Preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvT7Z5-51lZ2"},"outputs":[],"source":["# Prepare features and labels as float32 and int64 arrays\n","X = data.drop('Glass Class', axis=1).astype(np.float32).values\n","y = target.astype(np.int64)\n","\n","# First split: separate 20 samples for final testing\n","X_train_val, X_test, y_train_val, y_test = train_test_split(\n","    X,\n","    y,\n","    test_size=25,\n","    random_state=SEED,\n","    stratify=y\n",")\n","\n","# Second split: divide remaining data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train_val,\n","    y_train_val,\n","    test_size=25,\n","    random_state=SEED,\n","    stratify=y_train_val\n",")\n","\n","# Show final dataset sizes\n","print('Training set shape:\\t', X_train.shape, y_train.shape)\n","print('Validation set shape:\\t', X_val.shape, y_val.shape)\n","print('Test set shape:\\t\\t', X_test.shape, y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hl4jbx641lXV"},"outputs":[],"source":["# Find maximum value for each feature in training data\n","max_df = X_train.max(axis=0)\n","print('Iris dataset maximum values')\n","print(max_df)\n","\n","# Find minimum value for each feature in training data\n","min_df = X_train.min(axis=0)\n","print('\\nIris dataset minimum values')\n","print(min_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pw5j9byR1lUe"},"outputs":[],"source":["# Apply min-max scaling using training data statistics\n","X_train = (X_train - min_df) / (max_df - min_df)\n","X_val = (X_val - min_df) / (max_df - min_df)\n","X_test = (X_test - min_df) / (max_df - min_df)\n","\n","# Verify normalization worked (should be 0.0 to 1.0)\n","print(f\"New maximum values: {X_train.max(axis=0)}\")\n","print(f\"New minimum values: {X_train.min(axis=0)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MROuaCwk1lRm"},"outputs":[],"source":["# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n","train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n","test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_6p1dMP1lOm"},"outputs":[],"source":["# Define the batch size, which is the number of samples in each batch\n","BATCH_SIZE = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxswApXm1lL3"},"outputs":[],"source":["def make_loader(ds, batch_size, shuffle, drop_last):\n","    # Determine optimal number of worker processes for data loading\n","    cpu_cores = os.cpu_count() or 2\n","    num_workers = max(2, min(4, cpu_cores))\n","\n","    # Create DataLoader with performance optimizations\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers,\n","        pin_memory=True,  # Faster GPU transfer\n","        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n","        prefetch_factor=4,  # Load 4 batches ahead\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RmZ2LJEH1lJM"},"outputs":[],"source":["# Create data loaders with different settings for each phase\n","train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n","test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELJCMBqm1lGH"},"outputs":[],"source":["# Get one batch from the training data loader\n","for xb, yb in train_loader:\n","    print(\"Features batch shape:\", xb.shape)\n","    print(\"Labels batch shape:\", yb.shape)\n","    break # Stop after getting one batch"]},{"cell_type":"markdown","metadata":{"id":"Yq6ADZrD4xrA"},"source":["## üõ†Ô∏è **Model Building**"]},{"cell_type":"code","source":["# Model architecture configuration\n","HIDDEN_LAYERS = 2\n","HIDDEN_SIZE = 512"],"metadata":{"id":"2iwd4FJAYwa6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJpcWRGb1lDU"},"outputs":[],"source":["# Define a simple feedforward neural network\n","class FeedForwardNet(nn.Module):\n","    def __init__(self, in_features=input_features, hidden_layers=1, hidden_size=16, dropout_rate=0.0, num_classes=num_classes):\n","        super().__init__()\n","        modules = []\n","        # First layer\n","        modules.append(nn.Linear(in_features, hidden_size))\n","        if dropout_rate > 0 :\n","            modules.append(nn.Dropout(dropout_rate))\n","        modules.append(nn.ReLU())\n","\n","        # Additional hidden layers\n","        for _ in range(hidden_layers):\n","            modules.append(nn.Linear(hidden_size, hidden_size))\n","            if dropout_rate > 0 :\n","                modules.append(nn.Dropout(dropout_rate))\n","            modules.append(nn.ReLU())\n","\n","        # Output layer\n","        modules.append(nn.Linear(hidden_size, num_classes))\n","        self.net = nn.Sequential(*modules)\n","\n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvdbqtWy1lAN"},"outputs":[],"source":["# Create model and display architecture with parameter count\n","model = FeedForwardNet(hidden_layers=HIDDEN_LAYERS, hidden_size=HIDDEN_SIZE).to(device)\n","summary(model, input_size=(input_features,))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtDyQqREKkdK"},"outputs":[],"source":["# Set up TensorBoard logging and save model architecture\n","experiment_name = \"baseline\"\n","writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n","x = torch.randn(1, input_features).to(device)\n","writer.add_graph(model, x)\n","writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cBaInNyKop5"},"outputs":[],"source":["# Copy TensorBoard logs to accessible location for Colab\n","!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n","\n","# Launch TensorBoard interface\n","%tensorboard --logdir \"/content/\"$logs_dir"]},{"cell_type":"markdown","metadata":{"id":"_SeAVVtC47KT"},"source":["## üßÆ **Network Parameters**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LF9ujjA31k9g"},"outputs":[],"source":["# Training configuration\n","LEARNING_RATE = 1e-3\n","EPOCHS = 500\n","\n","# Set up loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# Enable mixed precision training for GPU acceleration\n","scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"]},{"cell_type":"markdown","metadata":{"id":"hb7P-CSm4-Jg"},"source":["## üß† **Model Training**"]},{"cell_type":"code","source":["# Initialize best model tracking variables\n","best_model = None\n","best_performance = float('-inf')"],"metadata":{"id":"VaaxZI8hAUcC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5yhvl7I1k6f"},"outputs":[],"source":["def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n","    \"\"\"\n","    Perform one complete training epoch through the entire training dataset.\n","\n","    Args:\n","        model (nn.Module): The neural network model to train\n","        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n","        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n","        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n","        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","        l1_lambda (float): Lambda for L1 regularization\n","        l2_lambda (float): Lambda for L2 regularization\n","\n","    Returns:\n","        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n","    \"\"\"\n","    model.train()  # Set model to training mode\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_targets = []\n","\n","    # Iterate through training batches\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        # Move data to device (GPU/CPU)\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Clear gradients from previous step\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        # Forward pass with mixed precision (if CUDA available)\n","        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","            logits = model(inputs)\n","            loss = criterion(logits, targets)\n","\n","            # Add L1 and L2 regularization\n","            l1_norm = sum(p.abs().sum() for p in model.parameters())\n","            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n","            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n","\n","\n","        # Backward pass with gradient scaling\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # Accumulate metrics\n","        running_loss += loss.item() * inputs.size(0)\n","        predictions = logits.argmax(dim=1)\n","        all_predictions.append(predictions.cpu().numpy())\n","        all_targets.append(targets.cpu().numpy())\n","\n","    # Calculate epoch metrics\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    epoch_f1 = f1_score(\n","        np.concatenate(all_targets),\n","        np.concatenate(all_predictions),\n","        average='weighted'\n","    )\n","\n","    return epoch_loss, epoch_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kd-4dZt-1k3n"},"outputs":[],"source":["def validate_one_epoch(model, val_loader, criterion, device):\n","    \"\"\"\n","    Perform one complete validation epoch through the entire validation dataset.\n","\n","    Args:\n","        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n","        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n","        criterion (nn.Module): Loss function used to calculate validation loss\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","\n","    Returns:\n","        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n","\n","    Note:\n","        This function automatically sets the model to evaluation mode and disables\n","        gradient computation for efficiency during validation.\n","    \"\"\"\n","    model.eval()  # Set model to evaluation mode\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_targets = []\n","\n","    # Disable gradient computation for validation\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            # Move data to device\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            # Forward pass with mixed precision (if CUDA available)\n","            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","                logits = model(inputs)\n","                loss = criterion(logits, targets)\n","\n","            # Accumulate metrics\n","            running_loss += loss.item() * inputs.size(0)\n","            predictions = logits.argmax(dim=1)\n","            all_predictions.append(predictions.cpu().numpy())\n","            all_targets.append(targets.cpu().numpy())\n","\n","    # Calculate epoch metrics\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    epoch_accuracy = f1_score(\n","        np.concatenate(all_targets),\n","        np.concatenate(all_predictions),\n","        average='weighted'\n","    )\n","\n","    return epoch_loss, epoch_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTSWCMxi1k1H"},"outputs":[],"source":["def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n","    \"\"\"\n","    Log training metrics and model parameters to TensorBoard for visualization.\n","\n","    Args:\n","        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n","        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n","        train_loss (float): Training loss for this epoch\n","        train_f1 (float): Training f1 score for this epoch\n","        val_loss (float): Validation loss for this epoch\n","        val_f1 (float): Validation f1 score for this epoch\n","        model (nn.Module): The neural network model (for logging weights/gradients)\n","\n","    Note:\n","        This function logs scalar metrics (loss/f1 score) and histograms of model\n","        parameters and gradients, which helps monitor training progress and detect\n","        issues like vanishing/exploding gradients.\n","    \"\"\"\n","    # Log scalar metrics\n","    writer.add_scalar('Loss/Training', train_loss, epoch)\n","    writer.add_scalar('Loss/Validation', val_loss, epoch)\n","    writer.add_scalar('F1/Training', train_f1, epoch)\n","    writer.add_scalar('F1/Validation', val_f1, epoch)\n","\n","    # Log model parameters and gradients\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            if param.numel() > 0:\n","                writer.add_histogram(f'{name}/weights', param.data, epoch)\n","            if param.grad is not None:\n","                if param.grad.numel() > 0:\n","                    writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"]},{"cell_type":"code","source":["def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n","        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n","        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n","    \"\"\"\n","    Train the neural network model on the training data and validate on the validation data.\n","\n","    Args:\n","        model (nn.Module): The neural network model to train\n","        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n","        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n","        epochs (int): Number of training epochs\n","        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n","        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n","        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","        l1_lambda (float): L1 regularization coefficient (default: 0)\n","        l2_lambda (float): L2 regularization coefficient (default: 0)\n","        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n","        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n","        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n","        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n","        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n","        verbose (int, optional): Frequency of printing training progress (default: 10)\n","        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n","\n","    Returns:\n","        tuple: (model, training_history) - Trained model and metrics history\n","    \"\"\"\n","\n","    # Initialize metrics tracking\n","    training_history = {\n","        'train_loss': [], 'val_loss': [],\n","        'train_f1': [], 'val_f1': []\n","    }\n","\n","    # Configure early stopping if patience is set\n","    if patience > 0:\n","        patience_counter = 0\n","        best_metric = float('-inf') if mode == 'max' else float('inf')\n","        best_epoch = 0\n","\n","    print(f\"Training {epochs} epochs...\")\n","\n","    # Main training loop: iterate through epochs\n","    for epoch in range(1, epochs + 1):\n","\n","        # Forward pass through training data, compute gradients, update weights\n","        train_loss, train_f1 = train_one_epoch(\n","            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n","        )\n","\n","        # Evaluate model on validation data without updating weights\n","        val_loss, val_f1 = validate_one_epoch(\n","            model, val_loader, criterion, device\n","        )\n","\n","        # Store metrics for plotting and analysis\n","        training_history['train_loss'].append(train_loss)\n","        training_history['val_loss'].append(val_loss)\n","        training_history['train_f1'].append(train_f1)\n","        training_history['val_f1'].append(val_f1)\n","\n","        # Write metrics to TensorBoard for visualization\n","        if writer is not None:\n","            log_metrics_to_tensorboard(\n","                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n","            )\n","\n","        # Print progress every N epochs or on first epoch\n","        if epoch % verbose == 0 or epoch == 1:\n","            print(f\"Epoch {epoch:3d}/{epochs} | \"\n","                  f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n","                  f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n","\n","        # Early stopping logic: monitor metric and save best model\n","        if patience > 0:\n","            current_metric = training_history[evaluation_metric][-1]\n","            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n","\n","            if is_improvement:\n","                best_metric = current_metric\n","                best_epoch = epoch\n","                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= patience:\n","                    print(f\"Early stopping triggered after {epoch} epochs.\")\n","                    break\n","\n","    # Restore best model weights if early stopping was used\n","    if restore_best_weights and patience > 0:\n","        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n","        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n","\n","    # Save final model if no early stopping\n","    if patience == 0:\n","        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n","\n","    # Close TensorBoard writer\n","    if writer is not None:\n","        writer.close()\n","\n","    return model, training_history"],"metadata":{"id":"u7OpSai1FNPU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Experiment 1: Baseline**"],"metadata":{"id":"MxWLWPNTZ_5-"}},{"cell_type":"code","source":["%%time\n","# Train model and track training history\n","model, training_history = fit(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    epochs=EPOCHS,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scaler=scaler,\n","    device=device,\n","    writer=writer,\n","    verbose=10,\n","    experiment_name=\"baseline\"\n","    )\n","\n","# Update best model if current performance is superior\n","if training_history['val_f1'][-1] > best_performance:\n","    best_model = model\n","    best_performance = training_history['val_f1'][-1]"],"metadata":{"id":"e0rv3kvoQdrE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAtsKT7i1kqp","cellView":"form"},"outputs":[],"source":["# @title Plot Hitory\n","# Create a figure with two side-by-side subplots (two columns)\n","fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n","\n","# Plot of training and validation loss on the first axis\n","ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n","ax1.set_title('Loss')\n","ax1.legend()\n","ax1.grid(alpha=0.3)\n","\n","# Plot of training and validation accuracy on the second axis\n","ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n","ax2.set_title('F1 Score')\n","ax2.legend()\n","ax2.grid(alpha=0.3)\n","\n","# Adjust the layout and display the plot\n","plt.tight_layout()\n","plt.subplots_adjust(right=0.85)\n","plt.show()"]},{"cell_type":"code","source":["# @title Plot Confusion Matrix\n","# Collect predictions and ground truth labels\n","val_preds, val_targets = [], []\n","with torch.no_grad():  # Disable gradient computation for inference\n","    for xb, yb in val_loader:\n","        xb = xb.to(device)\n","\n","        # Forward pass: get model predictions\n","        logits = model(xb)\n","        preds = logits.argmax(dim=1).cpu().numpy()\n","\n","        # Store batch results\n","        val_preds.append(preds)\n","        val_targets.append(yb.numpy())\n","\n","# Combine all batches into single arrays\n","val_preds = np.concatenate(val_preds)\n","val_targets = np.concatenate(val_targets)\n","\n","# Calculate overall validation metrics\n","val_acc = accuracy_score(val_targets, val_preds)\n","val_prec = precision_score(val_targets, val_preds, average='weighted')\n","val_rec = recall_score(val_targets, val_preds, average='weighted')\n","val_f1 = f1_score(val_targets, val_preds, average='weighted')\n","print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n","print(f\"Precision over the validation set: {val_prec:.4f}\")\n","print(f\"Recall over the validation set: {val_rec:.4f}\")\n","print(f\"F1 score over the validation set: {val_f1:.4f}\")\n","\n","# Generate confusion matrix for detailed error analysis\n","cm = confusion_matrix(val_targets, val_preds)\n","\n","# Create numeric labels for heatmap annotation\n","labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n","\n","# Visualise confusion matrix\n","plt.figure(figsize=(8, 7))\n","sns.heatmap(cm, annot=labels, fmt='',\n","            cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix ‚Äî Validation Set')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"gw1I5EMjB6uB","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy TensorBoard logs to accessible location for Colab\n","!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n","\n","# Launch TensorBoard interface\n","%tensorboard --logdir \"/content/\"$logs_dir"],"metadata":{"id":"m9Kuzj1ua28j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Experiment 2: Early Stopping**"],"metadata":{"id":"-SwR0-cvaDKW"}},{"cell_type":"code","source":["# Display model architecture and parameter count\n","model = FeedForwardNet(hidden_layers=HIDDEN_LAYERS, hidden_size=HIDDEN_SIZE).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","summary(model, input_size=(input_features,))"],"metadata":{"id":"tGQuifCQbLU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up TensorBoard logging and save model architecture\n","experiment_name = \"early_stopping\"\n","writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n","x = torch.randn(1, input_features).to(device)\n","writer.add_graph(model, x)\n","writer.close()"],"metadata":{"id":"Rp15Jvz8l-9G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# Train model and track training history\n","model, training_history = fit(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    epochs=EPOCHS,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scaler=scaler,\n","    device=device,\n","    writer=writer,\n","    patience=200,\n","    verbose=10,\n","    experiment_name=experiment_name\n","    )\n","\n","# Update best model if current performance is superior\n","if max(training_history['val_f1']) > best_performance:\n","    best_model = model\n","    best_performance = max(training_history['val_f1'])"],"metadata":{"id":"8l44b0tER9b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Hitory\n","# Create a figure with two side-by-side subplots (two columns)\n","fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n","\n","# Plot of training and validation loss on the first axis\n","ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n","ax1.set_title('Loss')\n","ax1.legend()\n","ax1.grid(alpha=0.3)\n","\n","# Plot of training and validation accuracy on the second axis\n","ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n","ax2.set_title('F1 Score')\n","ax2.legend()\n","ax2.grid(alpha=0.3)\n","\n","# Adjust the layout and display the plot\n","plt.tight_layout()\n","plt.subplots_adjust(right=0.85)\n","plt.show()"],"metadata":{"id":"f50stMQsddXM","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Confusion Matrix\n","# Collect predictions and ground truth labels\n","val_preds, val_targets = [], []\n","with torch.no_grad():  # Disable gradient computation for inference\n","    for xb, yb in val_loader:\n","        xb = xb.to(device)\n","\n","        # Forward pass: get model predictions\n","        logits = model(xb)\n","        preds = logits.argmax(dim=1).cpu().numpy()\n","\n","        # Store batch results\n","        val_preds.append(preds)\n","        val_targets.append(yb.numpy())\n","\n","# Combine all batches into single arrays\n","val_preds = np.concatenate(val_preds)\n","val_targets = np.concatenate(val_targets)\n","\n","# Calculate overall validation metrics\n","val_acc = accuracy_score(val_targets, val_preds)\n","val_prec = precision_score(val_targets, val_preds, average='weighted')\n","val_rec = recall_score(val_targets, val_preds, average='weighted')\n","val_f1 = f1_score(val_targets, val_preds, average='weighted')\n","print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n","print(f\"Precision over the validation set: {val_prec:.4f}\")\n","print(f\"Recall over the validation set: {val_rec:.4f}\")\n","print(f\"F1 score over the validation set: {val_f1:.4f}\")\n","\n","# Generate confusion matrix for detailed error analysis\n","cm = confusion_matrix(val_targets, val_preds)\n","\n","# Create numeric labels for heatmap annotation\n","labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n","\n","# Visualise confusion matrix\n","plt.figure(figsize=(8, 7))\n","sns.heatmap(cm, annot=labels, fmt='',\n","            cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix ‚Äî Validation Set')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"cellView":"form","id":"nhYolhaFtOlP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy TensorBoard logs to accessible location for Colab\n","!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n","\n","# Launch TensorBoard interface\n","%tensorboard --logdir \"/content/\"$logs_dir"],"metadata":{"id":"Wb-CKrZ-ddVq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###\n","<img src=\"https://drive.google.com/uc?export=view&id=1wBEbGsDgGGliPVYZW_SqH-xCwb9VpLG8\" width=\"500\"/>"],"metadata":{"id":"XaOqmz7rygXy"}},{"cell_type":"markdown","source":["### **Experiment 3: Early Stopping and Dropout**\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1b15Yqt9FtfEkPMB5e-DvDlmLHgYtuH1s\" width=\"500\"/>"],"metadata":{"id":"qFoUVZn4aGVm"}},{"cell_type":"code","source":["# Display model architecture and parameter count\n","DROPOUT_RATE = 0.5\n","model = FeedForwardNet(hidden_layers=HIDDEN_LAYERS, hidden_size=HIDDEN_SIZE, dropout_rate=DROPOUT_RATE).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","summary(model, input_size=(input_features,))"],"metadata":{"id":"5SzaeOx4o7rF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up TensorBoard logging and save model architecture\n","experiment_name = \"early_stopping_and_dropout\"\n","writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n","x = torch.randn(1, input_features).to(device)\n","writer.add_graph(model, x)\n","writer.close()"],"metadata":{"id":"XvP23ILHo7i_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# Train model and track training history\n","model, training_history = fit(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    epochs=EPOCHS,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scaler=scaler,\n","    device=device,\n","    writer=writer,\n","    patience=200,\n","    verbose=10,\n","    experiment_name=experiment_name\n","    )\n","\n","# Update best model if current performance is superior\n","if max(training_history['val_f1']) > best_performance:\n","    best_model = model\n","    best_performance = max(training_history['val_f1'])"],"metadata":{"id":"dC71vUuUo7gs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Hitory\n","# Create a figure with two side-by-side subplots (two columns)\n","fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n","\n","# Plot of training and validation loss on the first axis\n","ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n","ax1.set_title('Loss')\n","ax1.legend()\n","ax1.grid(alpha=0.3)\n","\n","# Plot of training and validation accuracy on the second axis\n","ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n","ax2.set_title('F1 Score')\n","ax2.legend()\n","ax2.grid(alpha=0.3)\n","\n","# Adjust the layout and display the plot\n","plt.tight_layout()\n","plt.subplots_adjust(right=0.85)\n","plt.show()"],"metadata":{"id":"bjcUwbdLpjqL","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Confusion Matrix\n","# Collect predictions and ground truth labels\n","val_preds, val_targets = [], []\n","with torch.no_grad():  # Disable gradient computation for inference\n","    for xb, yb in val_loader:\n","        xb = xb.to(device)\n","\n","        # Forward pass: get model predictions\n","        logits = model(xb)\n","        preds = logits.argmax(dim=1).cpu().numpy()\n","\n","        # Store batch results\n","        val_preds.append(preds)\n","        val_targets.append(yb.numpy())\n","\n","# Combine all batches into single arrays\n","val_preds = np.concatenate(val_preds)\n","val_targets = np.concatenate(val_targets)\n","\n","# Calculate overall validation metrics\n","val_acc = accuracy_score(val_targets, val_preds)\n","val_prec = precision_score(val_targets, val_preds, average='weighted')\n","val_rec = recall_score(val_targets, val_preds, average='weighted')\n","val_f1 = f1_score(val_targets, val_preds, average='weighted')\n","print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n","print(f\"Precision over the validation set: {val_prec:.4f}\")\n","print(f\"Recall over the validation set: {val_rec:.4f}\")\n","print(f\"F1 score over the validation set: {val_f1:.4f}\")\n","\n","# Generate confusion matrix for detailed error analysis\n","cm = confusion_matrix(val_targets, val_preds)\n","\n","# Create numeric labels for heatmap annotation\n","labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n","\n","# Visualise confusion matrix\n","plt.figure(figsize=(8, 7))\n","sns.heatmap(cm, annot=labels, fmt='',\n","            cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix ‚Äî Validation Set')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"eeLuLAZh38vI","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy TensorBoard logs to accessible location for Colab\n","!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n","\n","# Launch TensorBoard interface\n","%tensorboard --logdir \"/content/\"$logs_dir"],"metadata":{"id":"7iATB2zwo7d6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Experiment 4: Early Stopping and L2 Regularisation**\n","\n","$$\\mathcal{L}_{\\mathrm{reg}} (y, \\hat{y}, w) = \\mathcal{L}(y, \\hat{y}) + \\lambda \\cdot R(w)$$\n","\n","where:\n","- $\\mathcal{L}(y, \\hat{y})$ is a generic loss function measuring the discrepancy between predictions $\\hat{y}$ and ground truth $y$\n","- $R(w)$ is the regularization term on weights:\n","  - **L2 (Ridge)**: $R(w) = ||w||_2^2 = \\sum^K_{k=1}w_k^2$\n","  - **L1 (Lasso)**: $R(w) = ||w||_1 = \\sum^K_{k=1}|w_k|$\n","  - **Elastic Net**: $R(w) = \\alpha||w||_1 + \\beta||w||_2^2$\n","- $\\lambda \\geq 0$ is the regularization coefficient controlling the trade-off between loss minimization and weight regularization"],"metadata":{"id":"DNM1lrfPaJoW"}},{"cell_type":"code","source":["# Display model architecture and parameter count\n","L2_LAMBDA = 0.001\n","model = FeedForwardNet(hidden_layers=HIDDEN_LAYERS, hidden_size=HIDDEN_SIZE).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","summary(model, input_size=(input_features,))"],"metadata":{"id":"Y1NVSpZ-tzt2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up TensorBoard logging and save model architecture\n","experiment_name = \"early_stopping_and_l2\"\n","writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n","x = torch.randn(1, input_features).to(device)\n","writer.add_graph(model, x)\n","writer.close()"],"metadata":{"id":"8VXmJB5ot0Px"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# Train model and track training history\n","model, training_history = fit(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    epochs=EPOCHS,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scaler=scaler,\n","    device=device,\n","    writer=writer,\n","    patience=200,\n","    verbose=10,\n","    l2_lambda=L2_LAMBDA,\n","    experiment_name=experiment_name\n","    )\n","\n","# Update best model if current performance is superior\n","if max(training_history['val_f1']) > best_performance:\n","    best_model = model\n","    best_performance = max(training_history['val_f1'])"],"metadata":{"id":"vnUtLvDPtznv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Hitory\n","# Create a figure with two side-by-side subplots (two columns)\n","fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n","\n","# Plot of training and validation loss on the first axis\n","ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n","ax1.set_title('Loss')\n","ax1.legend()\n","ax1.grid(alpha=0.3)\n","\n","# Plot of training and validation accuracy on the second axis\n","ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n","ax2.set_title('F1 Score')\n","ax2.legend()\n","ax2.grid(alpha=0.3)\n","\n","# Adjust the layout and display the plot\n","plt.tight_layout()\n","plt.subplots_adjust(right=0.85)\n","plt.show()"],"metadata":{"id":"5kG1aB_JtzlP","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Confusion Matrix\n","# Collect predictions and ground truth labels\n","val_preds, val_targets = [], []\n","with torch.no_grad():  # Disable gradient computation for inference\n","    for xb, yb in val_loader:\n","        xb = xb.to(device)\n","\n","        # Forward pass: get model predictions\n","        logits = model(xb)\n","        preds = logits.argmax(dim=1).cpu().numpy()\n","\n","        # Store batch results\n","        val_preds.append(preds)\n","        val_targets.append(yb.numpy())\n","\n","# Combine all batches into single arrays\n","val_preds = np.concatenate(val_preds)\n","val_targets = np.concatenate(val_targets)\n","\n","# Calculate overall validation metrics\n","val_acc = accuracy_score(val_targets, val_preds)\n","val_prec = precision_score(val_targets, val_preds, average='weighted')\n","val_rec = recall_score(val_targets, val_preds, average='weighted')\n","val_f1 = f1_score(val_targets, val_preds, average='weighted')\n","print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n","print(f\"Precision over the validation set: {val_prec:.4f}\")\n","print(f\"Recall over the validation set: {val_rec:.4f}\")\n","print(f\"F1 score over the validation set: {val_f1:.4f}\")\n","\n","# Generate confusion matrix for detailed error analysis\n","cm = confusion_matrix(val_targets, val_preds)\n","\n","# Create numeric labels for heatmap annotation\n","labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n","\n","# Visualise confusion matrix\n","plt.figure(figsize=(8, 7))\n","sns.heatmap(cm, annot=labels, fmt='',\n","            cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix ‚Äî Validation Set')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"cbTHN07ctzix","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy TensorBoard logs to accessible location for Colab\n","!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n","\n","# Launch TensorBoard interface\n","%tensorboard --logdir \"/content/\"$logs_dir"],"metadata":{"id":"eDya_rGI4TiI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Experiment 5: Early Stopping and AdamW**\n","\n","The L2 regularization term is added to the loss function, affecting the gradients:\n","\n","$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}(y, \\hat{y}) + \\lambda||w||_2^2$$\n","\n","$$\\nabla_w \\mathcal{L}_{\\text{total}} = \\nabla_w \\mathcal{L}(y, \\hat{y}) + 2\\lambda w$$\n","\n","Update rule:\n","$$w_{t+1} = w_t - \\alpha \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$\n","\n","where $m_t$ and $v_t$ are computed using gradients that **include** the L2 term.\n","\n","#### **AdamW (Decoupled Weight Decay)**\n","\n","The weight decay is applied **directly** to the weights, separate from the adaptive gradient computation:\n","\n","$$\\nabla_w \\mathcal{L} = \\nabla_w \\mathcal{L}(y, \\hat{y})$$\n","\n","Update rule:\n","$$w_{t+1} = (1 - \\lambda)w_t - \\alpha \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$\n","\n","where $m_t$ and $v_t$ are computed using gradients **without** the L2 term.\n","\n","**Note**: In AdamW, the weight decay $\\lambda$ acts as a true regularizer independent of the loss landscape, leading to better generalization in deep neural networks."],"metadata":{"id":"c3ZLGjsOaOny"}},{"cell_type":"code","source":["# Display model architecture and parameter count\n","L2_LAMBDA = 0.001\n","model = FeedForwardNet(hidden_layers=HIDDEN_LAYERS, hidden_size=HIDDEN_SIZE).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n","summary(model, input_size=(input_features,))"],"metadata":{"id":"OLLRDqwlyRDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up TensorBoard logging and save model architecture\n","experiment_name = \"early_stopping_and_adamw\"\n","writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n","x = torch.randn(1, input_features).to(device)\n","writer.add_graph(model, x)\n","writer.close()"],"metadata":{"id":"grHUPGliyQ-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# Train model and track training history\n","model, training_history = fit(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    epochs=EPOCHS,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scaler=scaler,\n","    device=device,\n","    writer=writer,\n","    patience=200,\n","    verbose=10,\n","    experiment_name=experiment_name\n","    )\n","\n","# Update best model if current performance is superior\n","if max(training_history['val_f1']) > best_performance:\n","    best_model = model\n","    best_performance = max(training_history['val_f1'])"],"metadata":{"id":"aoxsnmCJyQ7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Hitory\n","# Create a figure with two side-by-side subplots (two columns)\n","fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n","\n","# Plot of training and validation loss on the first axis\n","ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n","ax1.set_title('Loss')\n","ax1.legend()\n","ax1.grid(alpha=0.3)\n","\n","# Plot of training and validation accuracy on the second axis\n","ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n","ax2.set_title('F1 Score')\n","ax2.legend()\n","ax2.grid(alpha=0.3)\n","\n","# Adjust the layout and display the plot\n","plt.tight_layout()\n","plt.subplots_adjust(right=0.85)\n","plt.show()"],"metadata":{"id":"9qY-GCdXyQ5B","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Confusion Matrix\n","# Collect predictions and ground truth labels\n","val_preds, val_targets = [], []\n","with torch.no_grad():  # Disable gradient computation for inference\n","    for xb, yb in val_loader:\n","        xb = xb.to(device)\n","\n","        # Forward pass: get model predictions\n","        logits = model(xb)\n","        preds = logits.argmax(dim=1).cpu().numpy()\n","\n","        # Store batch results\n","        val_preds.append(preds)\n","        val_targets.append(yb.numpy())\n","\n","# Combine all batches into single arrays\n","val_preds = np.concatenate(val_preds)\n","val_targets = np.concatenate(val_targets)\n","\n","# Calculate overall validation metrics\n","val_acc = accuracy_score(val_targets, val_preds)\n","val_prec = precision_score(val_targets, val_preds, average='weighted')\n","val_rec = recall_score(val_targets, val_preds, average='weighted')\n","val_f1 = f1_score(val_targets, val_preds, average='weighted')\n","print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n","print(f\"Precision over the validation set: {val_prec:.4f}\")\n","print(f\"Recall over the validation set: {val_rec:.4f}\")\n","print(f\"F1 score over the validation set: {val_f1:.4f}\")\n","\n","# Generate confusion matrix for detailed error analysis\n","cm = confusion_matrix(val_targets, val_preds)\n","\n","# Create numeric labels for heatmap annotation\n","labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n","\n","# Visualise confusion matrix\n","plt.figure(figsize=(8, 7))\n","sns.heatmap(cm, annot=labels, fmt='',\n","            cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix ‚Äî Validation Set')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"cellView":"form","id":"kNGL0_9f4a9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy TensorBoard logs to accessible location for Colab\n","!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n","\n","# Launch TensorBoard interface\n","%tensorboard --logdir \"/content/\"$logs_dir"],"metadata":{"id":"1hBMfojGyQ3M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Experiment 5: Early Stopping, AdamW and Dropout**"],"metadata":{"id":"oDqt6eJLaRXd"}},{"cell_type":"code","source":["# Display model architecture and parameter count\n","L2_LAMBDA = 0.001\n","DROPOUT_RATE = 0.5\n","model = FeedForwardNet(hidden_layers=HIDDEN_LAYERS, hidden_size=HIDDEN_SIZE, dropout_rate=DROPOUT_RATE).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n","summary(model, input_size=(input_features,))"],"metadata":{"id":"4rwgAFj5R9kA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up TensorBoard logging and save model architecture\n","experiment_name = \"early_stopping_adamw_and_dropout\"\n","writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n","x = torch.randn(1, input_features).to(device)\n","writer.add_graph(model, x)\n","writer.close()"],"metadata":{"id":"RlKTig1KR9he"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# Train model and track training history\n","model, training_history = fit(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    epochs=EPOCHS,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scaler=scaler,\n","    device=device,\n","    writer=writer,\n","    patience=200,\n","    verbose=10,\n","    experiment_name=experiment_name\n","    )\n","\n","# Update best model if current performance is superior\n","if max(training_history['val_f1']) > best_performance:\n","    best_model = model\n","    best_performance = max(training_history['val_f1'])"],"metadata":{"id":"TbdhvEY3R9em"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Hitory\n","# Create a figure with two side-by-side subplots (two columns)\n","fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n","\n","# Plot of training and validation loss on the first axis\n","ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n","ax1.set_title('Loss')\n","ax1.legend()\n","ax1.grid(alpha=0.3)\n","\n","# Plot of training and validation accuracy on the second axis\n","ax2.plot(training_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax2.plot(training_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n","ax2.set_title('F1 Score')\n","ax2.legend()\n","ax2.grid(alpha=0.3)\n","\n","# Adjust the layout and display the plot\n","plt.tight_layout()\n","plt.subplots_adjust(right=0.85)\n","plt.show()"],"metadata":{"id":"U5o2kUWhR9ZH","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Plot Confusion Matrix\n","# Collect predictions and ground truth labels\n","val_preds, val_targets = [], []\n","with torch.no_grad():  # Disable gradient computation for inference\n","    for xb, yb in val_loader:\n","        xb = xb.to(device)\n","\n","        # Forward pass: get model predictions\n","        logits = model(xb)\n","        preds = logits.argmax(dim=1).cpu().numpy()\n","\n","        # Store batch results\n","        val_preds.append(preds)\n","        val_targets.append(yb.numpy())\n","\n","# Combine all batches into single arrays\n","val_preds = np.concatenate(val_preds)\n","val_targets = np.concatenate(val_targets)\n","\n","# Calculate overall validation metrics\n","val_acc = accuracy_score(val_targets, val_preds)\n","val_prec = precision_score(val_targets, val_preds, average='weighted')\n","val_rec = recall_score(val_targets, val_preds, average='weighted')\n","val_f1 = f1_score(val_targets, val_preds, average='weighted')\n","print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n","print(f\"Precision over the validation set: {val_prec:.4f}\")\n","print(f\"Recall over the validation set: {val_rec:.4f}\")\n","print(f\"F1 score over the validation set: {val_f1:.4f}\")\n","\n","# Generate confusion matrix for detailed error analysis\n","cm = confusion_matrix(val_targets, val_preds)\n","\n","# Create numeric labels for heatmap annotation\n","labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n","\n","# Visualise confusion matrix\n","plt.figure(figsize=(8, 7))\n","sns.heatmap(cm, annot=labels, fmt='',\n","            cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix ‚Äî Validation Set')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"zvWMg9a64r9w","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy TensorBoard logs to accessible location for Colab\n","!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n","\n","# Launch TensorBoard interface\n","%tensorboard --logdir \"/content/\"$logs_dir"],"metadata":{"id":"AiJnajeJR9Wd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üïπÔ∏è **Inference**"],"metadata":{"id":"kUo6lOGALfu3"}},{"cell_type":"code","source":["# Collect predictions and ground truth labels\n","test_preds, test_targets = [], []\n","with torch.no_grad():  # Disable gradient computation for inference\n","    for xb, yb in test_loader:\n","        xb = xb.to(device)\n","\n","        # Forward pass: get model predictions\n","        logits = best_model(xb)\n","        preds = logits.argmax(dim=1).cpu().numpy()\n","\n","        # Store batch results\n","        test_preds.append(preds)\n","        test_targets.append(yb.numpy())\n","\n","# Combine all batches into single arrays\n","test_preds = np.concatenate(test_preds)\n","test_targets = np.concatenate(test_targets)"],"metadata":{"id":"U4f43odZLgQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate overall test accuracy\n","test_acc = accuracy_score(test_targets, test_preds)\n","test_prec = precision_score(test_targets, test_preds, average='weighted')\n","test_rec = recall_score(test_targets, test_preds, average='weighted')\n","test_f1 = f1_score(test_targets, test_preds, average='weighted')\n","print(f\"Accuracy over the test set: {test_acc:.4f}\")\n","print(f\"Precision over the test set: {test_prec:.4f}\")\n","print(f\"Recall over the test set: {test_rec:.4f}\")\n","print(f\"F1 score over the test set: {test_f1:.4f}\")\n","\n","# Generate confusion matrix for detailed error analysis\n","cm = confusion_matrix(test_targets, test_preds)\n","\n","# Create numeric labels for heatmap annotation\n","labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n","\n","# Visualise confusion matrix\n","plt.figure(figsize=(8, 7))\n","sns.heatmap(cm, annot=labels, fmt='',\n","            cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix ‚Äî Test Set')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"9kK0-gmkLg6D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOZkZVOlzz0e"},"source":["#  \n","<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n","\n","##### Connect with us:\n","- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"14\"> **LinkedIn:**  [AIRLab Polimi](https://www.linkedin.com/company/airlab-polimi/)\n","- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"14\"> **Instagram:** [airlab_polimi](https://www.instagram.com/airlab_polimi/)\n","\n","##### Contributors:\n","- **Eugenio Lomurno**: eugenio.lomurno@polimi.it\n","- **Alberto Archetti**: alberto.archetti@polimi.it\n","- **Roberto Basla**: roberto.basla@polimi.it\n","- **Carlo Sgaravatti**: carlo.sgaravatti@polimi.it\n","\n","```\n","   Copyright 2025 Eugenio Lomurno, Alberto Archetti, Roberto Basla, Carlo Sgaravatti\n","\n","   Licensed under the Apache License, Version 2.0 (the \"License\");\n","   you may not use this file except in compliance with the License.\n","   You may obtain a copy of the License at\n","\n","       http://www.apache.org/licenses/LICENSE-2.0\n","\n","   Unless required by applicable law or agreed to in writing, software\n","   distributed under the License is distributed on an \"AS IS\" BASIS,\n","   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","   See the License for the specific language governing permissions and\n","   limitations under the License.\n","```\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"12ODGsMkMPxFykDXp0w1dWj_F1FDU65sh","timestamp":1633524062268}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}