{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "name": "ensemble"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 13644638,
          "sourceType": "datasetVersion",
          "datasetId": 8673684
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Artificial Neural Networks and Deep Learning**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7QqT4hHxvylV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è **Libraries Import**"
      ],
      "metadata": {
        "id": "9ytR-7lYvylW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:51:32.840642Z",
          "iopub.execute_input": "2025-11-14T23:51:32.841085Z",
          "iopub.status.idle": "2025-11-14T23:51:32.846369Z",
          "shell.execute_reply.started": "2025-11-14T23:51:32.841054Z",
          "shell.execute_reply": "2025-11-14T23:51:32.845406Z"
        },
        "id": "UnnKI8_OvylW",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logs_dir = \"tensorboard\"\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "!mkdir -p models\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import copy\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:51:32.848061Z",
          "iopub.execute_input": "2025-11-14T23:51:32.849058Z",
          "iopub.status.idle": "2025-11-14T23:52:02.609983Z",
          "shell.execute_reply.started": "2025-11-14T23:51:32.849026Z",
          "shell.execute_reply": "2025-11-14T23:52:02.608777Z"
        },
        "id": "H03ZqAA_vylX",
        "outputId": "019c9d75-555b-4423-bc7e-8e0c4ea69837",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚è≥ **Data Loading**"
      ],
      "metadata": {
        "id": "Kzr-FtsCvylX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "61LPUtMq1ELs",
        "outputId": "53247723-a1f3-4136-baba-ef59eabb9298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:46.361448Z",
          "iopub.execute_input": "2025-11-14T23:52:46.361803Z",
          "iopub.status.idle": "2025-11-14T23:52:46.369030Z",
          "shell.execute_reply.started": "2025-11-14T23:52:46.361778Z",
          "shell.execute_reply": "2025-11-14T23:52:46.367608Z"
        },
        "id": "gzLzedV40-lk"
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ROOT = Path(\"./dataset\")\n",
        "\n",
        "# --- 2Ô∏è‚É£ Kaggle ---\n",
        "DATASET_ROOT = Path(\"/content/drive/MyDrive/pirate-pain\")\n",
        "\n",
        "# --- 3Ô∏è‚É£ Server o cluster privato (es. Westworld/Elysium) ---\n",
        "# DATASET_ROOT = Path(\"/multiverse/datasets/private_dataset/pirate_pain\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:48.414332Z",
          "iopub.execute_input": "2025-11-14T23:52:48.414649Z",
          "iopub.status.idle": "2025-11-14T23:52:48.419878Z",
          "shell.execute_reply.started": "2025-11-14T23:52:48.414630Z",
          "shell.execute_reply": "2025-11-14T23:52:48.418778Z"
        },
        "id": "4IcYbGcbvylY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directory for models\n",
        "output_dir = './ensemble_models'\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "raVI5146VxFh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.955929Z",
          "iopub.status.idle": "2025-11-14T23:52:02.956265Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.956121Z",
          "shell.execute_reply": "2025-11-14T23:52:02.956134Z"
        }
      },
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Caricamento dati\n",
        "X_train = pd.read_csv(DATASET_ROOT / \"pirate_pain_train.csv\")\n",
        "X_TRAIN = pd.read_csv(DATASET_ROOT / \"pirate_pain_train.csv\")\n",
        "\n",
        "y_train = pd.read_csv(DATASET_ROOT / \"pirate_pain_train_labels.csv\")\n",
        "Y_TRAIN = pd.read_csv(DATASET_ROOT / \"pirate_pain_train_labels.csv\")\n",
        "\n",
        "X_test  = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\n",
        "\n",
        "print(f\"  X_train: {X_train.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"  X_test:  {X_test.shape}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:51.495157Z",
          "iopub.execute_input": "2025-11-14T23:52:51.495588Z",
          "iopub.status.idle": "2025-11-14T23:52:58.723094Z",
          "shell.execute_reply.started": "2025-11-14T23:52:51.495556Z",
          "shell.execute_reply": "2025-11-14T23:52:58.721987Z"
        },
        "id": "09LXqmh6vylY",
        "outputId": "fb5a8880-30f5-41d7-8d6d-5dbaa634af2e",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  X_train: (105760, 40)\n",
            "  y_train: (661, 2)\n",
            "  X_test:  (211840, 40)\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_conversion_type_embed_ready(df):\n",
        "    \"\"\"\n",
        "    Minimal, embedding-friendly preprocessing:\n",
        "    - joints: float32 (continuous features)\n",
        "    - pain_survey_*: int64 indices (0..2) for embeddings\n",
        "    - n_legs/hands/eyes: mapped to {0,1} as int64 for embeddings\n",
        "    Returns: df, meta\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1) continuous features\n",
        "    joint_cols = [c for c in df.columns if c.startswith(\"joint_\")]\n",
        "    df[joint_cols] = df[joint_cols].astype(\"float32\")\n",
        "\n",
        "    # 2) surveys as categorical indices (already 0/1/2)\n",
        "    pain_survey_cols = [c for c in df.columns if c.startswith(\"pain_survey_\")]\n",
        "    df[pain_survey_cols] = df[pain_survey_cols].astype(\"int64\")\n",
        "\n",
        "    # 3) 2-way categoricals ‚Üí indices\n",
        "    legs_map  = {\"two\": 0, \"one+peg_leg\": 1}\n",
        "    hands_map = {\"two\": 0, \"one+hook_hand\": 1}\n",
        "    eyes_map  = {\"two\": 0, \"one+eye_patch\": 1}\n",
        "\n",
        "    if \"n_legs\" in df.columns:\n",
        "        df[\"n_legs\"]  = df[\"n_legs\"].map(legs_map).astype(\"int64\")\n",
        "    if \"n_hands\" in df.columns:\n",
        "        df[\"n_hands\"] = df[\"n_hands\"].map(hands_map).astype(\"int64\")\n",
        "    if \"n_eyes\" in df.columns:\n",
        "        df[\"n_eyes\"]  = df[\"n_eyes\"].map(eyes_map).astype(\"int64\")\n",
        "\n",
        "    # 4) define columns\n",
        "    cat_two_cols = [c for c in [\"n_legs\",\"n_hands\",\"n_eyes\"] if c in df.columns]\n",
        "    cat_cols = pain_survey_cols + cat_two_cols\n",
        "    cont_cols = joint_cols  # keep only joints as continuous\n",
        "\n",
        "    # 5) cardinals for embeddings (compute on TRAIN ONLY in CV, reuse for VAL/TEST)\n",
        "    cardinals = {c: int(df[c].nunique()) for c in cat_cols}\n",
        "    # suggested tiny dims: 1 for binaries, 2 for 3-class surveys\n",
        "    emb_dims = {c: (1 if cardinals[c] == 2 else 2) for c in cat_cols}\n",
        "\n",
        "    meta = {\n",
        "        \"cont_cols\": cont_cols,\n",
        "        \"cat_cols\":  cat_cols,\n",
        "        \"cardinals\": cardinals,\n",
        "        \"emb_dims\":  emb_dims,\n",
        "        \"maps\": {\"n_legs\": legs_map, \"n_hands\": hands_map, \"n_eyes\": eyes_map},\n",
        "    }\n",
        "    return df, meta\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.959588Z",
          "iopub.status.idle": "2025-11-14T23:52:02.959991Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.959810Z",
          "shell.execute_reply": "2025-11-14T23:52:02.959828Z"
        },
        "id": "uRJrsXUWvylY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_joints(df,\n",
        "                      drop_redundant=False,\n",
        "                      drop_near_zero=False,\n",
        "                      drop_low_var=False,\n",
        "                      verbose=True):\n",
        "    \"\"\"\n",
        "    Simplify joint_* preprocessing based on EDA results.\n",
        "    Removes constant, redundant, or near-zero-variance joints.\n",
        "\n",
        "    Returns a (df_out, feature_cols) tuple.\n",
        "    \"\"\"\n",
        "    joint_cols = sorted([c for c in df.columns if c.startswith(\"joint_\")],\n",
        "                        key=lambda x: int(x.split(\"_\")[1]))\n",
        "    drop = set()\n",
        "\n",
        "    # 1 Drop constant joint_30\n",
        "    if \"joint_30\" in joint_cols:\n",
        "        drop.add(\"joint_30\")\n",
        "\n",
        "    if \"joint_11\" in joint_cols:\n",
        "        drop.add(\"joint_11\")\n",
        "\n",
        "    #  Drop redundant joints (from correlation heatmap)\n",
        "    if drop_redundant:\n",
        "        for c in [\"joint_01\", \"joint_02\", \"joint_05\"]:\n",
        "            if c in joint_cols:\n",
        "                drop.add(c)\n",
        "\n",
        "    # Drop near-zero variance joints (joint_13‚Äì25)\n",
        "    if drop_near_zero:\n",
        "        for i in range(13, 26):\n",
        "            c = f\"joint_{i:02d}\"\n",
        "            if c in joint_cols:\n",
        "                drop.add(c)\n",
        "\n",
        "    # (Optional) Drop low-variance but not-zero joints (joint_26‚Äì29)\n",
        "    if drop_low_var:\n",
        "        for i in range(26, 30):\n",
        "            c = f\"joint_{i:02d}\"\n",
        "            if c in joint_cols:\n",
        "                drop.add(c)\n",
        "\n",
        "    # apply\n",
        "    kept = [c for c in joint_cols if c not in drop]\n",
        "    df_out = df.drop(columns=list(drop), errors=\"ignore\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[preprocess_joints] start={len(joint_cols)} | kept={len(kept)} | dropped={len(drop)}\")\n",
        "        if drop:\n",
        "            print(\"  ‚Ä¢ dropped:\", sorted(list(drop)))\n",
        "\n",
        "    return df_out, kept\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.961724Z",
          "iopub.status.idle": "2025-11-14T23:52:02.962155Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.961956Z",
          "shell.execute_reply": "2025-11-14T23:52:02.961978Z"
        },
        "id": "IteRlov3vylZ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÑ **Data Preprocessing**"
      ],
      "metadata": {
        "id": "wTYIyyCivylZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sequences(\n",
        "    df: pd.DataFrame,\n",
        "    y: pd.DataFrame | np.ndarray | None = None,\n",
        "    window: int | None = None,\n",
        "    stride: int | None = None,\n",
        "    pad: bool = False,\n",
        "    add_time_features: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Build sequences from the dataset, either:\n",
        "      - full-length per sample_index (when window/stride are None), or\n",
        "      - sliding windows with given window and stride.\n",
        "\n",
        "    Data assumptions for THIS notebook:\n",
        "      ‚Ä¢ df already normalized/mapped (categoricals numeric; e.g., n_legs/hands/eyes ‚àà {0,1})\n",
        "      ‚Ä¢ df has columns: ['sample_index','time', joint_*, pain_survey_*, n_legs, n_hands, n_eyes]\n",
        "      ‚Ä¢ each sample_index has T=160 rows (fixed-length), but we still allow windowing/stride\n",
        "\n",
        "    Returns:\n",
        "        dataset: np.ndarray of shape (N,T,F) or (N,window,F)\n",
        "        labels:  np.ndarray of shape (N,) if y is provided, else None\n",
        "    \"\"\"\n",
        "    # ------------------------------------------------------------------\n",
        "    # Feature groups (already numeric at this stage)\n",
        "    joint_cols  = [c for c in df.columns if c.startswith('joint_')]\n",
        "    pain_cols   = [c for c in df.columns if c.startswith('pain_survey_')]\n",
        "    static_cols = [c for c in ['n_legs', 'n_hands', 'n_eyes'] if c in df.columns]\n",
        "\n",
        "    # Keep only the necessary columns in a copy; preserve order\n",
        "    cols_needed = ['sample_index', 'time'] + joint_cols + pain_cols + static_cols\n",
        "    df = df[cols_needed].copy()\n",
        "\n",
        "    # Sort to preserve chronological order within each sequence\n",
        "    df = df.sort_values([\"sample_index\", \"time\"])\n",
        "\n",
        "    # If labels are provided, build a lookup dictionary: sample_index ‚Üí label\n",
        "    label_dict = None\n",
        "    if y is not None:\n",
        "        if isinstance(y, np.ndarray):\n",
        "            # Build mapping using the unique order of sample_index in df\n",
        "            unique_ids = df[\"sample_index\"].unique()\n",
        "            label_dict = {sid: int(lbl) for sid, lbl in zip(unique_ids, y)}\n",
        "        elif isinstance(y, pd.DataFrame):\n",
        "            # Expect columns ['sample_index','label'] with already-int-mapped labels\n",
        "            label_dict = dict(zip(y[\"sample_index\"], y[\"label\"]))\n",
        "\n",
        "    # Prepare outputs\n",
        "    dataset = []\n",
        "    labels  = []\n",
        "\n",
        "    # If no window/stride provided ‚Üí fall back to full-length per sequence\n",
        "    full_length_mode = (window is None or stride is None)\n",
        "\n",
        "    # Iterate over each sequence\n",
        "    for sid, group in df.groupby(\"sample_index\", sort=False):\n",
        "        # --- Extract groups (preserve types for embeddings) ---\n",
        "        X_joints = group[joint_cols].to_numpy(dtype=np.float32)        # (T, J) - continuous features\n",
        "\n",
        "        # IMPORTANT: Pain survey features are categorical indices {0,1,2}\n",
        "        # Keep as int64 first, then convert to float32 to preserve exact integer values\n",
        "        X_pain = group[pain_cols].to_numpy(dtype=np.int64)             # (T, 4) - categorical indices\n",
        "        X_pain = X_pain.astype(np.float32)                              # Convert to float32 but keep 0.0, 1.0, 2.0\n",
        "\n",
        "        # IMPORTANT: Static features are categorical indices {0,1}\n",
        "        # Keep as int64 first, then convert to float32 to preserve exact integer values\n",
        "        if static_cols:\n",
        "            X_static = group[static_cols].to_numpy(dtype=np.int64)     # (T, 3) - categorical indices\n",
        "            X_static = X_static.astype(np.float32)                      # Convert to float32 but keep 0.0, 1.0\n",
        "        else:\n",
        "            X_static = None\n",
        "\n",
        "\n",
        "        # Time features: extract normalized time + sinusoidal encoding\n",
        "        if add_time_features:\n",
        "            time_values = group['time'].to_numpy(dtype=np.float32)\n",
        "            max_time = time_values.max()\n",
        "            normalized_time = time_values / max_time if max_time > 0 else time_values\n",
        "            time_sin = np.sin(2 * np.pi * normalized_time)\n",
        "            time_cos = np.cos(2 * np.pi * normalized_time)\n",
        "            X_time = np.stack([normalized_time, time_sin, time_cos], axis=1)  # (T, 3)\n",
        "        else:\n",
        "            X_time = None\n",
        "\n",
        "        # Concatenate all feature groups along last dimension\n",
        "        if X_static is not None:\n",
        "            X_full = np.concatenate([X_joints, X_pain, X_static], axis=1)  # (T, F_total)\n",
        "        else:\n",
        "            X_full = np.concatenate([X_joints, X_pain], axis=1)            # (T, F_total)\n",
        "\n",
        "        # Add time features if enabled\n",
        "        if X_time is not None:\n",
        "            X_full = np.concatenate([X_full, X_time], axis=1)              # (T, F_total + 3)\n",
        "\n",
        "        T = X_full.shape[0]\n",
        "\n",
        "        if full_length_mode:\n",
        "            # ----- FULL-LENGTH MODE -----\n",
        "            dataset.append(X_full)\n",
        "            if label_dict is not None and sid in label_dict:\n",
        "                labels.append(int(label_dict[sid]))\n",
        "        else:\n",
        "            # ----- WINDOWED MODE (window, stride) -----\n",
        "            W = int(window)\n",
        "            S = int(stride)\n",
        "            assert W > 0 and S > 0, \"window and stride must be positive integers\"\n",
        "\n",
        "            if pad and T % W != 0:\n",
        "                # pad at the end with zeros to allow the last partial window\n",
        "                pad_len = (W - (T % W)) % W\n",
        "                if pad_len > 0:\n",
        "                    X_pad = np.zeros((pad_len, X_full.shape[1]), dtype=np.float32)\n",
        "                    X_seq = np.concatenate([X_full, X_pad], axis=0)\n",
        "                else:\n",
        "                    X_seq = X_full\n",
        "                Tmax = X_seq.shape[0]\n",
        "                idx = 0\n",
        "                while idx + W <= Tmax:\n",
        "                    dataset.append(X_seq[idx:idx+W])\n",
        "                    if label_dict is not None and sid in label_dict:\n",
        "                        labels.append(int(label_dict[sid]))\n",
        "                    idx += S\n",
        "            else:\n",
        "                # no padding ‚Üí only windows fully inside the sequence\n",
        "                idx = 0\n",
        "                while idx + W <= T:\n",
        "                    dataset.append(X_full[idx:idx+W])\n",
        "                    if label_dict is not None and sid in label_dict:\n",
        "                        labels.append(int(label_dict[sid]))\n",
        "                    idx += S\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    dataset = np.asarray(dataset, dtype=np.float32) if len(dataset) > 0 else np.empty((0, 0, 0), dtype=np.float32)\n",
        "    labels  = np.asarray(labels,  dtype=np.int64)   if len(labels)  > 0 else None\n",
        "\n",
        "    if dataset.size > 0:\n",
        "        print(f\"Built {len(dataset)} sequence{'s' if len(dataset)!=1 else ''}; each shape = {dataset[0].shape}\")\n",
        "    else:\n",
        "        print(\"Built 0 sequences (check window/stride vs sequence length).\")\n",
        "\n",
        "    return dataset, labels\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.964115Z",
          "iopub.status.idle": "2025-11-14T23:52:02.964569Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.964419Z",
          "shell.execute_reply": "2025-11-14T23:52:02.964435Z"
        },
        "id": "X1oj12n1vyla",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_loader(ds, batch_size, shuffle, drop_last, sampler=None):\n",
        "    # Determine optimal number of worker processes for data loading\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "\n",
        "    final_shuffle = shuffle\n",
        "    if sampler is not None:\n",
        "        final_shuffle = False\n",
        "\n",
        "    # Create DataLoader with performance optimizations\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=final_shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        sampler=sampler,\n",
        "        pin_memory=True,  # Faster GPU transfer\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,  # Load 4 batches aheads\n",
        "    )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.966114Z",
          "iopub.status.idle": "2025-11-14T23:52:02.966523Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.966317Z",
          "shell.execute_reply": "2025-11-14T23:52:02.966335Z"
        },
        "id": "3y2Tfs6Pvylb",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è **Model Building**"
      ],
      "metadata": {
        "id": "BFp8SSwFvylh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recurrent_summary(model, input_size):\n",
        "    \"\"\"\n",
        "    Custom summary function that emulates torchinfo's output while correctly\n",
        "    counting parameters for RNN/GRU/LSTM layers.\n",
        "\n",
        "    This function is designed for models whose direct children are\n",
        "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to analyze.\n",
        "        input_size (tuple): Shape of the input tensor (e.g., (seq_len, features)).\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to store output shapes captured by forward hooks\n",
        "    output_shapes = {}\n",
        "    # List to track hook handles for later removal\n",
        "    hooks = []\n",
        "\n",
        "    def get_hook(name):\n",
        "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
        "        def hook(module, input, output):\n",
        "            # Handle RNN layer outputs (returns a tuple)\n",
        "            if isinstance(output, tuple):\n",
        "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
        "                shape1 = list(output[0].shape)\n",
        "                shape1[0] = -1  # Replace batch dimension with -1\n",
        "\n",
        "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
        "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
        "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
        "                else:  # RNN/GRU case: h_n only\n",
        "                    shape2 = list(output[1].shape)\n",
        "\n",
        "                # Replace batch dimension (middle position) with -1\n",
        "                shape2[1] = -1\n",
        "\n",
        "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
        "\n",
        "            # Handle standard layer outputs (e.g., Linear)\n",
        "            else:\n",
        "                shape = list(output.shape)\n",
        "                shape[0] = -1  # Replace batch dimension with -1\n",
        "                output_shapes[name] = f\"{shape}\"\n",
        "        return hook\n",
        "\n",
        "    # 1. Determine the device where model parameters reside\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
        "\n",
        "    # 2. Create a dummy input tensor with batch_size=1\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "\n",
        "    # 3. Register forward hooks on target layers\n",
        "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
        "            # Register the hook and store its handle for cleanup\n",
        "            hook_handle = module.register_forward_hook(get_hook(name))\n",
        "            hooks.append(hook_handle)\n",
        "\n",
        "    # 4. Execute a dummy forward pass in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            model(dummy_input)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during dummy forward pass: {e}\")\n",
        "            # Clean up hooks even if an error occurs\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "            return\n",
        "\n",
        "    # 5. Remove all registered hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # --- 6. Print the summary table ---\n",
        "\n",
        "    print(\"-\" * 79)\n",
        "    # Column headers\n",
        "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
        "    print(\"=\" * 79)\n",
        "\n",
        "    total_params = 0\n",
        "    total_trainable_params = 0\n",
        "\n",
        "    # Iterate through modules again to collect and display parameter information\n",
        "    for name, module in model.named_children():\n",
        "        if name in output_shapes:\n",
        "            # Count total and trainable parameters for this module\n",
        "            module_params = sum(p.numel() for p in module.parameters())\n",
        "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "            total_params += module_params\n",
        "            total_trainable_params += trainable_params\n",
        "\n",
        "            # Format strings for display\n",
        "            layer_name = f\"{name} ({type(module).__name__})\"\n",
        "            output_shape_str = str(output_shapes[name])\n",
        "            params_str = f\"{trainable_params:,}\"\n",
        "\n",
        "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
        "\n",
        "    print(\"=\" * 79)\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
        "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
        "    print(\"-\" * 79)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.970736Z",
          "iopub.status.idle": "2025-11-14T23:52:02.971299Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.970970Z",
          "shell.execute_reply": "2025-11-14T23:52:02.970999Z"
        },
        "id": "mPtTi-1gvylh",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "class RecurrentClassifier(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            num_classes,\n",
        "            rnn_type='GRU',\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.2,\n",
        "            rec_dropout_rate=None,\n",
        "            cnn_channels=None,\n",
        "            cnn_kernel_size=3,\n",
        "            cnn_dropout=None,\n",
        "            use_pain_embeddings=True,\n",
        "            pain_embedding_dim=4,\n",
        "            num_joint_features=29,\n",
        "            num_pain_features=4,\n",
        "            num_static_features=3,\n",
        "            num_time_features=3,\n",
        "            use_attention=True\n",
        "            ):\n",
        "        super().__init__()\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.uses_cnn = cnn_channels is not None\n",
        "        self.use_pain_embeddings = use_pain_embeddings\n",
        "        self.use_attention = use_attention\n",
        "        # ---------------------------------------------\n",
        "        # Store feature split indices (same as before)\n",
        "        self.num_joint_features = num_joint_features\n",
        "        self.num_pain_features = num_pain_features\n",
        "        self.num_static_features = num_static_features\n",
        "        self.num_time_features = num_time_features\n",
        "        self.joint_end = num_joint_features\n",
        "        self.pain_end = self.joint_end + num_pain_features\n",
        "        self.static_end = self.pain_end + num_static_features\n",
        "        # ---------------------------------------------\n",
        "        # Check input size\n",
        "        expected_input = num_joint_features + num_pain_features + num_static_features + num_time_features\n",
        "        if input_size != expected_input:\n",
        "            print(f\"WARNING: input_size={input_size} but expected {expected_input}\")\n",
        "        # ---------------------------------------------\n",
        "        # Embeddings (if enabled)\n",
        "        if self.use_pain_embeddings:\n",
        "            self.pain_embeddings = nn.ModuleList([\n",
        "                nn.Embedding(num_embeddings=3, embedding_dim=pain_embedding_dim)\n",
        "                for _ in range(num_pain_features)\n",
        "            ])\n",
        "            effective_input_size = (num_joint_features + num_pain_features * pain_embedding_dim + num_static_features + num_time_features)\n",
        "        else:\n",
        "            effective_input_size = input_size\n",
        "        # ---------------------------------------------\n",
        "        # Optionally CNN block\n",
        "        if self.uses_cnn:\n",
        "            self.cnn1 = nn.Conv1d(in_channels=effective_input_size,\n",
        "                                  out_channels=cnn_channels,\n",
        "                                  kernel_size=cnn_kernel_size,\n",
        "                                  padding=cnn_kernel_size // 2)\n",
        "            self.cnn_bn = nn.BatchNorm1d(cnn_channels)\n",
        "            self.cnn_act = nn.ReLU()\n",
        "            self.cnn_dropout = nn.Dropout(cnn_dropout if cnn_dropout is not None else dropout_rate)\n",
        "            rnn_input_size = cnn_channels\n",
        "        else:\n",
        "            rnn_input_size = effective_input_size\n",
        "        # ---------------------------------------------\n",
        "        # RNN\n",
        "        rnn_map = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=rnn_input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "        # ---------------------------------------------\n",
        "        # Classifier layer (after attention output)\n",
        "        rnn_output_size = hidden_size * (2 if bidirectional else 1)\n",
        "        self.classifier = nn.Linear(rnn_output_size, num_classes)\n",
        "        self.rec_dropout = nn.Dropout(rec_dropout_rate) if rec_dropout_rate is not None else None\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: (batch, 1, d)\n",
        "            key, value: (batch, seq_len, d)\n",
        "        Computes attention weights and context.\n",
        "        \"\"\"\n",
        "        # Compute dot products\n",
        "        scores = torch.matmul(query, key.transpose(1,2))  # (batch, 1, seq_len)\n",
        "        scores = scores / torch.sqrt(torch.tensor(query.size(-1), dtype=torch.float32, device=query.device))\n",
        "        attn_weights = F.softmax(scores, dim=-1)          # (batch, 1, seq_len)\n",
        "        context = torch.matmul(attn_weights, value)        # (batch, 1, d)\n",
        "        context = context.squeeze(1)                      # (batch, d)\n",
        "        return context, attn_weights.squeeze(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # --- Embedding and feature split ---\n",
        "        if self.use_pain_embeddings:\n",
        "            X_joints = x[:, :, :self.joint_end]\n",
        "            X_pain = x[:, :, self.joint_end:self.pain_end]\n",
        "            X_static = x[:, :, self.pain_end:self.static_end]\n",
        "            X_time = x[:, :, self.static_end:]\n",
        "            pain_embedded_list = [self.pain_embeddings[i](X_pain[:,:,i].long()) for i in range(self.num_pain_features)]\n",
        "            X_pain_embedded = torch.cat(pain_embedded_list, dim=-1)\n",
        "            x = torch.cat([X_joints, X_pain_embedded, X_static, X_time], dim=-1)\n",
        "        # --- Optionally CNN preprocessing ---\n",
        "        if self.uses_cnn:\n",
        "            x = x.transpose(1, 2)\n",
        "            x = self.cnn1(x)\n",
        "            x = self.cnn_bn(x)\n",
        "            x = self.cnn_act(x)\n",
        "            x = self.cnn_dropout(x)\n",
        "            x = x.transpose(1, 2)\n",
        "        # --- RNN ---\n",
        "        rnn_out, hidden = self.rnn(x)  # rnn_out: (batch, seq_len, hidden_size * directions)\n",
        "\n",
        "        # Use last hidden state as query for attention\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]\n",
        "        if self.bidirectional:\n",
        "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
        "            final_hidden = torch.cat([hidden[-1,0,:,:], hidden[-1,1,:,:]], dim=1)  # (batch, rnn_output_size)\n",
        "        else:\n",
        "            final_hidden = hidden[-1]  # (batch, rnn_output_size)\n",
        "        # --- Attention: query=final_hidden, key/value=all rnn_out ---\n",
        "        if self.use_attention:\n",
        "            query = final_hidden.unsqueeze(1)  # (batch, 1, rnn_output_size)\n",
        "            context, attn_weights = self.scaled_dot_product_attention(query, rnn_out, rnn_out)\n",
        "        else:\n",
        "            context = final_hidden\n",
        "\n",
        "        # --- Dropout and classification ---\n",
        "        if self.rec_dropout is not None:\n",
        "            context = self.rec_dropout(context)\n",
        "        logits = self.classifier(context)  # (batch, num_classes)\n",
        "        return logits  # Optionally also return attn_weights if you want visualization\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.972810Z",
          "iopub.status.idle": "2025-11-14T23:52:02.973085Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.972961Z",
          "shell.execute_reply": "2025-11-14T23:52:02.972973Z"
        },
        "id": "T0TEXRhBvylh",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† **Model Training**"
      ],
      "metadata": {
        "id": "4FcJNPTGvyli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_loss (float): Training loss for this epoch\n",
        "        train_f1 (float): Training f1 score for this epoch\n",
        "        val_loss (float): Validation loss for this epoch\n",
        "        val_f1 (float): Validation f1 score for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "\n",
        "    Note:\n",
        "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
        "        parameters and gradients, which helps monitor training progress and detect\n",
        "        issues like vanishing/exploding gradients.\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty before adding a histogram\n",
        "            if param.numel() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty before adding a histogram\n",
        "                if param.grad.numel() > 0:\n",
        "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
        "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
      ],
      "metadata": {
        "id": "stz3IPG2vo1l",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.975270Z",
          "iopub.status.idle": "2025-11-14T23:52:02.975714Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.975496Z",
          "shell.execute_reply": "2025-11-14T23:52:02.975514Z"
        }
      },
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize best model tracking variables\n",
        "best_model = None\n",
        "best_performance = float('-inf')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.977623Z",
          "iopub.status.idle": "2025-11-14T23:52:02.978052Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.977832Z",
          "shell.execute_reply": "2025-11-14T23:52:02.977852Z"
        },
        "id": "-Hdq3v00vyli",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler,\n",
        "                    device, l1_lambda=0, l2_lambda=0,max_grad_norm=1.0):\n",
        "    \"\"\"\n",
        "    Perform one complete training epoch through the entire training dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): Lambda for L1 regularization\n",
        "        l2_lambda (float): Lambda for L2 regularization\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Clear gradients from previous step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass with mixed precision (if CUDA available)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Add L1 and L2 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        if scaler is not None and device.type == 'cuda':\n",
        "            scaler.scale(loss).backward()            # grads are scaled\n",
        "            scaler.unscale_(optimizer)               # unscale to true grad values\n",
        "            torch.nn.utils.clip_grad_norm_(          # CLIP true gradients (magnitude cap)\n",
        "                model.parameters(), max_norm=max_grad_norm\n",
        "            )\n",
        "            scaler.step(optimizer)                   # safe optimizer.step() (skips on inf/NaN)\n",
        "            scaler.update()                          # update scaling factor\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='macro'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_f1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.979939Z",
          "iopub.status.idle": "2025-11-14T23:52:02.980372Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.980163Z",
          "shell.execute_reply": "2025-11-14T23:52:02.980181Z"
        },
        "id": "ZlRL0wYDvyli",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
        "\n",
        "    Note:\n",
        "        This function automatically sets the model to evaluation mode and disables\n",
        "        gradient computation for efficiency during validation.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision (if CUDA available)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='macro'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.981335Z",
          "iopub.status.idle": "2025-11-14T23:52:02.981617Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.981482Z",
          "shell.execute_reply": "2025-11-14T23:52:02.981494Z"
        },
        "id": "ht1PGUw9vyli",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, train_loader, val_loader, epochs, train_criterion, val_criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, scheduler=None, # Added scheduler parameter\n",
        "        evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=1, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Train the neural network model on the training data and validate on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        epochs (int): Number of training epochs\n",
        "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
        "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
        "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
        "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
        "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
        "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
        "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
        "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
        "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, training_history) - Trained model and metrics history\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping if patience is set\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop: iterate through epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Forward pass through training data, compute gradients, update weights\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, train_criterion, optimizer, scaler, device\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation data without updating weights\n",
        "        if val_loader is not None:\n",
        "            val_loss, val_f1 = validate_one_epoch(model, val_loader, val_criterion, device)\n",
        "        else:\n",
        "            val_loss, val_f1 = None, None\n",
        "\n",
        "\n",
        "        # Step the scheduler if provided (typically after validation)\n",
        "        if scheduler is not None:\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) or isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
        "                scheduler.step(val_f1)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "        # Store metrics for plotting and analysis\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Write metrics to TensorBoard for visualization\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
        "            )\n",
        "\n",
        "        # Print progress every N epochs or on first epoch\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "                if val_loss is not None:\n",
        "                    print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                          f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
        "                          f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                          f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f}\")\n",
        "\n",
        "\n",
        "        # Early stopping logic: monitor metric and save best model\n",
        "        if patience > 0 and val_loader is not None:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "\n",
        "    # Restore best model weights if early stopping was used\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "\n",
        "    # Save final model if no early stopping\n",
        "    if patience == 0:\n",
        "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "\n",
        "    if patience > 0:\n",
        "        training_history['best_epoch'] = best_epoch\n",
        "        training_history['best_metric'] = best_metric\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.982449Z",
          "iopub.status.idle": "2025-11-14T23:52:02.982880Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.982591Z",
          "shell.execute_reply": "2025-11-14T23:52:02.982602Z"
        },
        "id": "fXJVMsPOvyli",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 38
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è **Ensembling**"
      ],
      "metadata": {
        "id": "dYGu7ayRtmX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# UTILITY FUNCTIONS FOR VISUALIZATION\n",
        "# ============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def plot_training_history(history, config_name, fold_idx):\n",
        "    \"\"\"Plot training and validation loss/F1\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    ax1.plot(history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "    ax1.plot(history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
        "    ax1.set_title(f'{config_name} - Fold {fold_idx} - Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(alpha=0.3)\n",
        "\n",
        "    # F1 plot\n",
        "    ax2.plot(history['train_f1'], label='Training F1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
        "    ax2.plot(history['val_f1'], label='Validation F1', alpha=0.9, color='#ff7f0e')\n",
        "    ax2.set_title(f'{config_name} - Fold {fold_idx} - F1 Score')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('F1 Score')\n",
        "    ax2.legend()\n",
        "    ax2.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_and_plot_confusion_matrix(model, val_loader, X_val, window, stride, config_name, fold_idx, device):\n",
        "    \"\"\"Evaluate model on validation set and plot confusion matrix\"\"\"\n",
        "    # Collect window-level predictions\n",
        "    val_preds_windows = []\n",
        "    val_targets_windows = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "            val_preds_windows.append(preds)\n",
        "            val_targets_windows.append(yb.numpy())\n",
        "\n",
        "    val_preds_windows = np.concatenate(val_preds_windows)\n",
        "    val_targets_windows = np.concatenate(val_targets_windows)\n",
        "\n",
        "    # Aggregate windows to sequences (majority vote)\n",
        "    n_windows_per_seq = (160 - window) // stride + 1\n",
        "    unique_samples = sorted(X_val['sample_index'].unique())\n",
        "\n",
        "    sequence_preds = {}\n",
        "    sequence_targets = {}\n",
        "\n",
        "    for idx, sid in enumerate(unique_samples):\n",
        "        start_idx = idx * n_windows_per_seq\n",
        "        end_idx = start_idx + n_windows_per_seq\n",
        "\n",
        "        window_preds = val_preds_windows[start_idx:end_idx]\n",
        "        window_targets = val_targets_windows[start_idx:end_idx]\n",
        "\n",
        "        # Majority vote\n",
        "        vote_counts = Counter(window_preds)\n",
        "        final_pred = vote_counts.most_common(1)[0][0]\n",
        "\n",
        "        # Sanity check\n",
        "        assert len(np.unique(window_targets)) == 1, f\"Sample {sid} has inconsistent labels!\"\n",
        "        final_target = window_targets[0]\n",
        "\n",
        "        sequence_preds[sid] = final_pred\n",
        "        sequence_targets[sid] = final_target\n",
        "\n",
        "    # Convert to arrays\n",
        "    val_preds = np.array([sequence_preds[sid] for sid in unique_samples])\n",
        "    val_targets = np.array([sequence_targets[sid] for sid in unique_samples])\n",
        "\n",
        "    # Calculate metrics\n",
        "    val_acc = accuracy_score(val_targets, val_preds)\n",
        "    val_prec = precision_score(val_targets, val_preds, average='macro', zero_division=0)\n",
        "    val_rec = recall_score(val_targets, val_preds, average='macro', zero_division=0)\n",
        "    val_f1 = f1_score(val_targets, val_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\n  {config_name} - Fold {fold_idx} Validation Metrics (Sequence-Level):\")\n",
        "    print(f\"    Accuracy:  {val_acc:.4f}\")\n",
        "    print(f\"    Precision: {val_prec:.4f}\")\n",
        "    print(f\"    Recall:    {val_rec:.4f}\")\n",
        "    print(f\"    F1 Score:  {val_f1:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(val_targets, val_preds)\n",
        "    labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 7))\n",
        "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues',\n",
        "                xticklabels=['no_pain', 'low_pain', 'high_pain'],\n",
        "                yticklabels=['no_pain', 'low_pain', 'high_pain'])\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title(f'{config_name} - Fold {fold_idx}\\nConfusion Matrix ‚Äî Validation Set (Sequence-Level)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return val_f1\n",
        "\n",
        "print(\"‚úì Utility functions defined\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "4ISMjvkttmX4",
        "outputId": "9221935f-eb4c-4559-ad0c-4140c031d195",
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.984632Z",
          "iopub.status.idle": "2025-11-14T23:52:02.985071Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.984866Z",
          "shell.execute_reply": "2025-11-14T23:52:02.984884Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Utility functions defined\n"
          ]
        }
      ],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": [
        "ENSEMBLE_CONFIGS = [\n",
        "    # 1. Nessun CNN, nessuna attention => modello grande\n",
        "    {\n",
        "        \"name\": \"gru_big_no_cnn_no_att\",\n",
        "        \"seed\": 101,\n",
        "        \"lr\": 3e-4,\n",
        "        \"batch_size\": 256,\n",
        "        \"epochs\": 100,\n",
        "        \"patience\": 20,\n",
        "        \"hidden_layers\": 3,\n",
        "        \"hidden_size\": 256,\n",
        "        \"dropout\": 0.4,\n",
        "        \"rec_dropout\": 0.25,\n",
        "        \"l2\": 3e-5,\n",
        "        \"window\": 8,\n",
        "        \"stride\": 2,\n",
        "        \"rnn_type\": \"GRU\",\n",
        "        \"bidirectional\": True,\n",
        "        \"cnn_channels\": None,\n",
        "        \"cnn_kernel\": 3,\n",
        "        \"cnn_dropout\": None,\n",
        "        \"use_attention\": False,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"lstm_big_no_cnn_no_att\",\n",
        "        \"seed\": 102,\n",
        "        \"lr\": 3e-4,\n",
        "        \"batch_size\": 256,\n",
        "        \"epochs\": 100,\n",
        "        \"patience\": 20,\n",
        "        \"hidden_layers\": 3,\n",
        "        \"hidden_size\": 256,\n",
        "        \"dropout\": 0.4,\n",
        "        \"rec_dropout\": 0.25,\n",
        "        \"l2\": 3e-5,\n",
        "        \"window\": 8,\n",
        "        \"stride\": 2,\n",
        "        \"rnn_type\": \"LSTM\",\n",
        "        \"bidirectional\": True,\n",
        "        \"cnn_channels\": None,\n",
        "        \"cnn_kernel\": 3,\n",
        "        \"cnn_dropout\": None,\n",
        "        \"use_attention\": False,\n",
        "    },\n",
        "\n",
        "    # 2. CNN ma senza attention => modello medio-grande con 2 strati e 256 neuroni\n",
        "    {\n",
        "        \"name\": \"gru_medium_large_cnn_no_att\",\n",
        "        \"seed\": 103,\n",
        "        \"lr\": 3e-4,\n",
        "        \"batch_size\": 256,\n",
        "        \"epochs\": 100,\n",
        "        \"patience\": 20,\n",
        "        \"hidden_layers\": 2,\n",
        "        \"hidden_size\": 256,  # aumentato a 256\n",
        "        \"dropout\": 0.3,\n",
        "        \"rec_dropout\": 0.2,\n",
        "        \"l2\": 2e-5,\n",
        "        \"window\": 8,\n",
        "        \"stride\": 2,\n",
        "        \"rnn_type\": \"GRU\",\n",
        "        \"bidirectional\": True,\n",
        "        \"cnn_channels\": 48,\n",
        "        \"cnn_kernel\": 3,\n",
        "        \"cnn_dropout\": 0.3,\n",
        "        \"use_attention\": False,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"lstm_medium_large_cnn_no_att\",\n",
        "        \"seed\": 104,\n",
        "        \"lr\": 3e-4,\n",
        "        \"batch_size\": 256,\n",
        "        \"epochs\": 100,\n",
        "        \"patience\": 20,\n",
        "        \"hidden_layers\": 2,\n",
        "        \"hidden_size\": 256,  # aumentato a 256\n",
        "        \"dropout\": 0.25,\n",
        "        \"rec_dropout\": 0.15,\n",
        "        \"l2\": 2e-5,\n",
        "        \"window\": 8,\n",
        "        \"stride\": 2,\n",
        "        \"rnn_type\": \"LSTM\",\n",
        "        \"bidirectional\": True,\n",
        "        \"cnn_channels\": 48,\n",
        "        \"cnn_kernel\": 3,\n",
        "        \"cnn_dropout\": 0.35,\n",
        "        \"use_attention\": False,\n",
        "    },\n",
        "\n",
        "    # 3. CNN + Attention => modello medio\n",
        "    {\n",
        "        \"name\": \"gru_medium_cnn_att\",\n",
        "        \"seed\": 105,\n",
        "        \"lr\": 3e-4,\n",
        "        \"batch_size\": 256,\n",
        "        \"epochs\": 100,\n",
        "        \"patience\": 20,\n",
        "        \"hidden_layers\": 2,\n",
        "        \"hidden_size\": 192,\n",
        "        \"dropout\": 0.25,\n",
        "        \"rec_dropout\": 0.15,\n",
        "        \"l2\": 2e-5,\n",
        "        \"window\": 8,\n",
        "        \"stride\": 2,\n",
        "        \"rnn_type\": \"GRU\",\n",
        "        \"bidirectional\": True,\n",
        "        \"cnn_channels\": 32,\n",
        "        \"cnn_kernel\": 3,\n",
        "        \"cnn_dropout\": 0.3,\n",
        "        \"use_attention\": True,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"lstm_medium_cnn_att\",\n",
        "        \"seed\": 106,\n",
        "        \"lr\": 3e-4,\n",
        "        \"batch_size\": 256,\n",
        "        \"epochs\": 100,\n",
        "        \"patience\": 20,\n",
        "        \"hidden_layers\": 2,\n",
        "        \"hidden_size\": 192,\n",
        "        \"dropout\": 0.3,\n",
        "        \"rec_dropout\": 0.2,\n",
        "        \"l2\": 2e-5,\n",
        "        \"window\": 8,\n",
        "        \"stride\": 2,\n",
        "        \"rnn_type\": \"LSTM\",\n",
        "        \"bidirectional\": True,\n",
        "        \"cnn_channels\": 32,\n",
        "        \"cnn_kernel\": 3,\n",
        "        \"cnn_dropout\": 0.35,\n",
        "        \"use_attention\": True,\n",
        "    }\n",
        "]\n",
        "\n",
        "label_mapping = {\n",
        "    'no_pain': 0,\n",
        "    'low_pain': 1,\n",
        "    'high_pain': 2\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q_m-mhkEtmX4",
        "execution": {
          "iopub.status.busy": "2025-11-14T23:52:02.987987Z",
          "iopub.status.idle": "2025-11-14T23:52:02.988471Z",
          "shell.execute_reply.started": "2025-11-14T23:52:02.988298Z",
          "shell.execute_reply": "2025-11-14T23:52:02.988318Z"
        }
      },
      "outputs": [],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PHASE 1: 5-FOLD CV WITH SCHEDULER AND VISUALIZATION\n",
        "# ============================================================\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "config_ensemble_predictions = []\n",
        "config_mean_epochs = []\n",
        "config_mean_val_f1 = []\n",
        "all_cv_submissions = []\n",
        "\n",
        "for config_idx, config in enumerate(ENSEMBLE_CONFIGS):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"CONFIG [{config_idx+1}/5]: {config['name']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    fold_predictions = []\n",
        "    fold_stopped_epochs = []\n",
        "    fold_val_f1_scores = []\n",
        "\n",
        "    # Preprocess data once\n",
        "    DF, _ = preprocess_joints(X_TRAIN.copy())\n",
        "    X_train_full, _ = dataset_conversion_type_embed_ready(DF)\n",
        "    y_full = Y_TRAIN.copy()\n",
        "\n",
        "    sample_indices_unique = y_full['sample_index'].values\n",
        "    labels_for_split = y_full['label'].map(label_mapping).values\n",
        "\n",
        "    # 5-Fold CV\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=config['seed'])\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(sample_indices_unique, labels_for_split)):\n",
        "        print(f\"\\n  {'='*60}\")\n",
        "        print(f\"  Fold {fold_idx+1}/5 for {config['name']}\")\n",
        "        print(f\"  {'='*60}\")\n",
        "\n",
        "        # Set seed\n",
        "        torch.manual_seed(config['seed'] + fold_idx)\n",
        "        np.random.seed(config['seed'] + fold_idx)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(config['seed'] + fold_idx)\n",
        "\n",
        "        # Split data\n",
        "        train_samples = sample_indices_unique[train_idx]\n",
        "        val_samples = sample_indices_unique[val_idx]\n",
        "\n",
        "        X_train = X_train_full[X_train_full['sample_index'].isin(train_samples)]\n",
        "        y_train = y_full[y_full['sample_index'].isin(train_samples)]\n",
        "\n",
        "        X_val = X_train_full[X_train_full['sample_index'].isin(val_samples)]\n",
        "        y_val = y_full[y_full['sample_index'].isin(val_samples)]\n",
        "\n",
        "        # Normalize\n",
        "        train_merged = X_train.merge(y_train, on='sample_index')\n",
        "        train_merged['label'] = train_merged['label'].map(label_mapping)\n",
        "\n",
        "        val_merged = X_val.merge(y_val, on='sample_index')\n",
        "        val_merged['label'] = val_merged['label'].map(label_mapping)\n",
        "\n",
        "        scale_columns = [col for col in train_merged.columns if col.startswith('joint_')]\n",
        "        mins = train_merged[scale_columns].min()\n",
        "        maxs = train_merged[scale_columns].max()\n",
        "\n",
        "        for column in scale_columns:\n",
        "            train_merged[column] = (train_merged[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "            val_merged[column] = (val_merged[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "        # Build sequences\n",
        "        y_train_df = pd.DataFrame({\n",
        "            \"sample_index\": train_merged[\"sample_index\"].unique(),\n",
        "            \"label\": train_merged.groupby(\"sample_index\")[\"label\"].first().values\n",
        "        })\n",
        "\n",
        "        y_val_df = pd.DataFrame({\n",
        "            \"sample_index\": val_merged[\"sample_index\"].unique(),\n",
        "            \"label\": val_merged.groupby(\"sample_index\")[\"label\"].first().values\n",
        "        })\n",
        "\n",
        "        X_train_seq, y_train_seq = build_sequences(\n",
        "            train_merged.drop(columns=['label']),\n",
        "            y_train_df,\n",
        "            window=config['window'],\n",
        "            stride=config['stride']\n",
        "        )\n",
        "\n",
        "        X_val_seq, y_val_seq = build_sequences(\n",
        "            val_merged.drop(columns=['label']),\n",
        "            y_val_df,\n",
        "            window=config['window'],\n",
        "            stride=config['stride']\n",
        "        )\n",
        "\n",
        "        # Weighted sampler\n",
        "        labels_np = y_train_seq\n",
        "        class_counts = np.bincount(labels_np, minlength=3)\n",
        "        # class_weights_for_sampling = 1.0 / (class_counts + 1e-8)\n",
        "        # sample_weights = class_weights_for_sampling[labels_np]\n",
        "\n",
        "                # SQRT dampening\n",
        "        max_count = class_counts.max()\n",
        "        class_weights_raw = max_count / class_counts\n",
        "        # class_weights_dampened = np.sqrt(class_weights_raw)\n",
        "\n",
        "        # Convert to tensor (CRITICAL!)\n",
        "        class_weights = torch.tensor(class_weights_raw, dtype=torch.float32)\n",
        "\n",
        "\n",
        "        # DataLoaders\n",
        "        train_ds = TensorDataset(\n",
        "            torch.from_numpy(X_train_seq).float(),\n",
        "            torch.from_numpy(y_train_seq).long()\n",
        "        )\n",
        "\n",
        "        val_ds = TensorDataset(\n",
        "            torch.from_numpy(X_val_seq).float(),\n",
        "            torch.from_numpy(y_val_seq).long()\n",
        "        )\n",
        "\n",
        "        train_loader = make_loader(train_ds, batch_size=config['batch_size'],\n",
        "                                   shuffle=False, drop_last=False, sampler=sampler)\n",
        "        val_loader = make_loader(val_ds, batch_size=config['batch_size'],\n",
        "                                shuffle=False, drop_last=False)\n",
        "\n",
        "        # Model\n",
        "        model = RecurrentClassifier(\n",
        "            input_size=X_train_seq.shape[2],\n",
        "            hidden_size=config['hidden_size'],\n",
        "            num_layers=config['hidden_layers'],\n",
        "            num_classes=3,\n",
        "            dropout_rate=config['dropout'],\n",
        "            rec_dropout_rate=config['rec_dropout'],\n",
        "            bidirectional=config['bidirectional'],\n",
        "            cnn_channels=config['cnn_channels'],\n",
        "            cnn_kernel_size=config['cnn_kernel'],\n",
        "            cnn_dropout=config['cnn_dropout'],\n",
        "            rnn_type=config['rnn_type'],\n",
        "            use_attention=config['use_attention'],\n",
        "            num_joint_features=29,\n",
        "            num_pain_features=4,\n",
        "            num_static_features=3,\n",
        "            num_time_features=3\n",
        "        ).to(device)\n",
        "\n",
        "\n",
        "        # Optimizer & Loss\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['l2'])\n",
        "        train_criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1).to(device)\n",
        "\n",
        "        val_criterion = nn.CrossEntropyLoss().to(device)\n",
        "        scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "        # ‚úì SCHEDULER\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='max',\n",
        "            factor=0.7,      # For example\n",
        "            patience=7,\n",
        "            min_lr=1e-6      # Prevents LR going too low\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        model, history = fit(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=config['epochs'],\n",
        "            train_criterion=train_criterion,\n",
        "            val_criterion=val_criterion,\n",
        "            optimizer=optimizer,\n",
        "            scaler=scaler,\n",
        "            device=device,\n",
        "            patience=config['patience'],\n",
        "            verbose=10,\n",
        "            evaluation_metric='val_f1',\n",
        "            mode='max',\n",
        "            restore_best_weights=True,\n",
        "            experiment_name=f\"{config['name']}_fold{fold_idx+1}\",\n",
        "            scheduler=scheduler\n",
        "        )\n",
        "\n",
        "        # ‚úì VISUALIZE\n",
        "        plot_training_history(history, config['name'], fold_idx+1)\n",
        "        val_f1 = evaluate_and_plot_confusion_matrix(\n",
        "            model, val_loader, X_val,\n",
        "            config['window'], config['stride'],\n",
        "            config['name'], fold_idx+1, device\n",
        "        )\n",
        "\n",
        "        # Track metrics\n",
        "        stopped_epoch = len(history['val_f1'])\n",
        "        fold_stopped_epochs.append(stopped_epoch)\n",
        "        fold_val_f1_scores.append(val_f1)\n",
        "\n",
        "        os.makedirs(\"models\", exist_ok=True)\n",
        "        # Save model after training\n",
        "        save_path = f\"models/{config['name']}_fold{fold_idx+1}.pth\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"‚úì Saved model: {save_path}\")\n",
        "\n",
        "\n",
        "        # Predict on test\n",
        "        X_test = pd.read_csv(DATASET_ROOT / \"pirate_pain_test.csv\")\n",
        "        DF_test, _ = preprocess_joints(X_test.copy())\n",
        "        X_test, _ = dataset_conversion_type_embed_ready(DF_test)\n",
        "\n",
        "        for column in scale_columns:\n",
        "            X_test[column] = (X_test[column] - mins[column]) / (maxs[column] - mins[column])\n",
        "\n",
        "        X_test_seq, _ = build_sequences(X_test, None, window=config['window'], stride=config['stride'])\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = []\n",
        "            for i_batch in range(0, len(X_test_seq), 256):\n",
        "                xb = torch.from_numpy(X_test_seq[i_batch:i_batch+256]).float().to(device)\n",
        "                logits.append(model(xb).cpu().numpy())\n",
        "            logits = np.concatenate(logits, axis=0)\n",
        "\n",
        "        # Aggregate\n",
        "        n_windows_per_seq = (160 - config['window']) // config['stride'] + 1\n",
        "        sample_indices = sorted(X_test['sample_index'].unique())\n",
        "\n",
        "        sequence_preds = []\n",
        "        for idx in range(len(sample_indices)):\n",
        "            start_idx = idx * n_windows_per_seq\n",
        "            end_idx = start_idx + n_windows_per_seq\n",
        "\n",
        "            window_preds = logits[start_idx:end_idx].argmax(axis=1)\n",
        "            vote_counts = Counter(window_preds)\n",
        "            final_pred = vote_counts.most_common(1)[0][0]\n",
        "            sequence_preds.append(final_pred)\n",
        "\n",
        "        fold_predictions.append(np.array(sequence_preds))\n",
        "\n",
        "    # Config summary\n",
        "    mean_epoch = int(np.mean(fold_stopped_epochs))\n",
        "    mean_f1 = np.mean(fold_val_f1_scores)\n",
        "    config_mean_epochs.append(mean_epoch)\n",
        "    config_mean_val_f1.append(mean_f1)\n",
        "    print(f\"\\n  ‚úì {config['name']} SUMMARY: mean_epoch={mean_epoch}, mean_val_f1={mean_f1:.4f}\")\n",
        "\n",
        "    # Level 1 ensemble\n",
        "    stacked_folds = np.stack(fold_predictions, axis=0)\n",
        "    config_ensemble_preds = []\n",
        "    for i in range(stacked_folds.shape[1]):\n",
        "        votes = stacked_folds[:, i]\n",
        "        vote_counts = Counter(votes)\n",
        "        config_ensemble_preds.append(vote_counts.most_common(1)[0][0])\n",
        "\n",
        "    config_ensemble_predictions.append(np.array(config_ensemble_preds))\n",
        "\n",
        "    # Save\n",
        "    idx2label = {0: 'no_pain', 1: 'low_pain', 2: 'high_pain'}\n",
        "    pred_labels = [idx2label[int(p)] for p in config_ensemble_preds]\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'sample_index': [str(sid).zfill(3) for sid in sample_indices],\n",
        "        'label': pred_labels\n",
        "    })\n",
        "\n",
        "    cv_submission_filename = f'{config[\"name\"]}_cv_ensemble.csv'\n",
        "    submission.to_csv(cv_submission_filename, index=False)\n",
        "    all_cv_submissions.append(cv_submission_filename)\n",
        "\n",
        "print(\"\\n‚úì Phase 1 complete\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "bmut_VentmX4",
        "outputId": "287c102c-cbde-484e-caaf-47cd58c577a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CONFIG [1/5]: gru_big_no_cnn_no_att\n",
            "======================================================================\n",
            "[preprocess_joints] start=31 | kept=29 | dropped=2\n",
            "  ‚Ä¢ dropped: ['joint_11', 'joint_30']\n",
            "\n",
            "  ============================================================\n",
            "  Fold 1/5 for gru_big_no_cnn_no_att\n",
            "  ============================================================\n",
            "Built 40656 sequences; each shape = (8, 39)\n",
            "Built 10241 sequences; each shape = (8, 39)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'class_sample_counts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-917296093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mlabels_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mclass_weights_for_sampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclass_sample_counts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights_for_sampling\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels_np\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'class_sample_counts' is not defined"
          ]
        }
      ],
      "execution_count": 41
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PHASE 2: CV META-ENSEMBLES\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PHASE 2: CV META-ENSEMBLES\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "sorted_indices = np.argsort(config_mean_val_f1)[::-1]\n",
        "print(\"\\nConfig ranking by mean val_f1:\")\n",
        "for rank, idx in enumerate(sorted_indices, 1):\n",
        "    print(f\"  {rank}. {ENSEMBLE_CONFIGS[idx]['name']}: {config_mean_val_f1[idx]:.4f}\")\n",
        "\n",
        "def create_cv_ensemble(indices, name_suffix, predictions_source):\n",
        "    \"\"\"Create ensemble from CV predictions\"\"\"\n",
        "    selected_preds = [predictions_source[i] for i in indices]\n",
        "    stacked = np.stack(selected_preds, axis=0)\n",
        "\n",
        "    ensemble_preds = []\n",
        "    for i in range(stacked.shape[1]):\n",
        "        votes = stacked[:, i]\n",
        "        vote_counts = Counter(votes)\n",
        "        ensemble_preds.append(vote_counts.most_common(1)[0][0])\n",
        "\n",
        "    idx2label = {0: 'no_pain', 1: 'low_pain', 2: 'high_pain'}\n",
        "    pred_labels = [idx2label[int(p)] for p in ensemble_preds]\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'sample_index': [str(sid).zfill(3) for sid in sample_indices],\n",
        "        'label': pred_labels\n",
        "    })\n",
        "\n",
        "    filename = f'cv_ensemble_{name_suffix}.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "    print(f\"‚úì Saved: {filename}\")\n",
        "    return filename\n",
        "\n",
        "# Assume you have 6 models: config_ensemble_predictions is length 6\n",
        "cv_ensemble_files = []\n",
        "cv_ensemble_files.append(create_cv_ensemble(sorted_indices[:3], \"top3\", config_ensemble_predictions))\n",
        "cv_ensemble_files.append(create_cv_ensemble(sorted_indices[:4], \"top4\", config_ensemble_predictions))\n",
        "cv_ensemble_files.append(create_cv_ensemble(sorted_indices[:5], \"top5\", config_ensemble_predictions))\n",
        "cv_ensemble_files.append(create_cv_ensemble(sorted_indices[:6], \"all6\", config_ensemble_predictions))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "U-JbuvlvtmX5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Example load function for later use\n",
        "def load_model(config, fold, device):\n",
        "    model = RecurrentClassifier(\n",
        "        input_size=YOUR_INPUT_SIZE,  # set as used in training\n",
        "        hidden_size=config['hidden_size'],\n",
        "        num_layers=config['hidden_layers'],\n",
        "        num_classes=3,\n",
        "        dropout_rate=config['dropout'],\n",
        "        rec_dropout_rate=config['rec_dropout'],\n",
        "        bidirectional=config['bidirectional'],\n",
        "        cnn_channels=config['cnn_channels'],\n",
        "        cnn_kernel_size=config['cnn_kernel'],\n",
        "        cnn_dropout=config['cnn_dropout'],\n",
        "        rnn_type=config['rnn_type'],\n",
        "        use_attention=config.get('use_attention', True),\n",
        "        num_joint_features=29,\n",
        "        num_pain_features=4,\n",
        "        num_static_features=3,\n",
        "        num_time_features=3\n",
        "    ).to(device)\n",
        "\n",
        "    checkpoint_path = f\"models/{config['name']}_fold{fold}.pth\"\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    model.eval()\n",
        "    return model\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "_ZzQNfwR0-lo"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}